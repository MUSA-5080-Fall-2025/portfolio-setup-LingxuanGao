{
  "hash": "f672066cce9f37ae4e15f50f43c0e521",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assignment 5: Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor:\n  - \"Lingxuan Gao\"\n  - \"Jinyang Xu\"\ndate: \"2025-12-01\"\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    toc: true\n    toc-depth: 3\n    toc-location: left\n    theme: cosmo\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n  message: false\n---\n\n# Introduction\n\n## The Rebalancing Challenge in Philadelphia\n\nPhiladelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**.\n\nIn this lab. we build predictive models that forecast bike share demand across **space** (different stations) and **time** (different hours) to help solve this operational problem.\n\n------------------------------------------------------------------------\n\n# Setup\n\n\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n# Part 1: Replicate with Q3 2024 & Compare to Q1 2025\n\n## Data Import & Preparation\n\nIn this lab, we will use the Indego data (Q3 2024), because we want to explore the pattern's diffenrence between summer and winter.\n\n### Download data for Q3 2024\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q3 2024 data\nindego_q3 <- read_csv(\"data/indego-trips-2024-q3.csv\")\n```\n:::\n\n\n### Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q3 2024:\", nrow(indego_q3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q3 2024: 408408 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego_q3$start_time)), \"to\", \n    max(mdy_hm(indego_q3$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1719792120 to 1727740740 \n```\n\n\n:::\n\n```{.r .cell-code}\nas.POSIXct(1719792120, origin = \"1970-01-01\", tz = \"America/New_York\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2024-06-30 20:02:00 EDT\"\n```\n\n\n:::\n\n```{.r .cell-code}\nas.POSIXct(1727740740, origin = \"1970-01-01\", tz = \"America/New_York\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2024-09-30 19:59:00 EDT\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego_q3$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 261 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego_q3$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    380939      27469 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego_q3$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Day Pass  Indego30 Indego365   Walk-up \n    22885    239857    132358     13308 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego_q3$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  236839   171569 \n```\n\n\n:::\n:::\n\n\n### Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego_q3 <- indego_q3 %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego_q3 %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2024-07-01 00:02:00 2024-07-01 00:00:00    27 Mon       0       0\n2 2024-07-01 00:03:00 2024-07-01 00:00:00    27 Mon       0       0\n3 2024-07-01 00:04:00 2024-07-01 00:00:00    27 Mon       0       0\n4 2024-07-01 00:05:00 2024-07-01 00:00:00    27 Mon       0       0\n5 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n6 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Exploratory Analysis\n\nWe aggregated individual trip records by date to obtain total daily ridership, then used ggplot2 to plot a time-series line chart with a smoothed trend line to visualize how Indego bike-share demand changed over the third quarter of 2024.\n\n### Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego_q3 %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q3 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**:\n\nThe first pattern is the **weekly heartbeat**. Those sharp peaks and valleys repeat with metronomic regularity. Indego always dips on the same days each week, typically weekends, because commuting demand relaxes\n\nThen there’s the **gentle seasonal slope** underneath that weekly pulse. Early July starts relatively high, and then the baseline slowly drifts downward into early August (Maybe because of the summer vocation). Mid-August shows a little recovery, followed by a September lift since the fall semester began. Before the season’s first chilly whisper pushes ridership down again heading into October.\n\n### Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego_q3 %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**: On weekdays, the peaks are tied to commuting: A morning spike around 8–9 AM, when the city jolts awake and people pedal to work. A bigger afternoon/evening spike around 5–6 PM, the classic homebound surge. The slope up to these points is steep, like a heartbeat aligned with office life. On weekends, the rhythm relaxes. There’s no sharp, early-morning people are not rushing anywhere. Ridership climbs more slowly through the morning toward a broad mid-day plateau, roughly 11 AM to 3 PM, and then drifts downward without the evening spike seen on weekdays.\n\nSo the pattern suggests that weekday trips are purposeful and synchronized with the workday, while weekend trips stretch out across the middle of the day in a more leisurely wave.\n\n### Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego_q3 %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,654 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 5,436 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 4,421 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 4,305 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,305 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 4,252 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 4,226 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 4,154 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 4,144 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 4,114 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 4,022 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 3,954 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,948 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 3,942 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 3,921 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 3,919 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,796 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,046 </td>\n   <td style=\"text-align:right;\"> 39.95012 </td>\n   <td style=\"text-align:right;\"> -75.14472 </td>\n   <td style=\"text-align:right;\"> 3,734 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,726 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,678 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Get Philadelphia Spatial Context\n\n### Load Philadelphia Census Data\n\nWe'll get census tract data to add demographic context to our stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  progress_bar = FALSE\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n### Map Philadelphia Context\n\nOur code builds a contextual map by shading each Philadelphia census tract according to its median household income. Then it overlays red points representing Indego trip start locations. The figure shows that most bike-share activity clusters in central and lower-income neighborhoods, while higher-income outer areas have fewer stations and fewer trip origins.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego_q3,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\n### Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego_q3 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego_q3 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_q3_census <- indego_q3 %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego_q3 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\n### Dealing with missing data\n\nIn this step, we are going to remove the non-residential bike share stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_q3_census <- indego_q3 %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n## Get Weather Data\n\nWeather significantly affects bike share demand. We get hourly weather for Philadelphia in this part.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q3 2024: July 1 - September 31\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation        Wind_Speed    \n Min.   :55.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:70.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :76.00   Median :0.000000   Median : 7.000  \n Mean   :75.59   Mean   :0.007896   Mean   : 6.893  \n 3rd Qu.:81.00   3rd Qu.:0.000000   3rd Qu.: 9.000  \n Max.   :98.00   Max.   :1.250000   Max.   :44.000  \n```\n\n\n:::\n:::\n\n\n### Visualize Weather Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q3 2024\",\n    subtitle = \"Summer to early autumn transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**: The pattern is a classic seasonal transition: hot, volatile summer days gradually giving way to cooler, steadier early-fall conditions.\n\n## Create Space-Time Panel\n\n### Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_q3_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 193072\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 241\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2202\n```\n\n\n:::\n:::\n\n\n### Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 530,682 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 193,072 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 337,610 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 530,682 \n```\n\n\n:::\n:::\n\n\n### Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Create day of week factor with treatment (dummy) coding\nstudy_panel <- study_panel %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(study_panel$dotw_simple) <- contr.treatment(7)\n```\n:::\n\n\n### Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation   \n Min.   : 0.0000   Min.   :55.00   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:70.00   1st Qu.:0.0000  \n Median : 0.0000   Median :76.00   Median :0.0000  \n Mean   : 0.7068   Mean   :75.59   Mean   :0.0079  \n 3rd Qu.: 1.0000   3rd Qu.:81.00   3rd Qu.:0.0000  \n Max.   :24.0000   Max.   :98.00   Max.   :1.2500  \n                   NA's   :5784    NA's   :5784    \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n**Why Lags?**\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be \\~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24),\n    lag1week = lag(Trip_Count, 24*7)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 646,362 \n```\n\n\n:::\n:::\n\n\n### Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Temporal Train/Test Split\n\nThe code performs a clean temporal split for forecasting. It first separates the dataset into early weeks and later weeks, identifies stations that appear in both periods, and keeps only those consistent stations. Then it creates a training set using weeks 27–36 and a testing set using weeks 37–40, ensuring the model is trained on past data and evaluated on future data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q3 has weeks 27-40 (Jul-Sep)\n# Train on weeks 27-36 (Jan 1 - early September)\n# Test on weeks 37-40 (rest of September)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 37) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 37) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 37)\n\ntest <- study_panel_complete %>%\n  filter(week >= 37)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 470,940 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 159,330 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19905 to 19974 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 19975 to 19996 \n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n## Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\nThis is a model evaluation function that can generate data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model evaluation function\nmodel_metrics_lm <- function(model, test_data, response_var = \"Trip_Count\") {\n  model_name <- deparse(substitute(model))\n  \n  if (inherits(model, \"glm\") && family(model)$family == \"poisson\") {\n    type       <- \"Poisson\"\n    y_train    <- model$y\n    yhat_train <- predict(model, type = \"response\")\n    \n    sse <- sum((y_train - yhat_train)^2)\n    sst <- sum((y_train - mean(y_train))^2)\n    r2  <- 1 - sse / sst\n    \n    n <- length(y_train)\n    p <- length(coef(model))\n    adjr2 <- 1 - (1 - r2) * (n - 1) / (n - p - 1)\n    \n  } else if (inherits(model, \"lm\")) {\n    type       <- \"OLS\"\n    s          <- summary(model)\n    r2         <- s$r.squared\n    adjr2      <- s$adj.r.squared\n    y_train    <- model$model[[response_var]]\n    yhat_train <- fitted(model)\n    \n  } else {\n    stop(\"Only supports lm and glm(poisson).\")\n  }\n  \n  mae_train <- mean(abs(y_train - yhat_train))\n  \n  if (inherits(model, \"glm\") && family(model)$family == \"poisson\") {\n    yhat_test <- predict(model, newdata = test_data, type = \"response\")\n  } else {\n    yhat_test <- predict(model, newdata = test_data)\n  }\n  y_test <- test_data[[response_var]]\n  cc <- complete.cases(y_test, yhat_test)\n  mae_test <- mean(abs(y_test[cc] - yhat_test[cc]))\n  \n  cat(\"Model:\", model_name, \"\\n\")\n  cat(\"  Type:              \", type, \"\\n\")\n  cat(\"  R-squared (train): \", round(r2, 4), \"\\n\")\n  cat(\"  Adj R-squared:     \", round(adjr2, 4), \"\\n\")\n  cat(\"  MAE (train):       \", round(mae_train, 4), \"\\n\")\n  cat(\"  MAE (test):        \", round(mae_test, 4), \"\\n\\n\")\n}\n```\n:::\n\n\n### Model 1: Baseline (Time + Weather)\n\nThis model captures daily and weekly cycles, weather effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(model1, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: model1 \n  Type:               OLS \n  R-squared (train):  0.1085 \n  Adj R-squared:      0.1085 \n  MAE (train):        0.7937 \n  MAE (test):         0.8293 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: The model shows that trip counts vary strongly by hour of day. Compared to the baseline hour (midnight), coefficients rise sharply in the morning and peak around the late afternoon, matching the commuter pattern observed earlier. Day-of-week effects are smaller but still significant: mid-week days tend to have slightly higher ridership, while weekends show lower counts. Weather matters too—higher temperatures slightly reduce trips, and precipitation has a larger negative effect. Although all predictors are statistically significant (given the huge sample size), the model explains only about 11% of the variation, meaning most hour-to-hour fluctuations are driven by other unmeasured factors such as station-specific conditions or random demand swings.\n\n### Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(model2, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: model2 \n  Type:               OLS \n  R-squared (train):  0.3318 \n  Adj R-squared:      0.3317 \n  MAE (train):        0.6655 \n  MAE (test):         0.6857 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: The model still captures the usual hourly and day-of-week ridership patterns, but the major change comes from adding the lagged demand terms. The one-hour lag has the strongest effect—stations that were busy an hour ago are much more likely to be busy now. The three-hour lag and the previous-day same-hour lag also contribute meaningful predictive power. Weather remains significant but plays a smaller role. Overall model fit improves substantially: R² rises from about 0.11 to about 0.33, and residual error drops, showing that including recent past demand explains much more of the variation. This confirms that short-term station activity is strongly tied to its own immediate history.\n\n### Model 3: Add Demographics\n\nThis model captures neighborhood effects on demand\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(model3, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: model3 \n  Type:               OLS \n  R-squared (train):  0.3373 \n  Adj R-squared:      0.3373 \n  MAE (train):        0.6673 \n  MAE (test):         0.6868 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: The R square means the model explains about 34% of hourly variation in bike demand.Adding demographics raises R² slightly compared to Model 2, they help, but only modestly.Temporal lags dominate the model, while demographics show systematic spatial patterns. All three demographic variables are all significant, and it shows that demographics matter, but compared to temporal lags and hour-of-day, their effect sizes are small.\n\n### Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(model4, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: model4 \n  Type:               OLS \n  R-squared (train):  0.3627 \n  Adj R-squared:      0.3624 \n  MAE (train):        0.6667 \n  MAE (test):         0.6831 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: Model 4 adds a dummy variable for every station, letting the model control for unobserved station-specific factors. It shows that station-specific factors are strong drivers of bike-share demand. Adding fixed effects significantly boosts the model’s ability to explain past variation, but the improvement does not reliably boost predictive accuracy. Even though R² increases, the model’s MAE does not improve much. Because station fixed effects are great for explaining historical data, but they don’t generalize as well when predicting new periods.\n\n### Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(model5, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: model5 \n  Type:               OLS \n  R-squared (train):  0.3669 \n  Adj R-squared:      0.3665 \n  MAE (train):        0.6653 \n  MAE (test):         0.6833 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: Model 5 introduces an interaction between rush hour and weekends, allowing the model to differentiate the strong weekday commuting peaks from the much softer weekend patterns. It also adds month effects to capture seasonal shifts across July, August, and September. Even though Model 5 has the highest R², it performs worse on the test set. This indicates that Model 5 is overfit: it explains historical variation better but loses forecasting accuracy. Model 4 remains the best balance of simplicity and predictive accuracy.\n\n------------------------------------------------------------------------\n\n## Model Evaluation\n\n### Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results1 <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results1, \n      digits = 4,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.8293 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.6857 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.6868 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.6831 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.6833 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Interpretation**: 1. Temporal lags are the most important predictive feature.\n\nModel 2 gives the biggest drop in MAE, showing that short-term demand patterns drive accuracy.\n\n2.  Demographics help explain variation but don’t improve prediction.\n\n3.  Station fixed effects give a small but meaningful improvement.\n\nThey adjust for inherent differences between stations.\n\n4.  Extra complexity (Model 5) does not yield better predictions.\n\nThe model is already near its predictive ceiling.\n\n5.  Best predictive models are Model 2 (simplest) and Model 4 (slightly better).\n\n### Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results1, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\",alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 4)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\n## Compare results to Q1 2025:\n\n![](image/q1.png){width=\"100%\"}\n\n### 1. How do the MAE values compare across quarters?\n\nQ1 2025 MAE values are generally lower than those we reported for Q3 2024.\n\nSo predictions in Q1 are simply easier.\n\n### 2. Why might they differ?\n\nWinter(Q1) has smoother, more predictable demand: Riders may behave more consistently in colder months. There are fewer tourists. Weather is cold but stable—more consistently low temperatures, fewer big swings.\n\nSummer(Q3) is noisy and harder to predict: Weather-driven volatility: heat waves, thunderstorms, humidity. More variation in leisure rides, weekend trips, and irregular patterns. Tourism and events add bursts of unpredictable activity.\n\nSo the model in Q3 faces more randomness, and MAE naturally rises.\n\n### 3. Are temporal patterns different (summer vs. winter)?\n\nYes. In winter, ridership follows a tighter, more predictable rhythm. The daily totals rise gradually from January into March without the big, volatile spikes that appear in summer. Hourly patterns also become cleaner: weekday mornings and evenings show sharp commuter peaks, while weekends stay much flatter with only a modest mid-day bump. By contrast, summer brings stronger mid-day activity, higher weekend volume, and much more weather-driven noise. Winter demand is steadier and more commute-oriented, whereas summer mixes commuting with a large amount of leisure riding, creating far more variation.\n\n### 4. Which features are most important in your quarter?\n\nTemporal lags. The results show a large performance jump when lag variables (1 hour, 3 hours, 1 day) are added. Summer ridership changes quickly with events, weather shifts, and bursts of leisure activity.\n\n# Part 2: Error Analysis\n\n## Space-Time Error Analysis\n\nObserved vs. Predicted\n\nLet's use our best model (Model 4) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred4,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred4)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 4 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**: Where the model performs well: It does best in low-to-moderate demand conditions, especially overnight and during mid-day. Those periods have fewer big swings and smaller trip counts, so the model can track the pattern closely. On weekends—when demand is generally smoother—the points cluster tightly and the green line stays close to the red line. The model doesn’t dramatically over- or under-shoot in these calmer periods.\n\nWhere the model struggles: It struggles during high-demand commuter windows, especially the AM and PM rush hours. In these panels, the cloud of points stretches upward far past the green line, showing that the model systematically under-predicts the busiest moments. Sharp surges in demand are hard to anticipate with simple temporal lags, so the model tends to flatten out the peaks. There is also more scatter on weekday rush hours than on weekends, reflecting the added variability of work-day travel.\n\nIn conclusion, **the model handles routine, low-volume times well, but it misses the sudden spikes that define heavy commuting periods.**\n\n## Spatial Error Patterns\n\nIn this part, we are exploring whether prediction errors are clustered in certain parts of Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Calculate MAE by stations\nstation_errors <- test %>%\n  filter(!is.na(pred4)) %>%\n  group_by(start_station, start_lat.y, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.y), !is.na(start_lon.y))\n\n# 2. Map 1：Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = MAE),\n    size = 0.5, alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12, barheight = 1,\n    title.position = \"top\", title.hjust = 0.5\n  ))\n\n# 3. Map 2：Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = avg_demand),\n    size = 0.5, alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12, barheight = 1,\n    title.position = \"top\", title.hjust = 0.5\n  ))\n\n# 4. Combine two maps\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/spatial_errors-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**: Errors form a dense cluster in and around Center City, extending slightly into University City and parts of South Philly. The neighborhoods in center city have the highest errors.Because these stations often sit near job centers, transit hubs, parks, and retail corridors — all places with unpredictable surges. In short, The model struggles where human behavior is least predictable and demand is most dynamic.\n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**: When are errors highest? Errors peak during the commute periods: PM Rush and AM Rush. These periods have the most unpredictable spikes in demand.\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**: Prediction errors slightly related to neighborhood characteristics?\n\n1.  Higher-income neighborhoods show slightly higher errors. The trend line slopes upward: as median income increases, MAE tends to rise a bit. This likely reflects busier, more mixed-use areas where demand is less predictable.\n\n2.  Neighborhoods with high transit use show lower errors. Stations located in areas with more transit riders tend to have lower MAE. Transit-heavy areas typically have more stable, commute-driven travel patterns that are easier for the model to predict.\n\n3.  Stations in whiter neighborhoods show slightly higher errors. The relationship is mild, but the upward slope suggests that stations in predominantly white, higher-income areas may see more inconsistent demand—often driven by leisure or irregular trip purposes.\n\n# Part 3 : Feature Engineering & Model Improvement\n\n## Feature Selection and Engineering\n\nFrom the error analysis above, we noticed that stations with large prediction errors are concentrated in Center City. From the data structure, we also know that the distribution of Trip_Count is highly skewed. There are many observations with 0 or 1 trip and a few observations with very large counts. The large number of 0 or 1 counts makes the model under-predict for high values. The extreme large counts make the model over-predict for low values. This creates a structural bias that we cannot fully remove.\n\nTo address this, we want to add variables that can detect and partially adjust for this pattern. Distance to Center City is a natural candidate. It can help the model sense the spatial structure behind these errors. However, the station dummy variables may already capture most of the distance differences. Therefore, the distance variable needs to be transformed or used in interaction terms to avoid collinearity with the station fixed effects.\n\nWhen we examine the trip-count-over-time plots, we also see spikes that appear irregularly. These spikes may reflect special events that disrupt the usual demand pattern. They do not happen often, but they can create extreme values and break the roughly linear relationship that the model tries to learn. This can have a strong impact on model performance. Federal holidays are one potential source of such events. The current model does not include enough variables to detect these effects.\n\nBased on these observations, we decide to add two new variables:\n\n1.Holiday indicator\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\n\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    # Convert datetime column to Date\n    date = as.Date(interval60),\n\n    # Weekday with Monday = 1, Tuesday = 2, ..., Sunday = 7\n    wday_mon = wday(date, week_start = 1),\n    \n    # US Federal Holidays (rule-based, works for any year)\n    \n    # New Year's Day: January 1st\n    holiday_newyear = month(date) == 1  & mday(date) == 1,\n\n    # Martin Luther King Jr. Day: 3rd Monday in January\n    holiday_mlk = month(date) == 1 &\n                  wday_mon == 1 &        \n                  mday(date) >= 15 & mday(date) <= 21,\n\n    # Presidents’ Day: 3rd Monday in February\n    holiday_pres = month(date) == 2 &\n                   wday_mon == 1 &       \n                   mday(date) >= 15 & mday(date) <= 21,\n\n    # Memorial Day: last Monday in May (Monday on or after the 25th)\n    holiday_memorial = month(date) == 5 &\n                       wday_mon == 1 &     \n                       mday(date) >= 25,\n\n    # Independence Day: July 4th\n    holiday_july4 = month(date) == 7 & mday(date) == 4,\n\n    # Labor Day: 1st Monday in September (Monday between 1st and 7th)\n    holiday_labor = month(date) == 9 &\n                    wday_mon == 1 &        \n                    mday(date) <= 7,\n\n    # Thanksgiving: 4th Thursday in November\n    holiday_thanks = month(date) == 11 &\n                     wday_mon == 4 &              # Thursday\n                     mday(date) >= 22 & mday(date) <= 28,\n\n    # Christmas Day: December 25th\n    holiday_xmas = month(date) == 12 & mday(date) == 25,\n\n    # Combined holiday indicator: 1 if any of the above holidays, 0 otherwise\n    holiday_any = as.integer(\n      holiday_newyear | holiday_mlk | holiday_pres |\n      holiday_memorial | holiday_july4 | holiday_labor |\n      holiday_thanks | holiday_xmas\n    )\n  )\n```\n:::\n\n\n2.Distance to Center City\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. City Hall point\nphilly_cbd_3857 <- st_sfc(\n  st_point(c(-75.1636, 39.9526)),   # lon, lat\n  crs = 4326\n) %>%\n  st_transform(3857)\n\n# project the dataframe to the same CRS 3857 (to calculate distance in meter)\nstations_dist <- stations_sf %>%\n  st_transform(3857) %>%\n  mutate(\n    dist_center_m = as.numeric(st_distance(geometry, philly_cbd_3857))\n  ) %>%\n  st_drop_geometry() %>%\n  select(start_station, dist_center_m) %>%\n  distinct()\n\nstudy_panel_complete <- study_panel_complete %>%\n  left_join(stations_dist, by = \"start_station\")\n\n\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    Rain_any   = as.integer(Precipitation > 0),\n    Heavy_rain = as.integer(Precipitation >= 0.3) \n  )\n\n# let's make a little change in the intersection by using \"weekday*rush_hour\"\nstudy_panel_complete <- study_panel_complete %>%\n  mutate(\n    weekday = ifelse(dotw %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"), 1, 0)\n  )\n\n#split the train adn test data\ntrain <- study_panel_complete %>%\n  filter(week < 37)\n\ntest <- study_panel_complete %>%\n  filter(week >= 37)\n```\n:::\n\n\n## Model Improvement\n\n### Model 6: Adjust variables in model 4\n\nIn this model, we refine some existing variables and use the result as the second baseline for later improved models.\n\nFirst, based on the summary of Model 4, we noticed that the Precipitation variable did not perform well. However, rain conditions should be an important factor for bike demand. The original variable is very small in magnitude because the total amount of rain is spread across 24 hours. To better capture the effect of rain, we transform it into two dummy variables that indicate whether it is rainy and whether it is raining heavily.\n\nSecond, we believe the weekend indicator is important because the demand pattern on weekends is different from weekdays. In the previous model, it did not work well because the interaction term `weekend:rush_hour` is highly collinear with `as.factor(hour)`. To keep the weekend/weekday effect in the model, we replace this term with the interaction `as.factor(hour):weekend`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel6 <- lm(\n  Trip_Count ~ as.factor(hour)* weekend + dotw_simple + \n    Temperature + \n    Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day  + \n    as.factor(month) + \n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y + \n    as.factor(start_station),  # Rush hour effects different on weekends\n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(model6, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: model6 \n  Type:               OLS \n  R-squared (train):  0.3702 \n  Adj R-squared:      0.3698 \n  MAE (train):        0.664 \n  MAE (test):         0.6783 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: Model 6 serves as our second baseline model after refining several key predictors.\n\nOn weekdays, the model shows strong morning and evening peaks that are consistent with commuting.\\\nOn weekends, the peaks flatten and shift toward the middle of the day.\\\nThe new rain dummies behave as expected: both rainy and heavy-rain hours are associated with lower trip counts compared with dry conditions.\\\nDemographics and station fixed effects mainly shift the overall level of demand at each station, while the refined weather and weekend terms improve the shape of the predicted time profile without a large change in overall MAE.\n\n### Model 7: Add Holiday Indicator and Distance to center city Interation\n\nThis model is based on Model 6 and adds two new components: a holiday indicator and an interaction between distance to Center City and rush hour. As discussed above, the interaction term is not only used to capture the relationship between distance and peak-hour demand, but also to reduce collinearity with the station fixed effects. Adding these two variables may help the model better detect structural patterns in the data and improve overall performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel7 <- lm(\n  Trip_Count ~ as.factor(hour)* weekday + \n    dotw_simple + \n    Temperature + \n    Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day  + \n    as.factor(month) + \n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y + \n    dist_center_m*rush_hour  +                              # interaction\n    holiday_any +                                           # holiday indicator\n    as.factor(start_station),  \n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(model7, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: model7 \n  Type:               OLS \n  R-squared (train):  0.3751 \n  Adj R-squared:      0.3748 \n  MAE (train):        0.6584 \n  MAE (test):         0.6762 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: Model 7 builds on Model 6 by adding two variables that are motivated directly by our error analysis and knowledge of Philadelphia’s spatial and temporal structure: a holiday indicator and an interaction between distance to Center City and rush hour. The idea is that holidays and downtown peak periods are exactly when we observed the largest systematic errors in previous models. These additions slightly improve performance: the test MAE decreases from 0.6783 in Model 6 to 0.6762 in Model 7, and (R\\^2) rises only marginally from 0.3702 to 0.3751. This shows that, even after trying to encode what we learned about the error patterns, urban form, and time-of-day effects into new variables, most of the predictive power still comes from the core ingredients introduced earlier (time of day, temporal lags, and station fixed effects). The remaining errors are likely driven by noise and unobserved factors—such as special events or local disruptions—that are difficult to capture with simple linear terms.\n\n### Model 8: Try a poisson model for count data (base on model 7)\n\nIn addition to the linear regression models, we also estimate a Poisson regression with a log link to better reflect the count nature of the bike trip data. For this specification, we keep the same set of predictors as in Model 7 and only change the estimation method from OLS to Poisson. This allows us to directly compare the two approaches under an identical model structure. The Poisson model serves as a robustness check: it tests whether the main effects of time-of-day, weekend, weather, lagged demand, distance to Center City, and holidays remain consistent when we use a count model that is more appropriate for non-negative integer outcomes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Poisson model with log link\npoisson_model7 <- glm(\n  Trip_Count ~ \n    as.factor(hour) * weekday +       \n    dotw_simple +                     \n    Temperature + Rain_any + Heavy_rain +\n    lag1Hour + lag3Hours + lag1day +\n    as.factor(month) +\n    Med_Inc.y + Percent_Taking_Transit.y + Percent_White.y +\n    dist_center_m*rush_hour +          \n    holiday_any +\n    as.factor(start_station),\n  family = poisson(link = \"log\"),\n  data = train\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_metrics_lm(poisson_model7, test_data = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: poisson_model7 \n  Type:               Poisson \n  R-squared (train):  0.3828 \n  Adj R-squared:      0.3824 \n  MAE (train):        0.6221 \n  MAE (test):         0.6461 \n```\n\n\n:::\n:::\n\n\n**Interpretation**: Finally, we re-estimate Model 7 using a Poisson regression with a log link while keeping exactly the same set of predictors. This change in estimation method leads to a modest but clear improvement in predictive performance: the test MAE decreases from 0.6762 in the OLS version of Model 7 to 0.6461 in the Poisson model, and the (R\\^2) on the training set increases slightly from 0.3751 to 0.3828. The Poisson specification fits the count nature of bike trips better and reduces some of the systematic under- and over-prediction we saw in the linear model, but the gain is not dramatic, which suggests that most remaining errors are due to noise and unobserved shocks rather than the choice between OLS and Poisson.\n\n## Overall Model Evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test),\n    pred6 = predict(model6, newdata = test),\n    pred7 = predict(model7, newdata = test),\n    pred8 = predict(poisson_model7, newdata = test, type = \"response\")\n  )\n\n# Calculate MAE for each model\nmae_results2 <- data.frame(\n  \n  Model = paste0(\n    \"Model \", 1:8\n  ),\n  Description = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\",\n    \"6. Adjust variables (Base on M4)\",\n    \"7. + Holiday Indicator and Distance Interaction\",\n    \"8. Poisson using variables in model 7\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred8), na.rm = TRUE)\n  ) \n) %>%\n  mutate(\n    improvement_from_baseline = round((MAE[1] - MAE) / MAE[1] * 100, 1),\n    improvement_from_lags = round((MAE[6] - MAE) / MAE[2] * 100, 1)\n  )\n\nkable(mae_results2, \n      digits = 4,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"Description\", \"MAE (trips)\", \"% Better than M1\", \"% Better than M6\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:left;\"> Description </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n   <th style=\"text-align:right;\"> % Better than M1 </th>\n   <th style=\"text-align:right;\"> % Better than M6 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Model 1 </td>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.8293 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n   <td style=\"text-align:right;\"> -22.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 2 </td>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.6857 </td>\n   <td style=\"text-align:right;\"> 17.3 </td>\n   <td style=\"text-align:right;\"> -1.1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 3 </td>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.6868 </td>\n   <td style=\"text-align:right;\"> 17.2 </td>\n   <td style=\"text-align:right;\"> -1.2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 4 </td>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.6831 </td>\n   <td style=\"text-align:right;\"> 17.6 </td>\n   <td style=\"text-align:right;\"> -0.7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 5 </td>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.6833 </td>\n   <td style=\"text-align:right;\"> 17.6 </td>\n   <td style=\"text-align:right;\"> -0.7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 6 </td>\n   <td style=\"text-align:left;\"> 6. Adjust variables (Base on M4) </td>\n   <td style=\"text-align:right;\"> 0.6783 </td>\n   <td style=\"text-align:right;\"> 18.2 </td>\n   <td style=\"text-align:right;\"> 0.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 7 </td>\n   <td style=\"text-align:left;\"> 7. + Holiday Indicator and Distance Interaction </td>\n   <td style=\"text-align:right;\"> 0.6762 </td>\n   <td style=\"text-align:right;\"> 18.5 </td>\n   <td style=\"text-align:right;\"> 0.3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 8 </td>\n   <td style=\"text-align:left;\"> 8. Poisson using variables in model 7 </td>\n   <td style=\"text-align:right;\"> 0.6461 </td>\n   <td style=\"text-align:right;\"> 22.1 </td>\n   <td style=\"text-align:right;\"> 4.7 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Interpretation**: From Model 3 to Model 7, we keep adding more variables and refining the specification, but the improvement over Model 2 is quite limited. After including temporal lags in Model 2, the MAE already drops from 0.83 to about 0.69 trips, and this remains the dominant gain. Models 3–5 add demographics, station fixed effects, and a rush-hour interaction, and Models 6–7 further adjust weather, weekend, distance, and holiday variables. However, the MAE stays in a very narrow range around 0.68–0.676, which is only a small improvement compared to Model 2.\n\nThis pattern suggests that once we control for time-of-day and recent demand history, most additional predictors only provide marginal extra information for short-term station-level demand. Demographics and station dummies mainly shift overall levels rather than change hour-to-hour variation, and the added interaction terms capture more subtle structure but do not drastically reduce prediction error on the test set.\n\nIn contrast, Model 8 changes the estimation method rather than the predictor set. It applies a Poisson regression using the same variables as Model 7. This shift to a count model leads to a more noticeable improvement: the MAE decreases to 0.6461 trips, which is substantially better than the OLS models with the same predictors. This indicates that better matching the distributional form of the outcome (non-negative counts) can yield additional gains even when the set of explanatory variables is already rich.\n\n### Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results2, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\",alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 4)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.1))\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Gao_Xu_assignment_5_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n# Part 4: Critical Reflection\n\n1.  **Operational implications:**\n\nOur best model (Poisson, Model 8) has a test-set MAE of about 0.65 trips per station–hour. At this resolution a typical error is well under one bike, which is reasonably good given how noisy and skewed bike-share demand is. In practice Indego would often aggregate predictions over several hours and across groups of stations, so some of these small errors would cancel out and the effective error at the operational level would be even smaller.\n\nPrediction errors become operationally problematic mainly when they are systematic in high-demand contexts. In our results the largest errors tend to occur at busy Center City stations during peak hours and around special days such as holidays or events. Under-predicting in those situations can leave key stations empty or full exactly when demand is highest, while over-predicting at low-demand stations mostly has minor consequences.\n\nI would not use this model as a fully automated rebalancing system, but I would recommend it as a decision-support tool. It is best used on aggregated time windows and station clusters, to flag where demand is likely to be high relative to current inventory, while human operators retain control. As long as staff treat the forecasts as guidance rather than exact numbers, remain cautious around holidays or special events, and periodically monitor and retrain the model, the current performance is good enough to provide useful support for Indego’s rebalancing decisions.\n\n2.  **Equity considerations:**\n\nFrom an equity perspective, the key question is whether our prediction errors are larger or more systematic in certain kinds of neighborhoods. In our error maps, the most obvious clusters of high error appear in Center City and other high-demand stations, which are generally higher-income and whiter areas, so the model does not obviously underperform only in disadvantaged neighborhoods. However, even small absolute errors in lower-income or majority-nonwhite areas can matter more, because there are fewer nearby alternatives if a station is empty or full. If rebalancing decisions rely heavily on these forecasts, any systematic under-prediction in these communities could translate into more frequent stock-outs and longer walking distances, which would reinforce existing disparities in bike access.\n\nTo reduce this risk, the system should include explicit safeguards. First, errors and service levels should be monitored by neighborhood characteristics such as income, race, and transit dependence, not only in aggregate. Second, rebalancing rules can include minimum service standards for equity-priority zones, so that these stations are never deprioritized solely because predicted demand is low or uncertain. Third, planners should periodically audit the model for bias, adjust the specification if one group is consistently under-served, and involve community stakeholders when defining which areas are “must-serve” regardless of forecasted demand. Used with these safeguards, the forecasting system is less likely to worsen existing disparities and can even help identify where additional investment in stations or rebalancing capacity is needed.\n\n3.  **Model limitations:**\n\nOur model still misses several important patterns. It struggles with sharp spikes caused by special events, construction, or one-off disruptions, which do not follow the typical weekday/weekend and seasonal structure learned from the data. It also assumes that the relationship between predictors and demand is stable over time and largely linear on the log scale, even though real-world behavior can change with new infrastructure, pricing, or cultural shifts. In deployment, station capacity constraints, operational decisions (like temporary closures or rebalancing itself), and policy changes would all feed back into observed demand in ways the current model does not explicitly represent. With more time and data, I would incorporate richer features (such as real-time event data, land use and network connectivity, or app usage patterns), explore more flexible models such as gradient boosting or random forests with careful regularization, and estimate separate models for different station types or clusters. I would also set up an ongoing evaluation pipeline to monitor performance over time, detect drift, and retrain or recalibrate the model as Indego’s system and user behavior evolve.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}