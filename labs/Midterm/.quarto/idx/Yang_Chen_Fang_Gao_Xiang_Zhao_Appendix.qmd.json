{"title":"Philadelphia Housing Price Prediction – Technical Appendix","markdown":{"yaml":{"title":"Philadelphia Housing Price Prediction – Technical Appendix","author":"Zicheng Xiang","date":"October 24, 2025","format":{"html":{"toc":true,"toc-depth":3,"code-fold":false,"df-print":"paged"}},"execute":{"echo":true,"warning":false,"message":false}},"headingText":"=========================================================","containsRefs":false,"markdown":"\n\n\nStep0\n\n\n\n```{r}\n# Step 1: Load libraries and data\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(stringr)\n\nopa_raw <- read_csv(\"data/opa_properties_public.csv\",\n                    na = c(\"\", \"NA\", \"NaN\", \"NULL\"),\n                    guess_max = 1e6)\n\ncat(\"Rows (loaded):\", nrow(opa_raw), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 2: Data cleaning and filtering\n# =========================================================\nopa_res <- opa_raw %>%\n  mutate(\n    sale_date = as_date(sale_date),\n    # Prices are standardized as numeric values: regardless of whether they were originally numbers or strings (including $ or commas).\n    sale_price_num = suppressWarnings(\n      coalesce(as.numeric(sale_price), readr::parse_number(as.character(sale_price)))\n    ),\n    # Categories are standardized to a consistent data type.\n    cat_chr = as.character(category_code)\n  ) %>%\n  filter(\n    cat_chr %in% c(\"1\"),  \n    sale_date >= as_date(\"2023-01-01\"),\n    sale_date <= as_date(\"2024-12-31\"),\n    sale_price_num >= 10000,\n    total_livable_area > 0,\n    year_built > 0,\n    number_of_bedrooms > 0,\n    number_of_bathrooms > 0,\n    !is.na(census_tract),\n    census_tract != 0,\n    census_tract != \"\",\n    !is.na(zip_code),\n    zip_code != \"\",\n    !is.na(exterior_condition),\n    exterior_condition != \"\",\n    !is.na(interior_condition),\n    interior_condition != \"\",\n    !is.na(shape),\n    shape != \"\"\n  ) %>%\n  select(\n    parcel_number, sale_date,\n    sale_price = sale_price_num,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, shape\n  ) %>%\n  distinct() %>%\n  drop_na()\n\ncat(\"Rows (after cleaning and filtering):\", nrow(opa_res), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 3: Extract coordinates from shape field (using sf)\n# =========================================================\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Remove the prefix SRID=2272\nwkt <- sub(\"^SRID=\\\\d+;\\\\s*\", \"\", opa_res$shape)\n\n# Convert to sf format.\ngeom <- st_as_sfc(wkt, crs = 2272)\nopa_sf <- st_sf(opa_res, geometry = geom)\n\n# Extract coordinates (for scatter plot).\ncoords <- st_coordinates(opa_sf)\nopa_sf$X <- coords[, 1]\nopa_sf$Y <- coords[, 2]\n\n# Debug output.\ncat(\"Rows with valid coordinates:\", nrow(opa_sf), \"\\n\")\ncat(\"Sample X coordinates:\", head(opa_sf$X, 3), \"\\n\")\ncat(\"Sample Y coordinates:\", head(opa_sf$Y, 3), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 4: Save cleaned data\n# =========================================================\n# Create output directory\ndir.create(here::here(\"data\"), recursive = TRUE, showWarnings = FALSE)\n\n# Export data (remove the geometry column, keeping only coordinates)\nopa_export <- opa_sf %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, x_coord = X, y_coord = Y\n  ) %>%\n  st_drop_geometry()  \n\nwrite_csv(opa_export, \"./data/opa_sales_2023_2024_residential_clean.csv\")\n\ncat(\"Cleaned data saved to: opa_sales_2023_2024_residential_clean.csv\\n\")\ncat(\"Final dataset contains\", nrow(opa_export), \"rows with\", ncol(opa_export), \"columns\\n\")\ncat(\"Columns:\", paste(names(opa_export), collapse = \", \"), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 5: Load, clean crime data and calculate crime count for properties\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(sf)\n\n# Load crime data for 2023 and 2024\ncrime_2023 <- read_csv(here(\"./data/crime_2023.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\ncrime_2024 <- read_csv(here(\"./data/crime_2024.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\n\n# Merge, clean, and process crime data\ncrime_clean <- bind_rows(\n  crime_2023 %>% mutate(year = 2023),\n  crime_2024 %>% mutate(year = 2024)\n) %>%\n  mutate(\n    dispatch_date_time = as_datetime(dispatch_date_time),\n    dispatch_date = as_date(dispatch_date),\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    point_x = as.numeric(point_x),\n    point_y = as.numeric(point_y),\n    ucr_general = as.numeric(ucr_general),\n    text_general_code = as.character(text_general_code)\n  ) %>%\n  filter(\n    !is.na(lat) & !is.na(lng),\n    lat != 0 & lng != 0,\n    dispatch_date >= as_date(\"2023-01-01\"),\n    dispatch_date <= as_date(\"2024-12-31\")\n  ) %>%\n  select(\n    objectid, dc_dist, psa, dispatch_date_time, dispatch_date, \n    dispatch_time, hour, dc_key, location_block, \n    ucr_general, text_general_code, \n    lat, lng, point_x, point_y, year\n  ) %>%\n  distinct() %>%\n  drop_na()\n\n# Coordinate transformation: convert from WGS84 (EPSG:4326) to EPSG:2272 coordinate system\ncrime_sf <- crime_clean %>%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %>%\n  st_transform(crs = 2272) %>%\n  mutate(\n    x_coord_2272 = st_coordinates(.)[, 1],\n    y_coord_2272 = st_coordinates(.)[, 2]\n  ) %>%\n  st_drop_geometry()\n\n# Read the cleaned real estate data\nopa_clean <- read_csv(here(\"./data/opa_sales_2023_2024_residential_clean.csv\"))\n\n# Convert the real estate data to an sf object (EPSG:2272)\nopa_sf <- opa_clean %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n\n# Convert the crime data to an sf object (EPSG:2272)\ncrime_sf_spatial <- crime_sf %>%\n  st_as_sf(coords = c(\"x_coord_2272\", \"y_coord_2272\"), crs = 2272)\n\n# Create a 0.75-mile buffer around each home and count crimes (15-minute walkshed)\nopa_buffers <- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\ncrime_count <- st_intersects(opa_buffers, crime_sf_spatial)\n\n# Add the crime counts to the real estate dataset\nopa_with_crime <- opa_clean %>%\n  mutate(\n    crime_count_15min_walk = sapply(crime_count, length)\n  )\n\ncat(\"Crime data processing completed:\\n\")\ncat(\"Number of 2023 crime records:\", nrow(crime_2023), \"\\n\")\ncat(\"Number of 2024 crime records:\", nrow(crime_2024), \"\\n\")\ncat(\"Number of valid crime records after cleaning:\", nrow(crime_clean), \"\\n\")\ncat(\"Number of records after coordinate transformation:\", nrow(crime_sf), \"\\n\")\ncat(\"Number of real estate records:\", nrow(opa_with_crime), \"\\n\")\ncat(\"Average number of crimes within a 15-minute walkshed per property:\", round(mean(opa_with_crime$crime_count_15min_walk), 2), \"\\n\")\ncat(\"Crime count range:\", min(opa_with_crime$crime_count_15min_walk), \"-\", max(opa_with_crime$crime_count_15min_walk), \"\\n\")\n\n```\n\n```{r}\n# =========================================================\n# Step 6: Add park accessibility metrics (distance in feet, keep coordinate columns)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# Use the crime data processing results from the previous step\nopa_with_crime <- opa_with_crime\n\n# Convert to an sf object (coordinate system: EPSG:2272, unit: feet)\nopa_sf <- st_as_sf(\n  opa_with_crime,\n  coords = c(\"x_coord\", \"y_coord\"),\n  crs = 2272\n)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\n\n# Read and project the park data.\nparks <- st_read(here(\"./data/PPR_Program_Sites.geojson\"), quiet = TRUE) %>%\n  st_transform(2272)\n\n# Calculate park accessibility metrics (unit: feet)\nopa_sf <- opa_sf %>%\n  mutate(\n    dist_to_park_ft = apply(st_distance(opa_sf, parks), 1, min),   \n    # Nearest park distance (feet).\n    park_within_15min_walk = lengths(st_within(opa_sf, st_buffer(parks, 3960)))  # 0.75 mile = 3960 feet\n  )\n\n# Extract coordinates to prevent st_drop_geometry from removing them\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export data (retain coordinates and newly added indicators)\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,              # keep coordinates\n    crime_count_15min_walk,        # number of crimes within 15-minute walkshed\n    dist_to_park_ft,               # nearest park distance (feet)\n    park_within_15min_walk         # number of parks within 15-minute walkshed\n  )\n\n# Output summary information\n# Save the complete dataset with park accessibility indicators\nwrite_csv(opa_export, here(\"./data/opa_sales_with_parks.csv\"))\n\ncat(\" Park accessibility indicators added (unit: feet)\\n\")\ncat(\"  Average nearest park distance:\", round(mean(opa_export$dist_to_park_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of parks within 15-minute walkshed:\", round(mean(opa_export$park_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\ncat(\"🎉 Data saved to: ./data/opa_sales_with_parks.csv\\n\")\n\n```\n\n```{r}\n# =========================================================\n# Step 7: Add public transit accessibility metrics\n# (distance to nearest stop & number of stops within 1,000 ft; also 15-min walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# Use the same CRS (EPSG:2272, units = feet)\nanalysis_crs <- 2272\n\n# Read the dataset saved in the previous step (with price, crime, and park metrics)\nopa_data <- read_csv(here(\"./data/opa_sales_with_parks.csv\"))\nopa_sf <- opa_data %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\n# Read and project transit stop data\ntransit_path <- here(\"./data/Transit_Stops_(Spring_2025).geojson\")\nstopifnot(file.exists(transit_path))\n\ntransit_stops <- st_read(transit_path, quiet = TRUE) %>%\n  st_transform(analysis_crs) %>%\n  suppressWarnings(st_collection_extract(\"POINT\")) %>%\n  filter(!st_is_empty(geometry)) %>%\n  distinct(geometry, .keep_all = TRUE)\n\ncat(\"Number of transit stops:\", nrow(transit_stops), \"\\n\")\n\n# Compute accessibility metrics (units: feet)\n# Distance to nearest transit stop\nnearest_idx <- st_nearest_feature(opa_sf, transit_stops)\ndist_ft <- st_distance(opa_sf, transit_stops[nearest_idx, ], by_element = TRUE)\nopa_sf$dist_transit_ft <- as.numeric(set_units(dist_ft, \"ft\"))\n\n# Count of transit stops within a 15-minute walkshed (0.75 mile = 3960 ft)\nbuffer_3960ft <- st_buffer(opa_sf, dist = 3960)\nopa_sf$transit_15min_walk <- lengths(st_intersects(buffer_3960ft, transit_stops))\n\n# (Optional) Count of transit stops within 1,000 ft\nbuffer_1000ft <- st_buffer(opa_sf, dist = 1000)\nopa_sf$transit_within_1000ft <- lengths(st_intersects(buffer_1000ft, transit_stops))\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and newly added indicators)\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                 # keep coordinates\n    crime_count_15min_walk,           # crime metric\n    dist_to_park_ft, park_within_15min_walk, # park metrics\n    dist_transit_ft, transit_15min_walk,     # transit metrics (walkshed)\n    transit_within_1000ft                    # transit stops within 1,000 ft\n  )\n\n# Output summary information\n# Save the complete dataset with transit metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_transit.csv\"))\n\ncat(\"  Public transit metrics added\\n\")\ncat(\"  Average distance to nearest transit stop:\", round(mean(opa_export$dist_transit_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of transit stops within 15-minute walkshed:\", round(mean(opa_export$transit_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Average number of transit stops within 1,000 ft:\", round(mean(opa_export$transit_within_1000ft, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\ncat(\"🎉 Data saved to: ./data/opa_sales_with_transit.csv\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Step 8: Add hospital accessibility metrics\n# (distance to nearest hospital & count within 0.75-mile walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# CRS: EPSG:2272 (units: feet)\nanalysis_crs <- 2272\n\n# Read the dataset saved in the previous step (with price, crime, park, and transit metrics)\nopa_data <- read_csv(here(\"./data/opa_sales_with_transit.csv\"))\nopa_sf <- opa_data %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\n# Read hospital data and project to the same CRS\nhospitals <- st_read(here(\"./data/Hospitals.geojson\"), quiet = TRUE) %>%\n  st_transform(analysis_crs)\n\ncat(\"Number of hospitals:\", nrow(hospitals), \"\\n\")\n\n# Compute hospital accessibility metrics (units: feet)\nopa_sf <- opa_sf %>%\n  mutate(\n    # Distance to nearest hospital (feet)\n    dist_to_hospital_ft = as.numeric(apply(st_distance(opa_sf, hospitals), 1, min)),\n    # Number of hospitals within a 15-minute walkshed (0.75 mile = 3960 ft)\n    hospitals_15min_walk = lengths(st_within(opa_sf, st_buffer(hospitals, 3960)))\n  )\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and all features)\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                         # keep coordinates\n    crime_count_15min_walk,                   # crime metrics\n    dist_to_park_ft, park_within_15min_walk,  # park metrics\n    dist_transit_ft, transit_15min_walk,      # transit metrics\n    dist_to_hospital_ft, hospitals_15min_walk # hospital metrics (new)\n  )\n\n# Output summary information\n# Save the complete dataset with hospital metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_hospitals.csv\"))\n\ncat(\"  Hospital accessibility metrics added\\n\")\ncat(\"  Average distance to nearest hospital:\", round(mean(opa_export$dist_to_hospital_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of hospitals within 15-minute walkshed:\", round(mean(opa_export$hospitals_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\ncat(\"  Includes all features: real estate info + crime + park + transit + hospital\\n\")\ncat(\"  Data saved to: ./data/opa_sales_with_hospitals.csv\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Step 9: Enrich with ACS (Census) Socioeconomic Indicators\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(readr)\nlibrary(here)\n\n# ---------------------------------------------------------\n# Read the final property data (includes all accessibility metrics)\n# ---------------------------------------------------------\nopa_final <- read_csv(here(\"./data/opa_sales_with_hospitals.csv\"))\n\n# Convert to sf object (ensure CRS is EPSG:2272)\nopa_sf <- opa_final %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\n\n# ---------------------------------------------------------\n# Download ACS data for Philadelphia (year can be changed)\n# ---------------------------------------------------------\nyear_acs <- 2022\ncensus_api_key(\"86993dedbe98d77b9d79db6b8ba21a7fde55cb91\", install = FALSE)\n\nacs_vars <- c(\n  total_pop      = \"B01003_001\",\n  median_income  = \"B19013_001\",\n  per_cap_income = \"B19301_001\",\n  below_pov      = \"B17001_002\",\n  edu_total25    = \"B15003_001\",\n  edu_bach       = \"B15003_022\",\n  edu_mast       = \"B15003_023\",\n  edu_prof       = \"B15003_024\",\n  edu_phd        = \"B15003_025\"\n)\n\nphl_acs <- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = year_acs,\n  geometry = TRUE,\n  output = \"wide\",\n  variables = acs_vars\n) %>%\n  mutate(\n    PCBACHMORE = 100 * ((edu_bachE + edu_mastE + edu_profE + edu_phdE) / edu_total25E),\n    PCTPOVERTY = 100 * (below_povE / total_popE)\n  ) %>%\n  select(geometry, GEOID, total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY)\n\n# ---------------------------------------------------------\n# Reproject to align (EPSG:2272)\n# ---------------------------------------------------------\nphl_acs <- st_transform(phl_acs, 2272)\n\n# ---------------------------------------------------------\n# Spatial join: assign each home to the tract it falls within\n# ---------------------------------------------------------\nopa_joined <- st_join(\n  opa_sf,\n  phl_acs,\n  join = st_within,\n  left = TRUE\n)\n\n# ---------------------------------------------------------\n# For out-of-bound samples (occasional NAs), fill with nearest tract\n# ---------------------------------------------------------\nmissing <- is.na(opa_joined$median_incomeE)\nif (any(missing)) {\n  idx <- st_nearest_feature(opa_joined[missing, ], phl_acs)\n  repl <- phl_acs[idx, ] %>% st_drop_geometry()\n  cols <- names(repl)\n  opa_joined[missing, cols] <- repl\n}\n\n# ---------------------------------------------------------\n# Export results\n# ---------------------------------------------------------\nopa_export <- opa_joined %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  # crime\n    dist_to_park_ft, park_within_15min_walk, # parks\n    dist_transit_ft, transit_15min_walk,     # transit\n    dist_to_hospital_ft, hospitals_15min_walk, # hospitals\n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY  # ACS\n  )\n\n# Save the complete dataset with Census indicators\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  ACS socioeconomic indicators added\\n\")\ncat(\"  Mean household income (USD):\", round(mean(opa_export$median_incomeE, na.rm = TRUE), 0), \"\\n\")\ncat(\"  Mean poverty rate (%):\", round(mean(opa_export$PCTPOVERTY, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Mean share with bachelor's degree or higher (%):\", round(mean(opa_export$PCBACHMORE, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Includes all features: price + crime + parks + transit + hospitals + socioeconomic\\n\")\ncat(\"  Data saved to: opa_sales_final_complete.csv\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Step 10: Add Education Accessibility Indicators (Schools)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# ---------------------------------------------------------\n# 1.Read the complete dataset saved from the previous step (including Census and accessibility features)\n# ---------------------------------------------------------\nopa_data <- read_csv(here(\"opa_sales_final_complete.csv\"))\nopa_sf <- opa_data %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\ncat(\"Data column names:\", paste(names(opa_sf), collapse = \", \"), \"\\n\")\n\n# ---------------------------------------------------------\n# 2.Read the school data (only one file)）\n# ---------------------------------------------------------\nschools <- st_read(here(\"./data/Schools_Parcels.geojson\"), quiet = TRUE) %>%\n  st_transform(2272) %>%\n  filter(!st_is_empty(geometry))  \n\ncat(\"School data loaded:\", nrow(schools), \"records\\n\")\ncat(\"School data coordinate system:\", st_crs(schools)$input, \"\\n\")\ncat(\"Property data coordinate system:\", st_crs(opa_sf)$input, \"\\n\")\n\n# ---------------------------------------------------------\n# 3.Calculate education accessibility indicators\n# ---------------------------------------------------------\n# Distance to the nearest school (in feet)\ncat(\"Starting to calculate the nearest school distance...\\n\")\ndist_matrix <- st_distance(opa_sf, schools)\ncat(\"Distance matrix dimensions:\", dim(dist_matrix), \"\\n\")\nopa_sf$dist_to_nearest_school_ft <- as.numeric(apply(dist_matrix, 1, min))\ncat(\"Nearest school distance calculation completed, range:\",\n    min(opa_sf$dist_to_nearest_school_ft), \"-\", \n    max(opa_sf$dist_to_nearest_school_ft), \"feet\\n\")\n\n# Number of schools within a 15-minute walking distance\ncat(\"Starting to calculate the number of schools within a 15-minute walking distance...\\n\")\nbuffer_3960ft <- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\nopa_sf$schools_within_15min_walk <- lengths(st_intersects(buffer_3960ft, schools))\ncat(\"Calculation of schools within a 15-minute walking distance completed, range:\",\n    min(opa_sf$schools_within_15min_walk), \"-\", \n    max(opa_sf$schools_within_15min_walk), \"\\n\")\n\n\n# ---------------------------------------------------------\n# 4.Extract coordinates and retain all columns\n# ---------------------------------------------------------\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# ---------------------------------------------------------\n# 5.Export the complete table with education accessibility indicators\n# ---------------------------------------------------------\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  \n    dist_to_park_ft, park_within_15min_walk, \n    dist_transit_ft, transit_15min_walk,     \n    dist_to_hospital_ft, hospitals_15min_walk, \n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY,  # Census\n    dist_to_nearest_school_ft, schools_within_15min_walk                  \n  )\n\n# ---------------------------------------------------------\n# 6.Clean the data: remove all rows containing empty or NA values\n# ---------------------------------------------------------\nrows_before <- nrow(opa_export)\n\nopa_export <- opa_export %>%\n  mutate(across(everything(), ~ {\n    if (is.character(.x)) {\n      clean_val <- trimws(tolower(as.character(.x)))\n      ifelse(clean_val == \"\" | clean_val == \"na\" | clean_val == \"n/a\" | clean_val == \"null\", \n             NA, .x)\n    } else {\n      .x\n    }\n  })) %>%\n  drop_na()\n\nrows_after <- nrow(opa_export)\nrows_removed <- rows_before - rows_after\n\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  Education accessibility indicators have been added\\n\")\ncat(\"  Average distance to the nearest school:\", \n    round(mean(opa_export$dist_to_nearest_school_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of schools within a 15-minute walking distance:\", \n    round(mean(opa_export$schools_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Data cleaning completed: removed\", rows_removed, \"rows containing empty or NA values\\n\")\ncat(\"  Before cleaning:\", rows_before, \"rows → After cleaning:\", rows_after, \"rows\\n\")\ncat(\"  Final complete dataset saved to: opa_sales_final_complete.csv\\n\")\ncat(\"  Includes all features: housing price + crime + parks + transit + hospitals + Census + schools\\n\")\ncat(\"  Number of columns:\", ncol(opa_export), \"(including coordinate columns)\\n\")\n\n\n```\n\n\n\n\n\nStep1\n\n\n\n\n```{r}\n# =========================================================\n# Step 1: Skewness Detection + Log Transformation + Descriptive Statistics\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(e1071)\nlibrary(patchwork)\nlibrary(readr)\nlibrary(tidyr)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"📊 Step 1: Data Cleaning, Transformation, and Descriptive Statistics\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n# === Read Data ===\ndf <- read_csv(\"opa_sales_final_complete.csv\", show_col_types = FALSE)\ncat(sprintf(\"Original sample size: %d\\n\", nrow(df)))\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\n\n# === Basic Processing ===\nif (\"year_built\" %in% names(df)) {\n  df <- df %>%\n    mutate(age = 2025 - year_built,\n           age2 = age^2) %>%\n    dplyr::select(-year_built)\n}\n\ncat_vars <- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncoord_vars <- c(\"x_coord\", \"y_coord\")\ndf[cat_vars] <- lapply(df[cat_vars], factor)\n\n# === Skewness Detection ===\ncat(\"=== Skewness Detection ===\\n\")\nnum_vars <- df %>% dplyr::select(where(is.numeric))\nskews <- sapply(num_vars, function(x) {\n  if (all(is.na(x)) || sd(x, na.rm = TRUE) == 0) return(0)\n  skewness(x, na.rm = TRUE)\n})\nlogged_vars <- names(skews[abs(skews) > 1])\ncat(sprintf(\"Variables with |skew| > 1: %d\\n\", length(logged_vars)))\n\n# === Apply log1p Transformation ===\ndf_trans <- df\nfor (v in logged_vars) {\n  df_trans[[v]] <- log1p(pmax(df[[v]], 0))\n}\n\n```\n\n```{r}\n# =========================================================\n# 1. Descriptive Statistics Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating Descriptive Statistics Table ===\\n\\n\")\n\n# Identify target variable\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df_trans) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df_trans) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df_trans) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n# Select key numeric variables\nkey_vars <- c(\n  y_var,\n  \"total_livable_area\", \"number_of_bedrooms\", \"number_of_bathrooms\", \"age\",\n  \"median_incomeE\", \"per_cap_incomeE\", \"PCTPOVERTY\", \"PCBACHMORE\"\n)\nkey_vars <- intersect(key_vars, names(df_trans))\n\nif (length(key_vars) > 0) {\n  # Compute descriptive statistics (more meaningful on pre-transformation data)\n  desc_stats <- df %>%\n    dplyr::select(all_of(key_vars)) %>%\n    summarise(across(everything(), list(\n      N = ~sum(!is.na(.)),\n      Mean = ~mean(., na.rm = TRUE),\n      SD = ~sd(., na.rm = TRUE),\n      Min = ~min(., na.rm = TRUE),\n      Q25 = ~quantile(., 0.25, na.rm = TRUE),\n      Median = ~median(., na.rm = TRUE),\n      Q75 = ~quantile(., 0.75, na.rm = TRUE),\n      Max = ~max(., na.rm = TRUE)\n    ), .names = \"{.col}_{.fn}\")) %>%\n    pivot_longer(everything(), names_to = \"stat\", values_to = \"value\") %>%\n    separate(stat, into = c(\"Variable\", \"Statistic\"), sep = \"_(?=[^_]+$)\") %>%\n    pivot_wider(names_from = Statistic, values_from = value) %>%\n    dplyr::select(Variable, N, Mean, SD, Min, Q25, Median, Q75, Max) %>%\n    mutate(across(c(Mean, SD, Min, Q25, Median, Q75, Max), ~round(., 3)))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\")\n  write_csv(desc_stats, \"file/descriptive_statistics.csv\")\n  cat(\"  ✓ file/descriptive_statistics.csv\\n\")\n  print(as.data.frame(desc_stats), row.names = FALSE)\n}\n\n\n```\n\n```{r}\n# =========================================================\n# 2. Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating distribution comparison plots ===\\n\\n\")\n\nplot_list <- lapply(names(num_vars), function(v) {\n  is_transformed <- v %in% logged_vars\n  \n  # Original distribution\n  p1 <- ggplot(df, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = \"#053061\", color = \"white\", alpha = 0.85) +\n    labs(title = paste0(v, \" (Original)\"), x = NULL, y = \"Count\") +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Transformed distribution\n  fill_color <- if (is_transformed) \"#67001F\" else \"#4393C3\"\n  \n  p2 <- ggplot(df_trans, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = fill_color, color = \"white\", alpha = 0.85) +\n    labs(\n      title = paste0(v, if (is_transformed) \" (Log-transformed)\" else \" (No transformation)\"),\n      x = NULL, y = \"Count\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Add skewness annotation\n  skew_val <- round(skews[v], 2)\n  p1 <- p1 + annotate(\"text\", x = Inf, y = Inf, \n                      label = paste0(\"Skewness: \", skew_val),\n                      hjust = 1.1, vjust = 1.5, size = 2.5, color = \"gray30\")\n  \n  pair_plot <- (p1 | p2) + plot_layout(widths = c(1, 1)) &\n    theme(plot.margin = margin(3, 3, 3, 3))\n  \n  wrap_elements(pair_plot) + \n    theme(\n      plot.background = element_rect(fill = \"white\", color = \"#B0B0B0\", linewidth = 1.2),\n      plot.margin = margin(6, 6, 6, 6)\n    )\n})\n\nall_plot <- wrap_plots(plot_list, ncol = 2) + \n  plot_annotation(\n    title = \"Variable Distributions: Original vs. Transformed\",\n    subtitle = paste0(\n      \"Dark Blue = Original | Dark Red = Log-transformed (|skew| > 1) | Medium Blue = No transformation\"\n    ),\n    theme = theme(\n      plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray30\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\n```\n\n```{r}\n# Skewness Summary Plot\nskew_df <- data.frame(\n  variable = names(skews),\n  skewness = skews,\n  transformed = names(skews) %in% logged_vars\n) %>% arrange(desc(abs(skewness)))\n\np_skew <- ggplot(skew_df, aes(x = reorder(variable, abs(skewness)), y = skewness, fill = transformed)) +\n  geom_col(alpha = 0.85) +\n  geom_hline(yintercept = c(-1, 1), linetype = \"dashed\", color = \"#67001F\", linewidth = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  scale_fill_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"No transformation\", \"TRUE\" = \"Log-transformed\"),\n    name = NULL\n  ) +\n  coord_flip() +\n  labs(\n    title = \"Skewness of All Variables\",\n    subtitle = \"Variables with |skewness| > 1 are log-transformed\",\n    x = \"Variable\", y = \"Skewness\",\n    caption = \"Dashed lines indicate skewness thresholds at ±1\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\n```\n\n```{r}\n# =========================================================\n# 3. Categorical Variable Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating categorical variable distribution plots ===\\n\\n\")\n\ncat_vars_viz <- c(\"interior_condition\", \"exterior_condition\")\ncat_vars_viz <- intersect(cat_vars_viz, names(df_trans))\n\nif (length(cat_vars_viz) > 0 && !is.na(y_var)) {\n  plot_list_cat <- list()\n  \n  for (var in cat_vars_viz) {\n    cat_summary <- df_trans %>%\n      group_by(!!sym(var)) %>%\n      summarise(\n        count = n(),\n        mean_price = mean(.data[[y_var]], na.rm = TRUE),\n        .groups = \"drop\"\n      ) %>%\n      filter(!is.na(!!sym(var))) %>%\n      arrange(desc(mean_price))\n    \n    if (nrow(cat_summary) > 0) {\n      p <- ggplot(cat_summary, aes(reorder(!!sym(var), mean_price), mean_price, fill = mean_price)) +\n        geom_col(alpha = 0.9) +\n        geom_text(aes(label = count), vjust = -0.5, size = 3) +\n        scale_fill_gradient2(\n          low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n          midpoint = median(cat_summary$mean_price, na.rm = TRUE),\n          guide = \"none\"\n        ) +\n        coord_flip() +\n        theme_minimal(base_size = 10) +\n        theme(\n          panel.grid.major.y = element_blank(),\n          plot.title = element_text(hjust = 0.5, face = \"bold\", size = 12),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA)\n        ) +\n        labs(\n          title = gsub(\"_\", \" \", toupper(var)),\n          x = NULL,\n          y = \"Average Sale Price (log)\",\n          caption = \"Numbers indicate the count in each category\"\n        )\n      \n      plot_list_cat[[var]] <- p\n    }\n  }\n  \n  if (length(plot_list_cat) > 0) {\n    p_cat_combined <- wrap_plots(plot_list_cat, ncol = 2) +\n      plot_annotation(\n        title = \"Average Sale Price by Property Condition\",\n        subtitle = \"Log-transformed sale prices\",\n        theme = theme(\n          plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n          plot.subtitle = element_text(size = 11, hjust = 0.5),\n          plot.background = element_rect(fill = \"white\", color = NA)\n        )\n      )\n    \n    if (!dir.exists(\"plot\")) dir.create(\"plot\")\n    ggsave(\"plot/categorical_price_comparison.png\", p_cat_combined,\n           width = 12, height = 6, dpi = 300, bg = \"white\")\n    cat(\"  ✓ plot/categorical_price_comparison.png\\n\")\n  }\n}\n```\n\n```{r}\n# =========================================================\n# Output Files\n# =========================================================\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nif (!dir.exists(\"file\")) dir.create(\"file\")\n\nggsave(\"plot/all_variables_distribution.png\", plot = all_plot, width = 20, height = 12, dpi = 300, bg = \"white\")\nggsave(\"plot/skewness_summary.png\", plot = p_skew, width = 10, height = 8, dpi = 300, bg = \"white\")\nwriteLines(logged_vars, \"file/logged_variables.txt\")\nwrite_csv(df_trans, \"file/opa_sales_step1_clean.csv\")\n\ntransform_summary <- data.frame(\n  Variable = names(skews),\n  Original_Skewness = round(skews, 3),\n  Transformed = names(skews) %in% logged_vars\n) %>% arrange(desc(abs(Original_Skewness)))\n\nwrite_csv(transform_summary, \"file/transformation_summary.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 1 Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\" Output Files:\\n\\n\")\ncat(\"Tables:\\n\")\ncat(\"  • file/descriptive_statistics.csv - Descriptive statistics\\n\")\ncat(\"  • file/transformation_summary.csv - Transformation summary\\n\")\ncat(\"  • file/logged_variables.txt - List of log-transformed variables\\n\")\ncat(\"  • file/opa_sales_step1_clean.csv - Cleaned dataset\\n\\n\")\n\ncat(\"Figures:\\n\")\ncat(\"  • plot/all_variables_distribution.png - All variables: original vs. transformed\\n\")\ncat(\"  • plot/skewness_summary.png - Skewness summary\\n\")\ncat(\"  • plot/categorical_price_comparison.png - Categorical variables vs. sale price\\n\\n\")\n\ncat(\"📈 Stats:\\n\")\ncat(sprintf(\"  • Variables with |skew| > 1: %d\\n\", length(logged_vars)))\ncat(sprintf(\"  • Final number of variables: %d\\n\", ncol(df_trans)))\ncat(sprintf(\"  • Final sample size: %d\\n\\n\", nrow(df_trans)))\n\n```\n\n\n\nStep2\n\n\n\n\n```{r}\n# =========================================================\n# Step 2 Enhanced: Correlation Matrix + VIF + Spatial Visualization + Scatterplots\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(reshape2)\nlibrary(readr)\nlibrary(patchwork)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"  Step 2: Correlation Analysis, Multicollinearity, and Spatial Exploration\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n# === Read Data ===\ndf <- read_csv(\"file/opa_sales_step1_clean.csv\", show_col_types = FALSE)\n\ncat(sprintf(\"Sample size: %d\\n\", nrow(df)))\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\n# === Auto-detect target variable ===\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat_vars <- intersect(c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\"), names(df))\ncoord_vars <- intersect(c(\"x_coord\", \"y_coord\"), names(df))\ndf[cat_vars] <- lapply(df[cat_vars], factor)\n\n```\n\n```{r}\n# =========================================================\n# 1. Correlation Matrix\n# =========================================================\n\ncat(\"=== Generating correlation matrix ===\\n\")\n\nnum_df <- df %>% dplyr::select(where(is.numeric))\nnum_df <- num_df[, sapply(num_df, function(x) sd(x, na.rm = TRUE) > 0), drop = FALSE]\n\nif (ncol(num_df) > 1) {\n  corr_mat <- cor(num_df, use = \"pairwise.complete.obs\")\n  corr_mat[upper.tri(corr_mat)] <- NA\n  corr_melt <- melt(corr_mat, na.rm = TRUE)\n  \n  # If there are too many variables, keep only the first 50\n  if (ncol(num_df) > 50) {\n    vars_top <- names(num_df)[1:50]\n    corr_melt <- corr_melt %>% filter(Var1 %in% vars_top & Var2 %in% vars_top)\n  }\n  \n  p_corr <- ggplot(corr_melt, aes(Var2, Var1, fill = value, size = abs(value))) +\n    geom_point(shape = 21, color = \"white\", stroke = 0.5) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0, limits = c(-1, 1), name = NULL,\n      breaks = seq(-1, 1, 0.2)\n    ) +\n    scale_size_continuous(range = c(0.5, 10), guide = \"none\") +\n    scale_x_discrete(position = \"top\") +\n    scale_y_discrete(limits = rev) +\n    theme_minimal(base_size = 11) +\n    theme(\n      axis.text.x.top = element_text(angle = 45, hjust = 0, vjust = 0, size = 9, color = \"black\"),\n      axis.text.y = element_text(size = 9, color = \"black\"),\n      axis.title = element_blank(),\n      panel.grid.major = element_line(color = \"#E0E0E0\", linewidth = 0.5),\n      panel.grid.minor = element_blank(),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"bottom\",\n      legend.direction = \"horizontal\",\n      legend.key.width = unit(3, \"cm\"),\n      legend.key.height = unit(0.4, \"cm\"),\n      plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n      plot.margin = margin(10, 10, 10, 10)\n    ) +\n    labs(title = \"Correlation Matrix\") +\n    coord_fixed()\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n  ggsave(\"plot/corr_matrix_enhanced.png\", p_corr, width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/corr_matrix_enhanced.png\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# 2. VIF Analysis\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== VIF Analysis ===\\n\\n\")\n\nvif_vars <- setdiff(names(df), c(y_var, cat_vars, coord_vars))\nnum_vif <- df %>% dplyr::select(all_of(vif_vars)) %>% dplyr::select(where(is.numeric))\nnum_vif <- num_vif[, sapply(num_vif, function(x) sd(x, na.rm = TRUE) > 0), drop = FALSE]\n\nif (ncol(num_vif) >= 2) {\n  df_vif <- df %>% dplyr::select(all_of(c(y_var, names(num_vif)))) %>% na.omit()\n  f_vif <- as.formula(paste(y_var, \"~ .\"))\n  vif_model <- lm(f_vif, data = df_vif)\n  vif_vals <- car::vif(vif_model)\n  vif_tbl <- data.frame(variable = names(vif_vals), VIF = round(vif_vals, 3))\n  vif_tbl <- vif_tbl %>% arrange(desc(VIF))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\n  write_csv(vif_tbl, \"file/vif_values.csv\")\n  cat(\"  ✓ file/vif_values.csv\\n\")\n  \n  # High VIF warning\n  high_vif <- vif_tbl %>% filter(VIF > 10)\n  if (nrow(high_vif) > 0) {\n    cat(\"\\n  Variables with high multicollinearity (VIF > 10):\\n\")\n    print(as.data.frame(high_vif), row.names = FALSE)\n  }\n  \n  # VIF visualization\n  vif_top20 <- vif_tbl %>% top_n(20, VIF)\n  \n  p_vif <- ggplot(vif_top20, aes(x = reorder(variable, VIF), y = VIF, fill = VIF)) +\n    geom_col(alpha = 0.9) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 5, name = \"VIF Value\"\n    ) +\n    geom_hline(yintercept = 10, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n    geom_hline(yintercept = 5, linetype = \"dashed\", color = \"gray50\", linewidth = 0.7) +\n    coord_flip() +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.major.y = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Variance Inflation Factor (VIF) - Top 20\",\n      subtitle = \"Color gradient: Blue (low VIF) → Red (high VIF)\",\n      x = \"Variable\", y = \"VIF Value\",\n      caption = \"Dark red line: VIF=10 (High multicollinearity) | Gray line: VIF=5 (Moderate)\"\n    )\n  \n  ggsave(\"plot/vif_analysis.png\", p_vif, width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/vif_analysis.png\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# 3. Spatial Visualization\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Spatial Distribution Visualization ===\\n\\n\")\n  \n  # 3.1 Spatial distribution of sale prices\n  p_spatial_price <- ggplot(df, aes(x_coord, y_coord, color = .data[[y_var]])) +\n    geom_point(alpha = 0.6, size = 0.8) +\n    scale_color_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(df[[y_var]], na.rm = TRUE),\n      name = \"Sale Price\\n(log)\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"right\"\n    ) +\n    labs(\n      title = \"Spatial Distribution of Sale Prices\",\n      subtitle = \"Philadelphia housing market\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_price_distribution.png\", p_spatial_price,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_price_distribution.png\\n\")\n  \n  # 3.2 Hexbin density map\n  p_hex <- ggplot(df, aes(x_coord, y_coord)) +\n    geom_hex(aes(fill = after_stat(count)), bins = 40) +\n    scale_fill_gradient(\n      low = \"#F7F7F7\", high = \"#67001F\",\n      name = \"Count\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Housing Density Heatmap\",\n      subtitle = \"Hexagonal binning of property locations\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_density_hexbin.png\", p_hex,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_density_hexbin.png\\n\")\n}\n```\n\n```{r}\n# =========================================================\n# 4. Key Variable Scatterplots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Scatterplots: Key Variables vs. Sale Price ===\\n\\n\")\n\nkey_numeric <- c(\"total_livable_area\", \"median_incomeE\", \"age\", \"number_of_bathrooms\")\nkey_numeric <- intersect(key_numeric, names(df))\n\nif (length(key_numeric) >= 4) {\n  scatter_plots <- list()\n  \n  for (var in key_numeric[1:4]) {\n    # Correlation coefficient\n    corr_val <- cor(df[[var]], df[[y_var]], use = \"pairwise.complete.obs\")\n    \n    p <- ggplot(df, aes(.data[[var]], .data[[y_var]])) +\n      geom_point(alpha = 0.3, size = 0.8, color = \"#053061\") +\n      geom_smooth(method = \"lm\", se = TRUE, color = \"#67001F\", linewidth = 1.2) +\n      annotate(\"text\", x = Inf, y = -Inf, \n               label = sprintf(\"r = %.3f\", corr_val),\n               hjust = 1.1, vjust = -0.5, size = 4, color = \"#67001F\", fontface = \"bold\") +\n      theme_minimal(base_size = 10) +\n      theme(\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 11),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      ) +\n      labs(\n        title = gsub(\"_\", \" \", var),\n        x = gsub(\"_\", \" \", var),\n        y = \"Sale Price (log)\"\n      )\n    \n    scatter_plots[[var]] <- p\n  }\n  \n  p_scatter <- wrap_plots(scatter_plots, ncol = 2) +\n    plot_annotation(\n      title = \"Key Variables vs. Sale Price\",\n      subtitle = \"Linear fit (OLS) with correlation coefficients\",\n      theme = theme(\n        plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 11, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/key_variables_scatter.png\", p_scatter,\n         width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/key_variables_scatter.png\\n\")\n}\n\n```\n\n\n```{r}\n# =========================================================\n# Save cleaned data\n# =========================================================\n\nwrite_csv(df, \"file/opa_sales_step2_clean.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"  Step 2 Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\"  Output files:\\n\\n\")\ncat(\"Tables:\\n\")\ncat(\"  • file/vif_values.csv - VIF analysis results\\n\")\ncat(\"  • file/opa_sales_step2_clean.csv - Cleaned dataset\\n\\n\")\n\ncat(\"Figures:\\n\")\ncat(\"  • plot/corr_matrix_enhanced.png - Correlation matrix\\n\")\ncat(\"  • plot/vif_analysis.png - VIF analysis\\n\")\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  • plot/spatial_price_distribution.png - Spatial distribution of sale prices\\n\")\n  cat(\"  • plot/spatial_density_hexbin.png - Housing density heatmap\\n\")\n}\nif (length(key_numeric) >= 4) {\n  cat(\"  • plot/key_variables_scatter.png - Key variables vs. sale price (scatterplots)\\n\")\n}\ncat(\"\\n\")\n```\n\n\n\n\n\nStep3\n\n\n\n\n\n```{r}\n# =========================================================\n# Step 3: VIF-based Filtering + LASSO Feature Selection (Streamlined)\n# =========================================================\n\nlibrary(glmnet)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\nset.seed(2025)\n\n# === Read data and VIF results ===\ndf <- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nvif_path <- \"file/vif_values.csv\"\nif (!file.exists(vif_path)) stop(\"  file/vif_values.csv not found. Please run step2.R first.\")\nvif_table <- read_csv(vif_path, show_col_types = FALSE)\n\n# === Determine target variable ===\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\"  Target variable sale_price_log or sale_price not found\")\n\n# === Variable filtering (VIF) ===\nvif_threshold <- 5\nvars_keep <- vif_table %>%\n  filter(VIF <= vif_threshold) %>%\n  pull(variable)\nvars_keep <- intersect(vars_keep, names(df))\n\ncat(\"  Number of variables passing VIF ≤\", vif_threshold, \":\", length(vars_keep), \"\\n\")\n```\n\n```{r}\n\n# === Prepare data matrix ===\ndf_lasso <- df %>%\n  dplyr::select(all_of(c(y_var, vars_keep))) %>%\n  na.omit()\n\nx <- as.matrix(df_lasso %>% dplyr::select(-all_of(y_var)))\ny <- df_lasso[[y_var]]\n\n# === Run LASSO regression ===\ncvfit <- cv.glmnet(\n  x, y,\n  alpha = 1,            # LASSO\n  nfolds = 10,\n  standardize = TRUE\n)\n\nlambda_min <- cvfit$lambda.min\nlambda_1se <- cvfit$lambda.1se\ncat(\"λ_min =\", signif(lambda_min, 5), \"\\n\")\ncat(\"λ_1se =\", signif(lambda_1se, 5), \"\\n\")\n\n# === Extract non-zero coefficients ===\ncoef_min <- coef(cvfit, s = \"lambda.min\")\nselected <- rownames(coef_min)[coef_min[, 1] != 0]\nselected <- selected[selected != \"(Intercept)\"]\n\nselected_tbl <- data.frame(\n  variable = selected,\n  coefficient = as.numeric(coef_min[selected, 1])\n)\n\n# === Remove variables with near-zero coefficients ===\nselected_tbl <- selected_tbl %>%\n  filter(abs(coefficient) >= 1e-5) %>%\n  arrange(desc(abs(coefficient)))\n\ncat(\"  Number of variables retained after LASSO:\", nrow(selected_tbl), \"\\n\")\n\n```\n\n```{r}\n\n# === Output files ===\nif (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\nif (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n\nwrite_csv(selected_tbl, \"file/lasso_selected_variables.csv\")\n```\n\n```{r}\n# === Visualization 1: LASSO Coefficient Importance (Top 20) ===\ntop_n_vars <- min(20, nrow(selected_tbl))\nselected_top <- selected_tbl %>% \n  top_n(top_n_vars, abs(coefficient)) %>%\n  arrange(coefficient)\n\np_coef <- ggplot(selected_top, aes(x = reorder(variable, coefficient), y = coefficient, fill = coefficient)) +\n  geom_col(alpha = 0.9) +\n  scale_fill_gradient2(\n    low = \"#053061\",      \n    mid = \"#F7F7F7\",      \n    high = \"#67001F\",     \n    midpoint = 0,\n    name = \"Coefficient\"\n  ) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  coord_flip() +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Selected Variables - Coefficient Importance\",\n    subtitle = paste0(\"Top \", top_n_vars, \" variables by absolute coefficient value\"),\n    x = \"Variable\",\n    y = \"Coefficient\",\n    caption = paste0(\"λ_min = \", signif(lambda_min, 4), \" | Total selected: \", nrow(selected_tbl), \" variables\")\n  )\n\nggsave(\"plot/lasso_coefficients.png\", p_coef, width = 10, height = 8, dpi = 300, bg = \"white\")\n\n```\n\n```{r}\n# === Visualization 2: Cross-Validation Curve (λ Selection) ===\ncv_df <- data.frame(\n  lambda = cvfit$lambda,\n  cvm = cvfit$cvm,\n  cvsd = cvfit$cvsd,\n  cvlo = cvfit$cvm - cvfit$cvsd,\n  cvup = cvfit$cvm + cvfit$cvsd\n)\n\np_cv <- ggplot(cv_df, aes(x = log(lambda), y = cvm)) +\n  geom_ribbon(aes(ymin = cvlo, ymax = cvup), fill = \"#4393C3\", alpha = 0.3) +\n  geom_line(color = \"#053061\", linewidth = 1) +\n  geom_point(color = \"#053061\", size = 2, alpha = 0.6) +\n  geom_vline(xintercept = log(lambda_min), linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  geom_vline(xintercept = log(lambda_1se), linetype = \"dashed\", color = \"#D73027\", linewidth = 0.7) +\n  annotate(\"text\", x = log(lambda_min), y = max(cv_df$cvm) * 0.95, \n           label = paste0(\"λ_min = \", signif(lambda_min, 3)), \n           hjust = -0.1, size = 3.5, color = \"#67001F\") +\n  annotate(\"text\", x = log(lambda_1se), y = max(cv_df$cvm) * 0.90, \n           label = paste0(\"λ_1se = \", signif(lambda_1se, 3)), \n           hjust = -0.1, size = 3.5, color = \"#D73027\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Cross-Validation Curve\",\n    subtitle = \"Mean Squared Error vs. log(λ)\",\n    x = \"log(λ)\",\n    y = \"Mean Squared Error\",\n    caption = \"Shaded area represents ±1 standard error\"\n  )\n\nggsave(\"plot/lasso_cv_curve.png\", p_cv, width = 10, height = 7, dpi = 300, bg = \"white\")\n```\n\n```{r}\n# === Selection Summary ===\ncat(\"\\n\", rep(\"=\", 60), \"\\n\", sep = \"\")\ncat(\" Summary of Feature Selection Results\\n\")\ncat(rep(\"=\", 60), \"\\n\")\ncat(\"Initial number of predictors:\", ncol(x), \"\\n\")\ncat(\"After VIF ≤\", vif_threshold, \":\", length(vars_keep), \"\\n\")\ncat(\"Retained after LASSO:\", nrow(selected_tbl), \"\\n\")\ncat(\"Selection rate:\", round((1 - nrow(selected_tbl) / ncol(x)) * 100, 1), \"%\\n\")\ncat(rep(\"=\", 60), \"\\n\\n\")\n\ncat(\" Step 3 Completed\\n\")\ncat(\" LASSO results: file/lasso_selected_variables.csv\\n\")\ncat(\" Coefficient importance: plot/lasso_coeff_importance_top20.png\\n\")\ncat(\" CV curve: plot/lasso_cv_curve.png\\n\")\n\n```\n\n\n\nStep4\n\n\n\n```{r}\n# =========================================================\n# Step 4 Enhanced: Four Progressive Models + Coefficient Tables + Feature Importance\n# =========================================================\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(car)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"  Step 4: OLS Progressive Modeling and Full Analysis\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n```\n\n```{r}\n# === Read Data ===\ndf <- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\n\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat(sprintf(\"Target variable: %s\\n\", y_var))\ncat(sprintf(\"Sample size: %d\\n\\n\", nrow(df)))\n\n```\n\n```{r}\n# === Data preprocessing ===\ncat(\"=== Data preprocessing ===\\n\")\n\ncat_vars <- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncat_vars <- intersect(cat_vars, names(df))\n\nfor (cat_var in cat_vars) {\n  if (cat_var %in% c(\"zip_code\", \"census_tract\")) {\n    freq_table <- table(df[[cat_var]])\n    sparse_levels <- names(freq_table[freq_table < 100])\n    df[[cat_var]] <- as.character(df[[cat_var]])\n    df[[cat_var]][df[[cat_var]] %in% sparse_levels] <- \"Other\"\n    df[[cat_var]] <- factor(df[[cat_var]])\n    cat(sprintf(\"  %s: %d → %d categories\\n\", cat_var, length(freq_table), length(levels(df[[cat_var]]))))\n  } else {\n    df[[cat_var]] <- factor(df[[cat_var]])\n  }\n}\n\ncoord_vars <- c(\"x_coord\", \"y_coord\")\nif (all(coord_vars %in% names(df))) {\n  center_x <- median(df$x_coord, na.rm = TRUE)\n  center_y <- median(df$y_coord, na.rm = TRUE)\n  df$dist_to_center <- sqrt((df$x_coord - center_x)^2 + (df$y_coord - center_y)^2)\n  cat(\"  Added: dist_to_center\\n\")\n} else {\n  coord_vars <- c()\n}\n\n# Identify variable groups\nstructural_vars <- c()\nfor (pattern in c(\"livable_area\", \"bedroom\", \"bathroom\", \"stories\", \"garage\", \"age\")) {\n  matched <- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  structural_vars <- c(structural_vars, matched)\n}\nstructural_vars <- unique(structural_vars)\nstructural_vars <- structural_vars[sapply(df[structural_vars], is.numeric)]\n\ncensus_vars <- c()\nfor (pattern in c(\"income\", \"poverty\", \"education\", \"bachelor\", \"population\", \"household\")) {\n  matched <- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  census_vars <- c(census_vars, matched)\n}\ncensus_vars <- unique(census_vars)\ncensus_vars <- census_vars[sapply(df[census_vars], is.numeric)]\n\nspatial_vars <- c(coord_vars, \"dist_to_center\")\nspatial_vars <- intersect(spatial_vars, names(df))\n\nfixed_effects <- c(\"zip_code\", \"census_tract\")\nfixed_effects <- intersect(fixed_effects, names(df))\n\ncat(sprintf(\"\\n  Structural features: %d\\n\", length(structural_vars)))\ncat(sprintf(\"  Census features: %d\\n\", length(census_vars)))\ncat(sprintf(\"  Spatial features: %d\\n\", length(spatial_vars)))\ncat(sprintf(\"  Fixed effects: %d\\n\", length(fixed_effects)))\n\n```\n\n```{r}\n# =========================================================\n# Build 4 Progressive Models\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Building 4 Progressive Models ===\\n\\n\")\n\nmake_formula <- function(y, main_vars, interact_vars = NULL, fe_vars = NULL) {\n  rhs <- main_vars\n  if (!is.null(interact_vars) && length(interact_vars) == 2) {\n    rhs <- c(rhs, paste(interact_vars, collapse = \":\"))\n  }\n  if (!is.null(fe_vars)) {\n    rhs <- c(rhs, fe_vars)\n  }\n  as.formula(paste(y, \"~\", paste(rhs, collapse = \" + \")))\n}\n\nf1 <- make_formula(y_var, structural_vars)\nf2 <- make_formula(y_var, c(structural_vars, census_vars))\nf3 <- make_formula(y_var, c(structural_vars, census_vars, spatial_vars))\n\ninteract_pairs <- NULL\nif (\"total_livable_area\" %in% structural_vars && \"median_incomeE\" %in% census_vars) {\n  interact_pairs <- c(\"total_livable_area\", \"median_incomeE\")\n}\n\nf4 <- make_formula(y_var, c(structural_vars, census_vars, spatial_vars),\n                   interact_vars = interact_pairs, fe_vars = fixed_effects)\n\nmodels <- list()\ncat(\"  Fitting Model 1...\"); models[[\"M1: Structural\"]] <- lm(f1, data = df, na.action = na.exclude); cat(\" ✓\\n\")\ncat(\"  Fitting Model 2...\"); models[[\"M2: +Census\"]] <- lm(f2, data = df, na.action = na.exclude); cat(\" ✓\\n\")\ncat(\"  Fitting Model 3...\"); models[[\"M3: +Spatial\"]] <- lm(f3, data = df, na.action = na.exclude); cat(\" ✓\\n\")\ncat(\"  Fitting Model 4...\"); models[[\"M4: +Interact+FE\"]] <- lm(f4, data = df, na.action = na.exclude); cat(\" ✓\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Model Performance Evaluation\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Model Performance Evaluation ===\\n\\n\")\n\ntrain_perf <- lapply(models, function(m) {\n  pred <- fitted(m)\n  actual <- df[[y_var]][!is.na(fitted(m))]\n  \n  # Log-scale metrics\n  rmse_log <- sqrt(mean((pred - actual)^2, na.rm = TRUE))\n  mae_log <- mean(abs(pred - actual), na.rm = TRUE)\n  \n  # Original-scale metrics — use expm1 to invert log1p\n  pred_original <- expm1(pred)\n  actual_original <- expm1(actual)\n  \n  # Method 3: remove outliers based on residuals (|residual| > 3 standard deviations)\n  residuals_original <- pred_original - actual_original\n  res_sd <- sd(residuals_original, na.rm = TRUE)\n  valid_idx <- abs(residuals_original) <= 3 * res_sd\n  \n  # Compute RMSE and MAE after removing outliers\n  rmse_original <- sqrt(mean((pred_original[valid_idx] - actual_original[valid_idx])^2, na.rm = TRUE))\n  mae_original <- mean(abs(pred_original[valid_idx] - actual_original[valid_idx]), na.rm = TRUE)\n  \n  data.frame(\n    RMSE_log = rmse_log,\n    RMSE_original = rmse_original,\n    MAE_log = mae_log,\n    MAE_original = mae_original,\n    R2 = summary(m)$r.squared,\n    Adj_R2 = summary(m)$adj.r.squared,\n    N_vars = length(coef(m)) - 1,\n    N = length(pred),\n    N_valid = sum(valid_idx, na.rm = TRUE),      # retained sample count\n    Pct_valid = mean(valid_idx, na.rm = TRUE) * 100  # percentage of retained samples\n  )\n}) %>% bind_rows(.id = \"Model\")\n\n# Print removal stats\ncat(\"\\nOutlier removal statistics (|residual| > 3σ):\\n\")\nprint(train_perf %>% select(Model, N, N_valid, Pct_valid, RMSE_original, MAE_original))\n\ncat(\"Performing 10-fold cross-validation...\\n\")\ncv_ctrl <- trainControl(method = \"cv\", number = 10, verboseIter = FALSE)\nformulas <- list(f1, f2, f3, f4)\nnames(formulas) <- names(models)\n\ncv_results <- lapply(formulas, function(fm) {\n  tryCatch(train(fm, data = df, method = \"lm\", trControl = cv_ctrl, na.action = na.omit),\n           error = function(e) NULL)\n})\ncv_results <- Filter(Negate(is.null), cv_results)\n\ncv_perf <- lapply(cv_results, function(m) {\n  data.frame(CV_RMSE_log = m$results$RMSE[1], CV_R2 = m$results$Rsquared[1])\n}) %>% bind_rows(.id = \"Model\")\n\nperf_table <- train_perf %>%\n  left_join(cv_perf, by = \"Model\") %>%\n  mutate(Overfit_RMSE_log = CV_RMSE_log - RMSE_log, Overfit_R2 = R2 - CV_R2)\n\ncat(\"\\n\")\nprint(perf_table, digits = 4, row.names = FALSE)\n\n```\n\n```{r}\n# =========================================================\n# Heteroskedasticity Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Heteroskedasticity Test (Model 4) ===\\n\\n\")\n\nbp_test <- bptest(models[[\"M4: +Interact+FE\"]])\ncat(sprintf(\"Breusch–Pagan Test:\\n  Statistic = %.2f\\n  p-value = %.4f\\n\", \n            bp_test$statistic, bp_test$p.value))\n\nif (bp_test$p.value < 0.05) {\n  cat(\"\\n️ Significant heteroskedasticity detected (p < 0.05)\\n   Recommendation: use robust standard errors\\n\")\n} else {\n  cat(\"\\n No significant heteroskedasticity detected\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# Model 4 Full Coefficient Table (Robust Standard Errors)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Model 4: Full Coefficient Table (Robust SE) ===\\n\\n\")\n\nmodel4 <- models[[\"M4: +Interact+FE\"]]\nrobust_coef <- coeftest(model4, vcov = vcovHC(model4, type = \"HC1\"))\n\ncoef_table <- tidy(robust_coef) %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(\n    sig = case_when(\n      p.value < 0.001 ~ \"***\",\n      p.value < 0.01 ~ \"**\",\n      p.value < 0.05 ~ \"*\",\n      p.value < 0.10 ~ \".\",\n      TRUE ~ \"\"\n    )\n  ) %>%\n  arrange(p.value) %>%\n  dplyr::select(term, estimate, std.error, statistic, p.value, sig)\n\nif (!dir.exists(\"file\")) dir.create(\"file\")\nif (!dir.exists(\"table\")) dir.create(\"table\")\n\nwrite_csv(coef_table, \"table/model4_full_coefficients.csv\")\ncat(\"  ✓ table/model4_full_coefficients.csv (All coefficients)\\n\")\n\ncoef_sig <- coef_table %>% filter(p.value < 0.05)\nwrite_csv(coef_sig, \"table/model4_significant_coefficients.csv\")\ncat(\"  ✓ table/model4_significant_coefficients.csv (Significant coefficients)\\n\")\n\ncat(sprintf(\"\\nNumber of significant variables (p < 0.05): %d\\n\", nrow(coef_sig)))\ncat(\"\\nTop 20 significant variables:\\n\")\nprint(as.data.frame(head(coef_sig, 20)), row.names = FALSE)\n\n\n```\n\n```{r}\n# =========================================================\n# Stargazer Regression Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating Stargazer Regression Table ===\\n\\n\")\n\nif (requireNamespace(\"stargazer\", quietly = TRUE)) {\n  sink(\"table/regression_table.txt\")\n  stargazer::stargazer(\n    models[[1]], models[[2]], models[[3]], models[[4]],\n    type = \"text\",\n    title = \"Progressive OLS Regression Results\",\n    dep.var.labels = \"Log Sale Price\",\n    column.labels = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"),\n    omit.stat = c(\"ser\", \"f\"),\n    digits = 3,\n    star.cutoffs = c(0.05, 0.01, 0.001),\n    notes = \"Robust standard errors in parentheses\"\n  )\n  sink()\n  cat(\"   table/regression_table.txt\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# Cross-Model Feature Importance\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Cross-Model Feature Importance Comparison ===\\n\\n\")\n\nimportance_list <- list()\nfor (i in 1:4) {\n  model_name <- names(models)[i]\n  model <- models[[i]]\n  \n  coefs <- tidy(model) %>%\n    filter(term != \"(Intercept)\") %>%\n    mutate(abs_estimate = abs(estimate)) %>%\n    arrange(desc(abs_estimate)) %>%\n    head(10) %>%\n    mutate(Model = model_name) %>%\n    dplyr::select(Model, term, estimate, abs_estimate)\n  \n  importance_list[[i]] <- coefs\n}\n\nimportance_all <- bind_rows(importance_list)\nwrite_csv(importance_all, \"table/feature_importance_all_models.csv\")\ncat(\"  ✓ table/feature_importance_all_models.csv\\n\")\n\n# Identify variables that are important across multiple models\ntop_vars <- importance_all %>%\n  group_by(term) %>%\n  summarise(appearances = n(), .groups = \"drop\") %>%\n  filter(appearances >= 3) %>%\n  pull(term)\n\nif (length(top_vars) > 0) {\n  importance_key <- importance_all %>% filter(term %in% top_vars)\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\")\n  \n  p_importance <- ggplot(importance_key, aes(Model, abs_estimate, fill = Model)) +\n    geom_col(alpha = 0.8) +\n    facet_wrap(~term, scales = \"free_y\", ncol = 4) +\n    scale_fill_manual(\n      values = c(\"#053061\", \"#2166AC\", \"#4393C3\", \"#D6604D\"),\n      guide = \"none\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1, size = 7),\n      strip.text = element_text(face = \"bold\", size = 9),\n      panel.grid.major.x = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      plot.subtitle = element_text(hjust = 0.5, size = 10),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Feature Importance Across Models\",\n      subtitle = \"Variables appearing in top 10 of at least 3 models\",\n      x = NULL, y = \"Absolute Coefficient\"\n    )\n  \n  ggsave(\"plot/feature_importance_across_models.png\", p_importance,\n         width = 14, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/feature_importance_across_models.png\\n\")\n}\n```\n\n```{r}\n# =========================================================\n# Visualization (Original 3 Figures)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generate Performance Visualizations ===\\n\\n\")\n\nperf_plot_data <- perf_table %>% mutate(Model_Num = 1:n())\n\np_r2 <- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = R2, color = \"Training R²\"), linewidth = 1.2) +\n  geom_line(aes(y = CV_R2, color = \"CV R²\"), linewidth = 1.2) +\n  geom_point(aes(y = R2, color = \"Training R²\"), size = 3) +\n  geom_point(aes(y = CV_R2, color = \"CV R²\"), size = 3) +\n  scale_color_manual(values = c(\"Training R²\" = \"#67001F\", \"CV R²\" = \"#053061\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"R² Improvement: Progressive Model Building\", x = NULL, y = \"R² Value\")\n\np_rmse <- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), linewidth = 1.2) +\n  geom_point(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), size = 3) +\n  scale_color_manual(values = c(\"RMSE ($1000s)\" = \"#67001F\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"RMSE Reduction (Original Scale)\", x = NULL, y = \"RMSE ($1000s)\")\n\np_combined <- (p_r2 | p_rmse) +\n  plot_annotation(\n    title = \"4-Model Progressive OLS: Performance Evolution\",\n    subtitle = \"From structural features to full specification with interactions and fixed effects\",\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 11, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/ols_4model_performance.png\", p_combined, width = 14, height = 6, dpi = 300, bg = \"white\")\ncat(\"  ✓ plot/ols_4model_performance.png\\n\")\n```\n\n```{r}\n# Prediction Scatter Plot\npred_m4 <- fitted(models[[\"M4: +Interact+FE\"]])\nactual_m4 <- df[[y_var]][!is.na(pred_m4)]\npred_data <- data.frame(\n  actual = actual_m4,\n  predicted = pred_m4[!is.na(pred_m4)],\n  residual = pred_m4[!is.na(pred_m4)] - actual_m4\n)\n\np_pred <- ggplot(pred_data, aes(actual, predicted)) +\n  geom_point(aes(color = residual), alpha = 0.4, size = 0.8) +\n  scale_color_gradient2(low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\", midpoint = 0, name = \"Residual\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = min(pred_data$actual) + 0.5, y = max(pred_data$predicted) - 0.5,\n           label = sprintf(\"R² = %.4f\\nRMSE = %.4f\", perf_table$R2[4], perf_table$RMSE[4]),\n           hjust = 0, size = 4.5, fontface = \"bold\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"Model 4: Prediction vs. Actual\",\n       subtitle = \"Full specification with interactions and fixed effects\",\n       x = \"Actual Sale Price (log)\", y = \"Predicted Sale Price (log)\")\n\nggsave(\"plot/ols_4model_prediction.png\", p_pred, width = 10, height = 8, dpi = 300, bg = \"white\")\ncat(\"   plot/ols_4model_prediction.png\\n\")\n```\n\n```{r}\n# Residual Diagnostics\nresid_data <- data.frame(\n  fitted = pred_m4[!is.na(pred_m4)],\n  residual = pred_data$residual,\n  std_resid = rstandard(models[[\"M4: +Interact+FE\"]])[!is.na(pred_m4)]\n)\n\np_r1 <- ggplot(resid_data, aes(fitted, residual)) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residuals vs. Fitted\", x = \"Fitted\", y = \"Residuals\")\n\np_r2 <- ggplot(resid_data, aes(sample = std_resid)) +\n  stat_qq(alpha = 0.4, size = 0.6, color = \"#053061\") +\n  stat_qq_line(color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical\", y = \"Std. Residuals\")\n\np_r3 <- ggplot(resid_data, aes(std_resid)) +\n  geom_histogram(bins = 50, fill = \"#053061\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residual Distribution\", x = \"Std. Residuals\", y = \"Count\")\n\np_r4 <- ggplot(resid_data, aes(fitted, sqrt(abs(std_resid)))) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Scale-Location\", x = \"Fitted\", y = \"√|Std. Residuals|\")\n\np_diag <- (p_r1 | p_r2) / (p_r3 | p_r4) +\n  plot_annotation(\n    title = sprintf(\"Model 4 Diagnostics (BP test p = %.4f)\", bp_test$p.value),\n    theme = theme(plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n                  plot.background = element_rect(fill = \"white\", color = NA))\n  )\n\nggsave(\"plot/ols_4model_diagnostics.png\", p_diag, width = 12, height = 10, dpi = 300, bg = \"white\")\ncat(\"  ✓ plot/ols_4model_diagnostics.png\\n\")\n```\n\n```{r}\n# =========================================================\n# Spatial Prediction Comparison Plot\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Generating Spatial Prediction Comparison Plot ===\\n\\n\")\n  \n  # Prepare data\n  spatial_pred_data <- df %>%\n    filter(!is.na(x_coord) & !is.na(y_coord)) %>%\n    mutate(\n      predicted = fitted(models[[\"M4: +Interact+FE\"]]),\n      residual = predicted - .data[[y_var]]\n    ) %>%\n    filter(!is.na(predicted))\n  \n  # If the dataset is too large, randomly sample to speed up plotting\n  if (nrow(spatial_pred_data) > 10000) {\n    set.seed(2025)\n    spatial_pred_data <- spatial_pred_data %>% \n      slice_sample(n = 10000)\n    cat(\"  Large dataset detected; randomly sampled 10,000 points for plotting\\n\")\n  }\n\n# 1. Spatial Distribution of Actual Sale Prices\np_actual <- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = .data[[y_var]])) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Actual Sale Price\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 2. Spatial Distribution of Predicted Sale Prices\np_predicted <- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = predicted)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Predicted Sale Price (Model 4)\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 3. Spatial Distribution of Residuals\np_residual_spatial <- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = residual)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = 0,\n    name = \"Residual\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Prediction Residuals\",\n    subtitle = \"Blue = Underpredicted | Red = Overpredicted\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# Combine the 3 plots\np_spatial_comparison <- (p_actual | p_predicted | p_residual_spatial) +\n  plot_annotation(\n    title = \"Spatial Distribution: Actual vs. Predicted Sale Prices\",\n    subtitle = sprintf(\"Model 4 predictions (R² = %.4f, RMSE = %.4f)\", \n                      perf_table$R2[4], perf_table$RMSE[4]),\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/spatial_prediction_comparison.png\", p_spatial_comparison,\n       width = 18, height = 6, dpi = 300, bg = \"white\")\ncat(\"   plot/spatial_prediction_comparison.png\\n\")\n\n  # 4. High-Resolution Hexbin Heatmap Version\n  p_actual_hex <- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = .data[[y_var]])) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Actual (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_predicted_hex <- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = predicted)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Predicted (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_residual_hex <- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = residual)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0,\n      name = \"Avg Residual\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Residuals (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_spatial_hexbin <- (p_actual_hex | p_predicted_hex | p_residual_hex) +\n    plot_annotation(\n      title = \"Spatial Distribution (Hexbin Aggregation): Actual vs. Predicted\",\n      subtitle = \"Hexagonal bins show average values in each area\",\n      theme = theme(\n        plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/spatial_prediction_hexbin.png\", p_spatial_hexbin,\n         width = 18, height = 6, dpi = 300, bg = \"white\")\n  cat(\"  ✓ plot/spatial_prediction_hexbin.png\\n\")\n  \n  # 5. Spatial Clustering of Residuals\n  # Calculate spatial statistics of residuals\n  residual_stats <- spatial_pred_data %>%\n    summarise(\n      mean_residual = mean(residual, na.rm = TRUE),\n      sd_residual = sd(residual, na.rm = TRUE),\n      min_residual = min(residual, na.rm = TRUE),\n      max_residual = max(residual, na.rm = TRUE)\n    )\n  \n  cat(\"\\n  Spatial Residual Statistics:\\n\")\n  cat(sprintf(\"    Mean: %.4f\\n\", residual_stats$mean_residual))\n  cat(sprintf(\"    Standard Deviation: %.4f\\n\", residual_stats$sd_residual))\n  cat(sprintf(\"    Range: [%.4f, %.4f]\\n\", \n              residual_stats$min_residual, residual_stats$max_residual))\n  \n  # Identify high-residual areas\n  high_residual_areas <- spatial_pred_data %>%\n    filter(abs(residual) > 2 * residual_stats$sd_residual) %>%\n    mutate(residual_type = ifelse(residual > 0, \"Overpredicted\", \"Underpredicted\"))\n  \n  if (nrow(high_residual_areas) > 0) {\n    cat(sprintf(\"\\n  ️ Detected %d high-residual points (|residual| > 2σ):\\n\", \n                nrow(high_residual_areas)))\n    cat(sprintf(\"    Overpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Overpredicted\")))\n    cat(sprintf(\"    Underpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Underpredicted\")))\n    \n    # Save high-residual point data\n    write_csv(high_residual_areas %>% \n                dplyr::select(x_coord, y_coord, !!sym(y_var), predicted, residual, residual_type),\n              \"file/high_residual_locations.csv\")\n    cat(\"     file/high_residual_locations.csv\\n\")\n  }\n}\n```\n\n```{r}\n# =========================================================\n# Save Results\n# =========================================================\n\nwrite_csv(perf_table, \"file/ols_4model_performance.csv\")\nsaveRDS(models, \"file/ols_4models.rds\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 4 Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\" Output Files:\\n\\n\")\ncat(\"Tables:\\n\")\ncat(\"  • table/model4_full_coefficients.csv - Full coefficient table (Model 4)\\n\")\ncat(\"  • table/model4_significant_coefficients.csv - Significant coefficients (Model 4)\\n\")\ncat(\"  • table/regression_table.txt - Stargazer regression summary\\n\")\ncat(\"  • table/feature_importance_all_models.csv - Cross-model feature importance\\n\")\ncat(\"  • file/ols_4model_performance.csv - Model performance comparison\\n\")\ncat(\"  • file/ols_4models.rds - RDS objects of all 4 models\\n\\n\")\n\ncat(\"Figures:\\n\")\ncat(\"  • plot/ols_4model_performance.png - Performance evolution\\n\")\ncat(\"  • plot/ols_4model_prediction.png - Model 4 prediction vs. actual\\n\")\ncat(\"  • plot/ols_4model_diagnostics.png - Model 4 diagnostic plots\\n\")\ncat(\"  • plot/feature_importance_across_models.png - Cross-model feature importance\\n\")\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  • plot/spatial_prediction_comparison.png - Spatial prediction comparison\\n\")\n  cat(\"  • plot/spatial_prediction_hexbin.png - Spatial prediction heatmap\\n\")\n}\ncat(\"\\n\")\n\ncat(\" Model Summary:\\n\")\n\nfor (i in 1:4) {\n  rmse_fmt <- formatC(perf_table$RMSE_original[i], format = \"f\", digits = 0, big.mark = \",\")\n  cat(sprintf(\"  Model %d: R² = %.4f, RMSE = $%s [%d variables]\\n\",\n              i, perf_table$R2[i], rmse_fmt, perf_table$N_vars[i]))\n}\n\ncat(\"\\n Paper Writing Suggestions:\\n\")\ncat(\"  1. Results (Opening): Descriptive statistics table\\n\")\ncat(\"  2. Results (Core): Stargazer regression summary (comparison of 4 models)\\n\")\ncat(\"  3. Results (Details): Significant coefficients of Model 4\\n\")\ncat(\"  4. Discussion: Cross-model feature importance comparison\\n\")\ncat(\"  5. Mention the use of robust standard errors to address heteroskedasticity\\n\\n\")\n\n```\n\n\n\n\nStep4 plus\n\n\n\n```{r}\n# =========================================================\n# Step 4 Supplement: Cook’s Distance + Moran’s I Spatial Autocorrelation\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(spdep)  # Moran’s I\nlibrary(sf)     # Spatial data\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 4 Supplement: Influential Point Diagnostics and Spatial Autocorrelation Test\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n# === Load Data and Models ===\ndf <- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nmodels <- readRDS(\"file/ols_4models.rds\")\nmodel4 <- models[[\"M4: +Interact+FE\"]]\n\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n```\n\n```{r}\n# =========================================================\n# 1. Cook’s Distance – Influential Observation Detection\n# =========================================================\n\ncat(\"=== Cook’s Distance Analysis ===\\n\\n\")\n\n# Calculate Cook’s distance\ncooks_d <- cooks.distance(model4)\nn <- length(cooks_d)\nthreshold <- 4/n  # Classic threshold\n\n# Identify influential points\ninfluential <- which(cooks_d > threshold)\ncat(sprintf(\"Sample size: %d\\n\", n))\ncat(sprintf(\"Cook’s D threshold: %.6f (4/n)\\n\", threshold))\ncat(sprintf(\"Number of influential points: %d (%.2f%%)\\n\", length(influential), 100*length(influential)/n))\n\n# Save influential observations\nif (length(influential) > 0) {\n  influential_data <- df[influential, ] %>%\n    mutate(\n      cooks_d = cooks_d[influential],\n      fitted = fitted(model4)[influential],\n      residual = residuals(model4)[influential]\n    ) %>%\n    arrange(desc(cooks_d)) %>%\n    head(100)  # Save Top 100\n  \n  write_csv(influential_data, \"file/influential_observations.csv\")\n  cat(\"  ✓ file/influential_observations.csv (Top 100 Influential Points)\\n\")\n  \n  # Display Top 10\n  cat(\"\\nTop 10 Influential Points:\\n\")\n  print(influential_data %>% \n          select(parcel_number, sale_price, cooks_d, fitted, residual) %>% \n          head(10), \n        n = 10)\n}\n\n# Visualize Cook’s Distance\ncooks_df <- data.frame(\n  index = 1:n,\n  cooks_d = cooks_d,\n  influential = cooks_d > threshold\n)\n\np_cooks <- ggplot(cooks_df, aes(index, cooks_d, color = influential)) +\n  geom_point(alpha = 0.4, size = 0.8) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  scale_color_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"Normal\", \"TRUE\" = \"Influential\"),\n    name = NULL\n  ) +\n  annotate(\"text\", x = n*0.8, y = threshold*1.2,\n           label = sprintf(\"Threshold = %.6f\", threshold),\n           color = \"#67001F\", size = 4) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Cook’s Distance – Detection of Influential Observations\",\n    subtitle = sprintf(\"%d influential points (%.2f%% of data)\", \n                      length(influential), 100*length(influential)/n),\n    x = \"Observation Index\",\n    y = \"Cook’s Distance\",\n    caption = \"Threshold = 4/n (classic rule)\"\n  )\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nggsave(\"plot/cooks_distance.png\", p_cooks, width = 12, height = 6, dpi = 300, bg = \"white\")\ncat(\"\\n   plot/cooks_distance.png\\n\")\n\n```\n\n```{r}\n# =========================================================\n# 2. Moran’s I – Spatial Autocorrelation Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Moran’s I Spatial Autocorrelation Test ===\\n\\n\")\n\n# Check coordinates\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  \n  # Prepare spatial data\n  df_spatial <- df %>%\n    filter(!is.na(x_coord), !is.na(y_coord)) %>%\n    mutate(\n      residual = residuals(model4)[!is.na(x_coord) & !is.na(y_coord)],\n      sale_price_actual = .data[[y_var]]\n    ) %>%\n    filter(!is.na(residual))\n  \n  # Convert to sf object\n  coords <- df_spatial %>% select(x_coord, y_coord)\n  coords_sf <- st_as_sf(coords, coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n  \n  cat(sprintf(\"Valid samples: %d\\n\\n\", nrow(df_spatial)))\n  \n  # === 2.1 Moran’s I for Actual Sale Price ===\n  cat(\"--- Spatial Autocorrelation of Actual Sale Price ---\\n\")\n  \n  # Build spatial weight matrix (K-nearest neighbors, k=8)\n  nb_knn <- knn2nb(knearneigh(coords_sf, k = 8))\n  listw_knn <- nb2listw(nb_knn, style = \"W\")\n  \n  # Moran’s I test\n  moran_price <- moran.test(df_spatial$sale_price_actual, listw_knn)\n  \n  cat(sprintf(\"  Moran’s I = %.4f\\n\", moran_price$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_price$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_price$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_price$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_price$p.value))\n  \n  if (moran_price$p.value < 0.001) {\n    cat(\"  *** Highly significant positive spatial autocorrelation (p < 0.001)\\n\")\n  } else if (moran_price$p.value < 0.05) {\n    cat(\"  ** Significant positive spatial autocorrelation (p < 0.05)\\n\")\n  } else {\n    cat(\"  No significant spatial autocorrelation detected\\n\")\n  }\n  \n  # === 2.2 Moran’s I for Residuals ===\n  cat(\"\\n--- Spatial Autocorrelation of Model 4 Residuals ---\\n\")\n  \n  moran_resid <- moran.test(df_spatial$residual, listw_knn)\n  \n  cat(sprintf(\"  Moran’s I = %.4f\\n\", moran_resid$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_resid$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_resid$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_resid$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_resid$p.value))\n  \n  if (moran_resid$p.value < 0.001) {\n    cat(\"  ️ Strong residual spatial autocorrelation detected (p < 0.001)\\n\")\n    cat(\"  Suggestion: Consider Spatial Lag Model (SAR) or Spatial Error Model (SEM)\\n\")\n  } else if (moran_resid$p.value < 0.05) {\n    cat(\"  ️ Residual spatial autocorrelation detected (p < 0.05)\\n\")\n    cat(\"  Suggestion: Add more spatial variables or use spatial regression models\\n\")\n  } else {\n    cat(\"  ✓ No significant residual spatial autocorrelation – spatial effects well captured\\n\")\n  }\n  \n  # === 2.3 Moran Scatterplots ===\n  \n  # Calculate spatial lags\n  lag_price <- lag.listw(listw_knn, df_spatial$sale_price_actual)\n  lag_resid <- lag.listw(listw_knn, df_spatial$residual)\n  \n  # Standardize\n  price_std <- scale(df_spatial$sale_price_actual)[,1]\n  lag_price_std <- scale(lag_price)[,1]\n  resid_std <- scale(df_spatial$residual)[,1]\n  lag_resid_std <- scale(lag_resid)[,1]\n  \n  # Define quadrants\n  quadrant_price <- case_when(\n    price_std > 0 & lag_price_std > 0 ~ \"HH (High-High)\",\n    price_std < 0 & lag_price_std < 0 ~ \"LL (Low-Low)\",\n    price_std > 0 & lag_price_std < 0 ~ \"HL (High-Low)\",\n    TRUE ~ \"LH (Low-High)\"\n  )\n  \n  quadrant_resid <- case_when(\n    resid_std > 0 & lag_resid_std > 0 ~ \"HH\",\n    resid_std < 0 & lag_resid_std < 0 ~ \"LL\",\n    resid_std > 0 & lag_resid_std < 0 ~ \"HL\",\n    TRUE ~ \"LH\"\n  )\n  \n  moran_df_price <- data.frame(\n    value = price_std,\n    lag_value = lag_price_std,\n    quadrant = quadrant_price\n  )\n  \n  moran_df_resid <- data.frame(\n    value = resid_std,\n    lag_value = lag_resid_std,\n    quadrant = quadrant_resid\n  )\n  \n  # Plot – Sale Price\n  p_moran_price <- ggplot(moran_df_price, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH (High-High)\" = \"#67001F\", \n                 \"LL (Low-Low)\" = \"#053061\",\n                 \"HL (High-Low)\" = \"#F4A582\",\n                 \"LH (Low-High)\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran’s I = %.4f***\", moran_price$estimate[1]),\n             size = 5, fontface = \"bold\", color = \"#67001F\") +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran’s I Scatter Plot – Sale Price\",\n      subtitle = \"Strong positive spatial autocorrelation\",\n      x = \"Standardized Sale Price\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Plot – Residuals\n  p_moran_resid <- ggplot(moran_df_resid, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH\" = \"#67001F\", \"LL\" = \"#053061\",\n                 \"HL\" = \"#F4A582\", \"LH\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran’s I = %.4f%s\", \n                           moran_resid$estimate[1],\n                           ifelse(moran_resid$p.value < 0.001, \"***\",\n                                  ifelse(moran_resid$p.value < 0.05, \"**\", \"\"))),\n             size = 5, fontface = \"bold\", \n             color = ifelse(moran_resid$p.value < 0.05, \"#67001F\", \"gray50\")) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran’s I Scatter Plot – Model 4 Residuals\",\n      subtitle = ifelse(moran_resid$p.value < 0.05, \n                       \"Residual spatial autocorrelation detected\",\n                       \"No significant residual spatial autocorrelation\"),\n      x = \"Standardized Residual\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Combine plots\n  p_moran_combined <- (p_moran_price | p_moran_resid) +\n    plot_annotation(\n      title = \"Spatial Autocorrelation Analysis (Moran’s I)\",\n      subtitle = \"Left: Original prices show strong clustering | Right: Model residuals\",\n      theme = theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/morans_i_scatter.png\", p_moran_combined, \n         width = 16, height = 7, dpi = 300, bg = \"white\")\n  cat(\"\\n   plot/morans_i_scatter.png\\n\")\n  \n  # Save Moran’s I results\n  moran_summary <- data.frame(\n    Variable = c(\"Sale Price\", \"Model 4 Residuals\"),\n    Morans_I = c(moran_price$estimate[1], moran_resid$estimate[1]),\n    Expected = c(moran_price$estimate[2], moran_resid$estimate[2]),\n    Variance = c(moran_price$estimate[3], moran_resid$estimate[3]),\n    Z_score = c(moran_price$statistic, moran_resid$statistic),\n    P_value = c(moran_price$p.value, moran_resid$p.value),\n    Significant = c(moran_price$p.value < 0.05, moran_resid$p.value < 0.05)\n  )\n  \n  write_csv(moran_summary, \"file/morans_i_results.csv\")\n  cat(\"  ✓ file/morans_i_results.csv\\n\")\n  \n} else {\n  cat(\" Missing coordinate data – Moran’s I test cannot be performed\\n\")\n}\n```\n\n```{r}\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 4 Supplement Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\" Output Files:\\n\\n\")\ncat(\"Tables:\\n\")\nif (exists(\"influential_data\")) {\n  cat(\"  • file/influential_observations.csv – List of influential observations\\n\")\n}\nif (exists(\"moran_summary\")) {\n  cat(\"  • file/morans_i_results.csv – Moran’s I test results\\n\")\n}\n\ncat(\"\\nFigures:\\n\")\ncat(\"  • plot/cooks_distance.png – Cook’s Distance Scatter Plot\\n\")\nif (exists(\"p_moran_combined\")) {\n  cat(\"  • plot/morans_i_scatter.png – Moran’s I Scatter Plot\\n\")\n}\n\ncat(\"\\n Key Findings:\\n\")\nif (exists(\"moran_resid\")) {\n  if (moran_resid$p.value < 0.05) {\n    cat(\"  ️ Significant spatial autocorrelation detected in residuals\\n\")\n    cat(\"     → The model did not fully capture spatial effects\\n\")\n    cat(\"     → Suggest using spatial econometric models (SAR/SEM)\\n\")\n  } else {\n    cat(\"   No significant spatial autocorrelation in residuals\\n\")\n    cat(\"     → Fixed effects adequately controlled for spatial dependence\\n\")\n  }\n}\n\ncat(\"\\n\")\n\n```\n\n```{r}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(patchwork)\n\n# tidytext\nif (!require(\"tidytext\", quietly = TRUE)) {\n  reorder_within <- function(x, by, within, fun = mean, sep = \"___\", ...) {\n    new_x <- paste(x, within, sep = sep)\n    stats::reorder(new_x, by, FUN = fun)\n  }\n  \n  scale_x_reordered <- function(..., sep = \"___\") {\n    reg <- paste0(sep, \".+$\")\n    ggplot2::scale_x_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n  \n  scale_y_reordered <- function(..., sep = \"___\") {\n    reg <- paste0(sep, \".+$\")\n    ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n} else {\n  library(tidytext)\n}\n```\n\n```{r}\n# Read data\ndf <- read_csv(\"./table/feature_importance_all_models.csv\", show_col_types = FALSE)\n\n# Data cleaning\ndf <- df %>%\n  mutate(\n    estimate = as.numeric(estimate),\n    abs_estimate = as.numeric(abs_estimate)\n  )\n\n# Variable classification\ndf <- df %>%\n  mutate(\n    var_type = case_when(\n      grepl(\"census_tract|zip_code\", term) ~ \"Location Fixed Effects\",\n      grepl(\"income|POVERTY\", term) ~ \"Socioeconomic\",\n      grepl(\"bathroom|bedroom|livable_area\", term) ~ \"Structural\",\n      grepl(\"age\", term) ~ \"Age\",\n      grepl(\"coord|dist\", term) ~ \"Spatial\",\n      TRUE ~ \"Other\"\n    ),\n    # Clean variable names\n    term_clean = case_when(\n      term == \"number_of_bathrooms\" ~ \"Bathrooms\",\n      term == \"number_of_bedrooms\" ~ \"Bedrooms\",\n      term == \"total_livable_area\" ~ \"Livable Area\",\n      term == \"per_cap_incomeE\" ~ \"Per Capita Income\",\n      term == \"median_incomeE\" ~ \"Median Income\",\n      term == \"PCTPOVERTY\" ~ \"Poverty Rate\",\n      term == \"age2\" ~ \"Age²\",\n      term == \"dist_to_center\" ~ \"Distance to Center\",\n      term == \"x_coord\" ~ \"X Coordinate\",\n      term == \"y_coord\" ~ \"Y Coordinate\",\n      grepl(\"census_tract\", term) ~ gsub(\"census_tract\", \"Census Tract \", term),\n      grepl(\"zip_code\", term) ~ gsub(\"zip_code\", \"ZIP \", term),\n      TRUE ~ term\n    )\n  )\n\n# Keep only top 10 most important variables per model\ntop_features <- df %>%\n  group_by(Model) %>%\n  slice_max(abs_estimate, n = 10) %>%\n  ungroup()\n\n# Color palette (deep blue to deep red)\ncolor_palette <- c(\n  \"Structural\" = \"#67001F\",        # deep red\n  \"Socioeconomic\" = \"#4393C3\",     # medium blue  \n  \"Age\" = \"#2166AC\",               # darker blue\n  \"Spatial\" = \"#053061\",           # darkest blue\n  \"Location Fixed Effects\" = \"#D6604D\"  # reddish tone\n)\n\ncat(\"Generating visualization (using deep blue–deep red palette)...\\n\\n\")\n\n```\n\n```{r}\n# ========== Figure 1: Lollipop Chart ==========\ncat(\"  Generating Figure 1: Lollipop chart...\\n\")\n\np1 <- ggplot(top_features, aes(x = reorder_within(term_clean, abs_estimate, Model), \n                                y = abs_estimate, color = var_type)) +\n  geom_segment(aes(xend = reorder_within(term_clean, abs_estimate, Model), yend = 0), \n               size = 1.2, alpha = 0.7) +\n  geom_point(size = 4, alpha = 0.9) +\n  coord_flip() +\n  facet_wrap(~Model, scales = \"free_y\", ncol = 2) +\n  scale_x_reordered() +\n  scale_color_manual(values = color_palette, name = \"Feature Type\") +\n  labs(\n    title = \"Feature Importance Across Progressive Models\",\n    subtitle = \"Lollipop chart showing top 10 features per model\",\n    x = NULL,\n    y = \"Absolute Coefficient\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    strip.background = element_rect(fill = \"gray95\", color = NA),\n    legend.position = \"bottom\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray90\", size = 0.3),\n    axis.text.y = element_text(size = 9),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_lollipop_v2.png\", p1, \n       width = 14, height = 10, dpi = 300, bg = \"white\")\n\ncat(\"    ✓ Saved to: plot/feature_importance_lollipop_v2.png\\n\")\n\n```\n\n```{r}\n# ========== Figure 2: Heatmap ==========\ncat(\"  Generating Figure 2: Heatmap...\\n\")\n\nheatmap_data <- df %>%\n  group_by(Model) %>%\n  slice_max(abs_estimate, n = 8) %>%\n  ungroup() %>%\n  mutate(Model_short = gsub(\"M[0-9]: \", \"\", Model))\n\np2 <- ggplot(heatmap_data, aes(x = Model_short, y = fct_reorder(term_clean, abs_estimate), \n                                fill = abs_estimate)) +\n  geom_tile(color = \"white\", size = 1) +\n  geom_text(aes(label = sprintf(\"%.3f\", abs_estimate)), \n            color = \"white\", size = 3.5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(heatmap_data$abs_estimate),\n    name = \"Coefficient\\nMagnitude\"\n  ) +\n  labs(\n    title = \"Feature Importance Heatmap Across Models\",\n    subtitle = \"Top 8 features per model (darker = stronger effect)\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\", size = 11),\n    axis.text.y = element_text(size = 10),\n    legend.position = \"right\",\n    panel.grid = element_blank(),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_heatmap_v2.png\", p2, \n       width = 12, height = 8, dpi = 300, bg = \"white\")\n\ncat(\"    ✓ Saved to: plot/feature_importance_heatmap_v2.png\\n\")\n\n```\n\n\n```{r}\n# ========== Figure 3: Nightingale Rose Chart ==========\ncat(\"  Generating Figure 3: Nightingale rose chart...\\n\")\n\nmodels <- unique(top_features$Model)\n\nrose_plots <- lapply(models, function(m) {\n  data_m <- top_features %>%\n    filter(Model == m) %>%\n    arrange(desc(abs_estimate)) %>%\n    head(8) %>%\n    mutate(term_clean = factor(term_clean, levels = term_clean))\n  \n  ggplot(data_m, aes(x = term_clean, y = abs_estimate, fill = abs_estimate)) +\n    geom_col(width = 1, alpha = 0.9, color = \"white\", size = 0.5) +\n    coord_polar(theta = \"x\") +\n    scale_fill_gradient2(\n      low = \"#4393C3\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(data_m$abs_estimate),\n      name = \"Importance\"\n    ) +\n    labs(title = m, x = NULL, y = NULL) +\n    theme_minimal(base_size = 10) +\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5, size = 11),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(size = 12, face = \"bold\"),\n      panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      legend.position = \"none\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n})\n\np3 <- wrap_plots(rose_plots, ncol = 2) +\n  plot_annotation(\n    title = \"Feature Importance: Nightingale Rose Chart\",\n    subtitle = \"Top 8 features visualized as rose petals (larger petal = more important)\",\n    theme = theme(\n      plot.title = element_text(face = \"bold\", size = 18, hjust = 0.5),\n      plot.subtitle = element_text(size = 13, hjust = 0.5, color = \"gray40\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/feature_importance_rose_v2.png\", p3, \n       width = 14, height = 12, dpi = 300, bg = \"white\")\n\ncat(\"     Saved to: plot/feature_importance_rose_v2.png\\n\")\n\n```\n\n```{r}\n# Print Summary\ncat(\"\\n==============================================\\n\")\ncat(\"🎨 All visualizations have been successfully generated! (Deep Blue–Red Palette)\\n\")\ncat(\"==============================================\\n\\n\")\ncat(\"📊 Generated Figures:\\n\")\ncat(\"  1. feature_importance_lollipop_v2.png  - Lollipop Chart\\n\")\ncat(\"  2. feature_importance_heatmap_v2.png   - Heatmap\\n\")\ncat(\"  3. feature_importance_rose_v2.png      - Nightingale Rose Chart\\n\\n\")\ncat(\"🎨 Color Palette (following Moran's I visualization):\\n\")\ncat(\"  • Structural: Deep Red (#67001F)\\n\")\ncat(\"  • Socioeconomic: Deep Blue (#4393C3)\\n\")\ncat(\"  • Age: Darker Blue (#2166AC)\\n\")\ncat(\"  • Spatial: Deepest Blue (#053061)\\n\")\ncat(\"  • Location FE: Reddish Tone (#D6604D)\\n\\n\")\n\n```\n\n```{r}\n\n```\n","srcMarkdownNoYaml":"\n\n\nStep0\n\n\n\n```{r}\n# =========================================================\n# Step 1: Load libraries and data\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(stringr)\n\nopa_raw <- read_csv(\"data/opa_properties_public.csv\",\n                    na = c(\"\", \"NA\", \"NaN\", \"NULL\"),\n                    guess_max = 1e6)\n\ncat(\"Rows (loaded):\", nrow(opa_raw), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 2: Data cleaning and filtering\n# =========================================================\nopa_res <- opa_raw %>%\n  mutate(\n    sale_date = as_date(sale_date),\n    # Prices are standardized as numeric values: regardless of whether they were originally numbers or strings (including $ or commas).\n    sale_price_num = suppressWarnings(\n      coalesce(as.numeric(sale_price), readr::parse_number(as.character(sale_price)))\n    ),\n    # Categories are standardized to a consistent data type.\n    cat_chr = as.character(category_code)\n  ) %>%\n  filter(\n    cat_chr %in% c(\"1\"),  \n    sale_date >= as_date(\"2023-01-01\"),\n    sale_date <= as_date(\"2024-12-31\"),\n    sale_price_num >= 10000,\n    total_livable_area > 0,\n    year_built > 0,\n    number_of_bedrooms > 0,\n    number_of_bathrooms > 0,\n    !is.na(census_tract),\n    census_tract != 0,\n    census_tract != \"\",\n    !is.na(zip_code),\n    zip_code != \"\",\n    !is.na(exterior_condition),\n    exterior_condition != \"\",\n    !is.na(interior_condition),\n    interior_condition != \"\",\n    !is.na(shape),\n    shape != \"\"\n  ) %>%\n  select(\n    parcel_number, sale_date,\n    sale_price = sale_price_num,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, shape\n  ) %>%\n  distinct() %>%\n  drop_na()\n\ncat(\"Rows (after cleaning and filtering):\", nrow(opa_res), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 3: Extract coordinates from shape field (using sf)\n# =========================================================\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Remove the prefix SRID=2272\nwkt <- sub(\"^SRID=\\\\d+;\\\\s*\", \"\", opa_res$shape)\n\n# Convert to sf format.\ngeom <- st_as_sfc(wkt, crs = 2272)\nopa_sf <- st_sf(opa_res, geometry = geom)\n\n# Extract coordinates (for scatter plot).\ncoords <- st_coordinates(opa_sf)\nopa_sf$X <- coords[, 1]\nopa_sf$Y <- coords[, 2]\n\n# Debug output.\ncat(\"Rows with valid coordinates:\", nrow(opa_sf), \"\\n\")\ncat(\"Sample X coordinates:\", head(opa_sf$X, 3), \"\\n\")\ncat(\"Sample Y coordinates:\", head(opa_sf$Y, 3), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 4: Save cleaned data\n# =========================================================\n# Create output directory\ndir.create(here::here(\"data\"), recursive = TRUE, showWarnings = FALSE)\n\n# Export data (remove the geometry column, keeping only coordinates)\nopa_export <- opa_sf %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, x_coord = X, y_coord = Y\n  ) %>%\n  st_drop_geometry()  \n\nwrite_csv(opa_export, \"./data/opa_sales_2023_2024_residential_clean.csv\")\n\ncat(\"Cleaned data saved to: opa_sales_2023_2024_residential_clean.csv\\n\")\ncat(\"Final dataset contains\", nrow(opa_export), \"rows with\", ncol(opa_export), \"columns\\n\")\ncat(\"Columns:\", paste(names(opa_export), collapse = \", \"), \"\\n\")\n```\n\n```{r}\n# =========================================================\n# Step 5: Load, clean crime data and calculate crime count for properties\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(sf)\n\n# Load crime data for 2023 and 2024\ncrime_2023 <- read_csv(here(\"./data/crime_2023.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\ncrime_2024 <- read_csv(here(\"./data/crime_2024.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\n\n# Merge, clean, and process crime data\ncrime_clean <- bind_rows(\n  crime_2023 %>% mutate(year = 2023),\n  crime_2024 %>% mutate(year = 2024)\n) %>%\n  mutate(\n    dispatch_date_time = as_datetime(dispatch_date_time),\n    dispatch_date = as_date(dispatch_date),\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    point_x = as.numeric(point_x),\n    point_y = as.numeric(point_y),\n    ucr_general = as.numeric(ucr_general),\n    text_general_code = as.character(text_general_code)\n  ) %>%\n  filter(\n    !is.na(lat) & !is.na(lng),\n    lat != 0 & lng != 0,\n    dispatch_date >= as_date(\"2023-01-01\"),\n    dispatch_date <= as_date(\"2024-12-31\")\n  ) %>%\n  select(\n    objectid, dc_dist, psa, dispatch_date_time, dispatch_date, \n    dispatch_time, hour, dc_key, location_block, \n    ucr_general, text_general_code, \n    lat, lng, point_x, point_y, year\n  ) %>%\n  distinct() %>%\n  drop_na()\n\n# Coordinate transformation: convert from WGS84 (EPSG:4326) to EPSG:2272 coordinate system\ncrime_sf <- crime_clean %>%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %>%\n  st_transform(crs = 2272) %>%\n  mutate(\n    x_coord_2272 = st_coordinates(.)[, 1],\n    y_coord_2272 = st_coordinates(.)[, 2]\n  ) %>%\n  st_drop_geometry()\n\n# Read the cleaned real estate data\nopa_clean <- read_csv(here(\"./data/opa_sales_2023_2024_residential_clean.csv\"))\n\n# Convert the real estate data to an sf object (EPSG:2272)\nopa_sf <- opa_clean %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n\n# Convert the crime data to an sf object (EPSG:2272)\ncrime_sf_spatial <- crime_sf %>%\n  st_as_sf(coords = c(\"x_coord_2272\", \"y_coord_2272\"), crs = 2272)\n\n# Create a 0.75-mile buffer around each home and count crimes (15-minute walkshed)\nopa_buffers <- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\ncrime_count <- st_intersects(opa_buffers, crime_sf_spatial)\n\n# Add the crime counts to the real estate dataset\nopa_with_crime <- opa_clean %>%\n  mutate(\n    crime_count_15min_walk = sapply(crime_count, length)\n  )\n\ncat(\"Crime data processing completed:\\n\")\ncat(\"Number of 2023 crime records:\", nrow(crime_2023), \"\\n\")\ncat(\"Number of 2024 crime records:\", nrow(crime_2024), \"\\n\")\ncat(\"Number of valid crime records after cleaning:\", nrow(crime_clean), \"\\n\")\ncat(\"Number of records after coordinate transformation:\", nrow(crime_sf), \"\\n\")\ncat(\"Number of real estate records:\", nrow(opa_with_crime), \"\\n\")\ncat(\"Average number of crimes within a 15-minute walkshed per property:\", round(mean(opa_with_crime$crime_count_15min_walk), 2), \"\\n\")\ncat(\"Crime count range:\", min(opa_with_crime$crime_count_15min_walk), \"-\", max(opa_with_crime$crime_count_15min_walk), \"\\n\")\n\n```\n\n```{r}\n# =========================================================\n# Step 6: Add park accessibility metrics (distance in feet, keep coordinate columns)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# Use the crime data processing results from the previous step\nopa_with_crime <- opa_with_crime\n\n# Convert to an sf object (coordinate system: EPSG:2272, unit: feet)\nopa_sf <- st_as_sf(\n  opa_with_crime,\n  coords = c(\"x_coord\", \"y_coord\"),\n  crs = 2272\n)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\n\n# Read and project the park data.\nparks <- st_read(here(\"./data/PPR_Program_Sites.geojson\"), quiet = TRUE) %>%\n  st_transform(2272)\n\n# Calculate park accessibility metrics (unit: feet)\nopa_sf <- opa_sf %>%\n  mutate(\n    dist_to_park_ft = apply(st_distance(opa_sf, parks), 1, min),   \n    # Nearest park distance (feet).\n    park_within_15min_walk = lengths(st_within(opa_sf, st_buffer(parks, 3960)))  # 0.75 mile = 3960 feet\n  )\n\n# Extract coordinates to prevent st_drop_geometry from removing them\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export data (retain coordinates and newly added indicators)\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,              # keep coordinates\n    crime_count_15min_walk,        # number of crimes within 15-minute walkshed\n    dist_to_park_ft,               # nearest park distance (feet)\n    park_within_15min_walk         # number of parks within 15-minute walkshed\n  )\n\n# Output summary information\n# Save the complete dataset with park accessibility indicators\nwrite_csv(opa_export, here(\"./data/opa_sales_with_parks.csv\"))\n\ncat(\" Park accessibility indicators added (unit: feet)\\n\")\ncat(\"  Average nearest park distance:\", round(mean(opa_export$dist_to_park_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of parks within 15-minute walkshed:\", round(mean(opa_export$park_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\ncat(\"🎉 Data saved to: ./data/opa_sales_with_parks.csv\\n\")\n\n```\n\n```{r}\n# =========================================================\n# Step 7: Add public transit accessibility metrics\n# (distance to nearest stop & number of stops within 1,000 ft; also 15-min walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# Use the same CRS (EPSG:2272, units = feet)\nanalysis_crs <- 2272\n\n# Read the dataset saved in the previous step (with price, crime, and park metrics)\nopa_data <- read_csv(here(\"./data/opa_sales_with_parks.csv\"))\nopa_sf <- opa_data %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\n# Read and project transit stop data\ntransit_path <- here(\"./data/Transit_Stops_(Spring_2025).geojson\")\nstopifnot(file.exists(transit_path))\n\ntransit_stops <- st_read(transit_path, quiet = TRUE) %>%\n  st_transform(analysis_crs) %>%\n  suppressWarnings(st_collection_extract(\"POINT\")) %>%\n  filter(!st_is_empty(geometry)) %>%\n  distinct(geometry, .keep_all = TRUE)\n\ncat(\"Number of transit stops:\", nrow(transit_stops), \"\\n\")\n\n# Compute accessibility metrics (units: feet)\n# Distance to nearest transit stop\nnearest_idx <- st_nearest_feature(opa_sf, transit_stops)\ndist_ft <- st_distance(opa_sf, transit_stops[nearest_idx, ], by_element = TRUE)\nopa_sf$dist_transit_ft <- as.numeric(set_units(dist_ft, \"ft\"))\n\n# Count of transit stops within a 15-minute walkshed (0.75 mile = 3960 ft)\nbuffer_3960ft <- st_buffer(opa_sf, dist = 3960)\nopa_sf$transit_15min_walk <- lengths(st_intersects(buffer_3960ft, transit_stops))\n\n# (Optional) Count of transit stops within 1,000 ft\nbuffer_1000ft <- st_buffer(opa_sf, dist = 1000)\nopa_sf$transit_within_1000ft <- lengths(st_intersects(buffer_1000ft, transit_stops))\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and newly added indicators)\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                 # keep coordinates\n    crime_count_15min_walk,           # crime metric\n    dist_to_park_ft, park_within_15min_walk, # park metrics\n    dist_transit_ft, transit_15min_walk,     # transit metrics (walkshed)\n    transit_within_1000ft                    # transit stops within 1,000 ft\n  )\n\n# Output summary information\n# Save the complete dataset with transit metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_transit.csv\"))\n\ncat(\"  Public transit metrics added\\n\")\ncat(\"  Average distance to nearest transit stop:\", round(mean(opa_export$dist_transit_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of transit stops within 15-minute walkshed:\", round(mean(opa_export$transit_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Average number of transit stops within 1,000 ft:\", round(mean(opa_export$transit_within_1000ft, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\ncat(\"🎉 Data saved to: ./data/opa_sales_with_transit.csv\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Step 8: Add hospital accessibility metrics\n# (distance to nearest hospital & count within 0.75-mile walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# CRS: EPSG:2272 (units: feet)\nanalysis_crs <- 2272\n\n# Read the dataset saved in the previous step (with price, crime, park, and transit metrics)\nopa_data <- read_csv(here(\"./data/opa_sales_with_transit.csv\"))\nopa_sf <- opa_data %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\n# Read hospital data and project to the same CRS\nhospitals <- st_read(here(\"./data/Hospitals.geojson\"), quiet = TRUE) %>%\n  st_transform(analysis_crs)\n\ncat(\"Number of hospitals:\", nrow(hospitals), \"\\n\")\n\n# Compute hospital accessibility metrics (units: feet)\nopa_sf <- opa_sf %>%\n  mutate(\n    # Distance to nearest hospital (feet)\n    dist_to_hospital_ft = as.numeric(apply(st_distance(opa_sf, hospitals), 1, min)),\n    # Number of hospitals within a 15-minute walkshed (0.75 mile = 3960 ft)\n    hospitals_15min_walk = lengths(st_within(opa_sf, st_buffer(hospitals, 3960)))\n  )\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and all features)\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                         # keep coordinates\n    crime_count_15min_walk,                   # crime metrics\n    dist_to_park_ft, park_within_15min_walk,  # park metrics\n    dist_transit_ft, transit_15min_walk,      # transit metrics\n    dist_to_hospital_ft, hospitals_15min_walk # hospital metrics (new)\n  )\n\n# Output summary information\n# Save the complete dataset with hospital metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_hospitals.csv\"))\n\ncat(\"  Hospital accessibility metrics added\\n\")\ncat(\"  Average distance to nearest hospital:\", round(mean(opa_export$dist_to_hospital_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of hospitals within 15-minute walkshed:\", round(mean(opa_export$hospitals_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\ncat(\"  Includes all features: real estate info + crime + park + transit + hospital\\n\")\ncat(\"  Data saved to: ./data/opa_sales_with_hospitals.csv\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Step 9: Enrich with ACS (Census) Socioeconomic Indicators\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(readr)\nlibrary(here)\n\n# ---------------------------------------------------------\n# Read the final property data (includes all accessibility metrics)\n# ---------------------------------------------------------\nopa_final <- read_csv(here(\"./data/opa_sales_with_hospitals.csv\"))\n\n# Convert to sf object (ensure CRS is EPSG:2272)\nopa_sf <- opa_final %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\n\n# ---------------------------------------------------------\n# Download ACS data for Philadelphia (year can be changed)\n# ---------------------------------------------------------\nyear_acs <- 2022\ncensus_api_key(\"86993dedbe98d77b9d79db6b8ba21a7fde55cb91\", install = FALSE)\n\nacs_vars <- c(\n  total_pop      = \"B01003_001\",\n  median_income  = \"B19013_001\",\n  per_cap_income = \"B19301_001\",\n  below_pov      = \"B17001_002\",\n  edu_total25    = \"B15003_001\",\n  edu_bach       = \"B15003_022\",\n  edu_mast       = \"B15003_023\",\n  edu_prof       = \"B15003_024\",\n  edu_phd        = \"B15003_025\"\n)\n\nphl_acs <- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = year_acs,\n  geometry = TRUE,\n  output = \"wide\",\n  variables = acs_vars\n) %>%\n  mutate(\n    PCBACHMORE = 100 * ((edu_bachE + edu_mastE + edu_profE + edu_phdE) / edu_total25E),\n    PCTPOVERTY = 100 * (below_povE / total_popE)\n  ) %>%\n  select(geometry, GEOID, total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY)\n\n# ---------------------------------------------------------\n# Reproject to align (EPSG:2272)\n# ---------------------------------------------------------\nphl_acs <- st_transform(phl_acs, 2272)\n\n# ---------------------------------------------------------\n# Spatial join: assign each home to the tract it falls within\n# ---------------------------------------------------------\nopa_joined <- st_join(\n  opa_sf,\n  phl_acs,\n  join = st_within,\n  left = TRUE\n)\n\n# ---------------------------------------------------------\n# For out-of-bound samples (occasional NAs), fill with nearest tract\n# ---------------------------------------------------------\nmissing <- is.na(opa_joined$median_incomeE)\nif (any(missing)) {\n  idx <- st_nearest_feature(opa_joined[missing, ], phl_acs)\n  repl <- phl_acs[idx, ] %>% st_drop_geometry()\n  cols <- names(repl)\n  opa_joined[missing, cols] <- repl\n}\n\n# ---------------------------------------------------------\n# Export results\n# ---------------------------------------------------------\nopa_export <- opa_joined %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  # crime\n    dist_to_park_ft, park_within_15min_walk, # parks\n    dist_transit_ft, transit_15min_walk,     # transit\n    dist_to_hospital_ft, hospitals_15min_walk, # hospitals\n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY  # ACS\n  )\n\n# Save the complete dataset with Census indicators\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  ACS socioeconomic indicators added\\n\")\ncat(\"  Mean household income (USD):\", round(mean(opa_export$median_incomeE, na.rm = TRUE), 0), \"\\n\")\ncat(\"  Mean poverty rate (%):\", round(mean(opa_export$PCTPOVERTY, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Mean share with bachelor's degree or higher (%):\", round(mean(opa_export$PCBACHMORE, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Includes all features: price + crime + parks + transit + hospitals + socioeconomic\\n\")\ncat(\"  Data saved to: opa_sales_final_complete.csv\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Step 10: Add Education Accessibility Indicators (Schools)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# ---------------------------------------------------------\n# 1.Read the complete dataset saved from the previous step (including Census and accessibility features)\n# ---------------------------------------------------------\nopa_data <- read_csv(here(\"opa_sales_final_complete.csv\"))\nopa_sf <- opa_data %>%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\ncat(\"Data column names:\", paste(names(opa_sf), collapse = \", \"), \"\\n\")\n\n# ---------------------------------------------------------\n# 2.Read the school data (only one file)）\n# ---------------------------------------------------------\nschools <- st_read(here(\"./data/Schools_Parcels.geojson\"), quiet = TRUE) %>%\n  st_transform(2272) %>%\n  filter(!st_is_empty(geometry))  \n\ncat(\"School data loaded:\", nrow(schools), \"records\\n\")\ncat(\"School data coordinate system:\", st_crs(schools)$input, \"\\n\")\ncat(\"Property data coordinate system:\", st_crs(opa_sf)$input, \"\\n\")\n\n# ---------------------------------------------------------\n# 3.Calculate education accessibility indicators\n# ---------------------------------------------------------\n# Distance to the nearest school (in feet)\ncat(\"Starting to calculate the nearest school distance...\\n\")\ndist_matrix <- st_distance(opa_sf, schools)\ncat(\"Distance matrix dimensions:\", dim(dist_matrix), \"\\n\")\nopa_sf$dist_to_nearest_school_ft <- as.numeric(apply(dist_matrix, 1, min))\ncat(\"Nearest school distance calculation completed, range:\",\n    min(opa_sf$dist_to_nearest_school_ft), \"-\", \n    max(opa_sf$dist_to_nearest_school_ft), \"feet\\n\")\n\n# Number of schools within a 15-minute walking distance\ncat(\"Starting to calculate the number of schools within a 15-minute walking distance...\\n\")\nbuffer_3960ft <- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\nopa_sf$schools_within_15min_walk <- lengths(st_intersects(buffer_3960ft, schools))\ncat(\"Calculation of schools within a 15-minute walking distance completed, range:\",\n    min(opa_sf$schools_within_15min_walk), \"-\", \n    max(opa_sf$schools_within_15min_walk), \"\\n\")\n\n\n# ---------------------------------------------------------\n# 4.Extract coordinates and retain all columns\n# ---------------------------------------------------------\ncoords <- st_coordinates(opa_sf)\nopa_sf <- opa_sf %>%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# ---------------------------------------------------------\n# 5.Export the complete table with education accessibility indicators\n# ---------------------------------------------------------\nopa_export <- opa_sf %>%\n  st_drop_geometry() %>%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  \n    dist_to_park_ft, park_within_15min_walk, \n    dist_transit_ft, transit_15min_walk,     \n    dist_to_hospital_ft, hospitals_15min_walk, \n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY,  # Census\n    dist_to_nearest_school_ft, schools_within_15min_walk                  \n  )\n\n# ---------------------------------------------------------\n# 6.Clean the data: remove all rows containing empty or NA values\n# ---------------------------------------------------------\nrows_before <- nrow(opa_export)\n\nopa_export <- opa_export %>%\n  mutate(across(everything(), ~ {\n    if (is.character(.x)) {\n      clean_val <- trimws(tolower(as.character(.x)))\n      ifelse(clean_val == \"\" | clean_val == \"na\" | clean_val == \"n/a\" | clean_val == \"null\", \n             NA, .x)\n    } else {\n      .x\n    }\n  })) %>%\n  drop_na()\n\nrows_after <- nrow(opa_export)\nrows_removed <- rows_before - rows_after\n\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  Education accessibility indicators have been added\\n\")\ncat(\"  Average distance to the nearest school:\", \n    round(mean(opa_export$dist_to_nearest_school_ft, na.rm = TRUE), 1), \"ft\\n\")\ncat(\"  Average number of schools within a 15-minute walking distance:\", \n    round(mean(opa_export$schools_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\ncat(\"  Data cleaning completed: removed\", rows_removed, \"rows containing empty or NA values\\n\")\ncat(\"  Before cleaning:\", rows_before, \"rows → After cleaning:\", rows_after, \"rows\\n\")\ncat(\"  Final complete dataset saved to: opa_sales_final_complete.csv\\n\")\ncat(\"  Includes all features: housing price + crime + parks + transit + hospitals + Census + schools\\n\")\ncat(\"  Number of columns:\", ncol(opa_export), \"(including coordinate columns)\\n\")\n\n\n```\n\n\n\n\n\nStep1\n\n\n\n\n```{r}\n# =========================================================\n# Step 1: Skewness Detection + Log Transformation + Descriptive Statistics\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(e1071)\nlibrary(patchwork)\nlibrary(readr)\nlibrary(tidyr)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"📊 Step 1: Data Cleaning, Transformation, and Descriptive Statistics\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n# === Read Data ===\ndf <- read_csv(\"opa_sales_final_complete.csv\", show_col_types = FALSE)\ncat(sprintf(\"Original sample size: %d\\n\", nrow(df)))\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\n\n# === Basic Processing ===\nif (\"year_built\" %in% names(df)) {\n  df <- df %>%\n    mutate(age = 2025 - year_built,\n           age2 = age^2) %>%\n    dplyr::select(-year_built)\n}\n\ncat_vars <- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncoord_vars <- c(\"x_coord\", \"y_coord\")\ndf[cat_vars] <- lapply(df[cat_vars], factor)\n\n# === Skewness Detection ===\ncat(\"=== Skewness Detection ===\\n\")\nnum_vars <- df %>% dplyr::select(where(is.numeric))\nskews <- sapply(num_vars, function(x) {\n  if (all(is.na(x)) || sd(x, na.rm = TRUE) == 0) return(0)\n  skewness(x, na.rm = TRUE)\n})\nlogged_vars <- names(skews[abs(skews) > 1])\ncat(sprintf(\"Variables with |skew| > 1: %d\\n\", length(logged_vars)))\n\n# === Apply log1p Transformation ===\ndf_trans <- df\nfor (v in logged_vars) {\n  df_trans[[v]] <- log1p(pmax(df[[v]], 0))\n}\n\n```\n\n```{r}\n# =========================================================\n# 1. Descriptive Statistics Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating Descriptive Statistics Table ===\\n\\n\")\n\n# Identify target variable\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df_trans) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df_trans) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df_trans) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n# Select key numeric variables\nkey_vars <- c(\n  y_var,\n  \"total_livable_area\", \"number_of_bedrooms\", \"number_of_bathrooms\", \"age\",\n  \"median_incomeE\", \"per_cap_incomeE\", \"PCTPOVERTY\", \"PCBACHMORE\"\n)\nkey_vars <- intersect(key_vars, names(df_trans))\n\nif (length(key_vars) > 0) {\n  # Compute descriptive statistics (more meaningful on pre-transformation data)\n  desc_stats <- df %>%\n    dplyr::select(all_of(key_vars)) %>%\n    summarise(across(everything(), list(\n      N = ~sum(!is.na(.)),\n      Mean = ~mean(., na.rm = TRUE),\n      SD = ~sd(., na.rm = TRUE),\n      Min = ~min(., na.rm = TRUE),\n      Q25 = ~quantile(., 0.25, na.rm = TRUE),\n      Median = ~median(., na.rm = TRUE),\n      Q75 = ~quantile(., 0.75, na.rm = TRUE),\n      Max = ~max(., na.rm = TRUE)\n    ), .names = \"{.col}_{.fn}\")) %>%\n    pivot_longer(everything(), names_to = \"stat\", values_to = \"value\") %>%\n    separate(stat, into = c(\"Variable\", \"Statistic\"), sep = \"_(?=[^_]+$)\") %>%\n    pivot_wider(names_from = Statistic, values_from = value) %>%\n    dplyr::select(Variable, N, Mean, SD, Min, Q25, Median, Q75, Max) %>%\n    mutate(across(c(Mean, SD, Min, Q25, Median, Q75, Max), ~round(., 3)))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\")\n  write_csv(desc_stats, \"file/descriptive_statistics.csv\")\n  cat(\"  ✓ file/descriptive_statistics.csv\\n\")\n  print(as.data.frame(desc_stats), row.names = FALSE)\n}\n\n\n```\n\n```{r}\n# =========================================================\n# 2. Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating distribution comparison plots ===\\n\\n\")\n\nplot_list <- lapply(names(num_vars), function(v) {\n  is_transformed <- v %in% logged_vars\n  \n  # Original distribution\n  p1 <- ggplot(df, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = \"#053061\", color = \"white\", alpha = 0.85) +\n    labs(title = paste0(v, \" (Original)\"), x = NULL, y = \"Count\") +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Transformed distribution\n  fill_color <- if (is_transformed) \"#67001F\" else \"#4393C3\"\n  \n  p2 <- ggplot(df_trans, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = fill_color, color = \"white\", alpha = 0.85) +\n    labs(\n      title = paste0(v, if (is_transformed) \" (Log-transformed)\" else \" (No transformation)\"),\n      x = NULL, y = \"Count\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Add skewness annotation\n  skew_val <- round(skews[v], 2)\n  p1 <- p1 + annotate(\"text\", x = Inf, y = Inf, \n                      label = paste0(\"Skewness: \", skew_val),\n                      hjust = 1.1, vjust = 1.5, size = 2.5, color = \"gray30\")\n  \n  pair_plot <- (p1 | p2) + plot_layout(widths = c(1, 1)) &\n    theme(plot.margin = margin(3, 3, 3, 3))\n  \n  wrap_elements(pair_plot) + \n    theme(\n      plot.background = element_rect(fill = \"white\", color = \"#B0B0B0\", linewidth = 1.2),\n      plot.margin = margin(6, 6, 6, 6)\n    )\n})\n\nall_plot <- wrap_plots(plot_list, ncol = 2) + \n  plot_annotation(\n    title = \"Variable Distributions: Original vs. Transformed\",\n    subtitle = paste0(\n      \"Dark Blue = Original | Dark Red = Log-transformed (|skew| > 1) | Medium Blue = No transformation\"\n    ),\n    theme = theme(\n      plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray30\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\n```\n\n```{r}\n# Skewness Summary Plot\nskew_df <- data.frame(\n  variable = names(skews),\n  skewness = skews,\n  transformed = names(skews) %in% logged_vars\n) %>% arrange(desc(abs(skewness)))\n\np_skew <- ggplot(skew_df, aes(x = reorder(variable, abs(skewness)), y = skewness, fill = transformed)) +\n  geom_col(alpha = 0.85) +\n  geom_hline(yintercept = c(-1, 1), linetype = \"dashed\", color = \"#67001F\", linewidth = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  scale_fill_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"No transformation\", \"TRUE\" = \"Log-transformed\"),\n    name = NULL\n  ) +\n  coord_flip() +\n  labs(\n    title = \"Skewness of All Variables\",\n    subtitle = \"Variables with |skewness| > 1 are log-transformed\",\n    x = \"Variable\", y = \"Skewness\",\n    caption = \"Dashed lines indicate skewness thresholds at ±1\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\n```\n\n```{r}\n# =========================================================\n# 3. Categorical Variable Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating categorical variable distribution plots ===\\n\\n\")\n\ncat_vars_viz <- c(\"interior_condition\", \"exterior_condition\")\ncat_vars_viz <- intersect(cat_vars_viz, names(df_trans))\n\nif (length(cat_vars_viz) > 0 && !is.na(y_var)) {\n  plot_list_cat <- list()\n  \n  for (var in cat_vars_viz) {\n    cat_summary <- df_trans %>%\n      group_by(!!sym(var)) %>%\n      summarise(\n        count = n(),\n        mean_price = mean(.data[[y_var]], na.rm = TRUE),\n        .groups = \"drop\"\n      ) %>%\n      filter(!is.na(!!sym(var))) %>%\n      arrange(desc(mean_price))\n    \n    if (nrow(cat_summary) > 0) {\n      p <- ggplot(cat_summary, aes(reorder(!!sym(var), mean_price), mean_price, fill = mean_price)) +\n        geom_col(alpha = 0.9) +\n        geom_text(aes(label = count), vjust = -0.5, size = 3) +\n        scale_fill_gradient2(\n          low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n          midpoint = median(cat_summary$mean_price, na.rm = TRUE),\n          guide = \"none\"\n        ) +\n        coord_flip() +\n        theme_minimal(base_size = 10) +\n        theme(\n          panel.grid.major.y = element_blank(),\n          plot.title = element_text(hjust = 0.5, face = \"bold\", size = 12),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA)\n        ) +\n        labs(\n          title = gsub(\"_\", \" \", toupper(var)),\n          x = NULL,\n          y = \"Average Sale Price (log)\",\n          caption = \"Numbers indicate the count in each category\"\n        )\n      \n      plot_list_cat[[var]] <- p\n    }\n  }\n  \n  if (length(plot_list_cat) > 0) {\n    p_cat_combined <- wrap_plots(plot_list_cat, ncol = 2) +\n      plot_annotation(\n        title = \"Average Sale Price by Property Condition\",\n        subtitle = \"Log-transformed sale prices\",\n        theme = theme(\n          plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n          plot.subtitle = element_text(size = 11, hjust = 0.5),\n          plot.background = element_rect(fill = \"white\", color = NA)\n        )\n      )\n    \n    if (!dir.exists(\"plot\")) dir.create(\"plot\")\n    ggsave(\"plot/categorical_price_comparison.png\", p_cat_combined,\n           width = 12, height = 6, dpi = 300, bg = \"white\")\n    cat(\"  ✓ plot/categorical_price_comparison.png\\n\")\n  }\n}\n```\n\n```{r}\n# =========================================================\n# Output Files\n# =========================================================\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nif (!dir.exists(\"file\")) dir.create(\"file\")\n\nggsave(\"plot/all_variables_distribution.png\", plot = all_plot, width = 20, height = 12, dpi = 300, bg = \"white\")\nggsave(\"plot/skewness_summary.png\", plot = p_skew, width = 10, height = 8, dpi = 300, bg = \"white\")\nwriteLines(logged_vars, \"file/logged_variables.txt\")\nwrite_csv(df_trans, \"file/opa_sales_step1_clean.csv\")\n\ntransform_summary <- data.frame(\n  Variable = names(skews),\n  Original_Skewness = round(skews, 3),\n  Transformed = names(skews) %in% logged_vars\n) %>% arrange(desc(abs(Original_Skewness)))\n\nwrite_csv(transform_summary, \"file/transformation_summary.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 1 Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\" Output Files:\\n\\n\")\ncat(\"Tables:\\n\")\ncat(\"  • file/descriptive_statistics.csv - Descriptive statistics\\n\")\ncat(\"  • file/transformation_summary.csv - Transformation summary\\n\")\ncat(\"  • file/logged_variables.txt - List of log-transformed variables\\n\")\ncat(\"  • file/opa_sales_step1_clean.csv - Cleaned dataset\\n\\n\")\n\ncat(\"Figures:\\n\")\ncat(\"  • plot/all_variables_distribution.png - All variables: original vs. transformed\\n\")\ncat(\"  • plot/skewness_summary.png - Skewness summary\\n\")\ncat(\"  • plot/categorical_price_comparison.png - Categorical variables vs. sale price\\n\\n\")\n\ncat(\"📈 Stats:\\n\")\ncat(sprintf(\"  • Variables with |skew| > 1: %d\\n\", length(logged_vars)))\ncat(sprintf(\"  • Final number of variables: %d\\n\", ncol(df_trans)))\ncat(sprintf(\"  • Final sample size: %d\\n\\n\", nrow(df_trans)))\n\n```\n\n\n\nStep2\n\n\n\n\n```{r}\n# =========================================================\n# Step 2 Enhanced: Correlation Matrix + VIF + Spatial Visualization + Scatterplots\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(reshape2)\nlibrary(readr)\nlibrary(patchwork)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"  Step 2: Correlation Analysis, Multicollinearity, and Spatial Exploration\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n# === Read Data ===\ndf <- read_csv(\"file/opa_sales_step1_clean.csv\", show_col_types = FALSE)\n\ncat(sprintf(\"Sample size: %d\\n\", nrow(df)))\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\n# === Auto-detect target variable ===\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat_vars <- intersect(c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\"), names(df))\ncoord_vars <- intersect(c(\"x_coord\", \"y_coord\"), names(df))\ndf[cat_vars] <- lapply(df[cat_vars], factor)\n\n```\n\n```{r}\n# =========================================================\n# 1. Correlation Matrix\n# =========================================================\n\ncat(\"=== Generating correlation matrix ===\\n\")\n\nnum_df <- df %>% dplyr::select(where(is.numeric))\nnum_df <- num_df[, sapply(num_df, function(x) sd(x, na.rm = TRUE) > 0), drop = FALSE]\n\nif (ncol(num_df) > 1) {\n  corr_mat <- cor(num_df, use = \"pairwise.complete.obs\")\n  corr_mat[upper.tri(corr_mat)] <- NA\n  corr_melt <- melt(corr_mat, na.rm = TRUE)\n  \n  # If there are too many variables, keep only the first 50\n  if (ncol(num_df) > 50) {\n    vars_top <- names(num_df)[1:50]\n    corr_melt <- corr_melt %>% filter(Var1 %in% vars_top & Var2 %in% vars_top)\n  }\n  \n  p_corr <- ggplot(corr_melt, aes(Var2, Var1, fill = value, size = abs(value))) +\n    geom_point(shape = 21, color = \"white\", stroke = 0.5) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0, limits = c(-1, 1), name = NULL,\n      breaks = seq(-1, 1, 0.2)\n    ) +\n    scale_size_continuous(range = c(0.5, 10), guide = \"none\") +\n    scale_x_discrete(position = \"top\") +\n    scale_y_discrete(limits = rev) +\n    theme_minimal(base_size = 11) +\n    theme(\n      axis.text.x.top = element_text(angle = 45, hjust = 0, vjust = 0, size = 9, color = \"black\"),\n      axis.text.y = element_text(size = 9, color = \"black\"),\n      axis.title = element_blank(),\n      panel.grid.major = element_line(color = \"#E0E0E0\", linewidth = 0.5),\n      panel.grid.minor = element_blank(),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"bottom\",\n      legend.direction = \"horizontal\",\n      legend.key.width = unit(3, \"cm\"),\n      legend.key.height = unit(0.4, \"cm\"),\n      plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n      plot.margin = margin(10, 10, 10, 10)\n    ) +\n    labs(title = \"Correlation Matrix\") +\n    coord_fixed()\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n  ggsave(\"plot/corr_matrix_enhanced.png\", p_corr, width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/corr_matrix_enhanced.png\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# 2. VIF Analysis\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== VIF Analysis ===\\n\\n\")\n\nvif_vars <- setdiff(names(df), c(y_var, cat_vars, coord_vars))\nnum_vif <- df %>% dplyr::select(all_of(vif_vars)) %>% dplyr::select(where(is.numeric))\nnum_vif <- num_vif[, sapply(num_vif, function(x) sd(x, na.rm = TRUE) > 0), drop = FALSE]\n\nif (ncol(num_vif) >= 2) {\n  df_vif <- df %>% dplyr::select(all_of(c(y_var, names(num_vif)))) %>% na.omit()\n  f_vif <- as.formula(paste(y_var, \"~ .\"))\n  vif_model <- lm(f_vif, data = df_vif)\n  vif_vals <- car::vif(vif_model)\n  vif_tbl <- data.frame(variable = names(vif_vals), VIF = round(vif_vals, 3))\n  vif_tbl <- vif_tbl %>% arrange(desc(VIF))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\n  write_csv(vif_tbl, \"file/vif_values.csv\")\n  cat(\"  ✓ file/vif_values.csv\\n\")\n  \n  # High VIF warning\n  high_vif <- vif_tbl %>% filter(VIF > 10)\n  if (nrow(high_vif) > 0) {\n    cat(\"\\n  Variables with high multicollinearity (VIF > 10):\\n\")\n    print(as.data.frame(high_vif), row.names = FALSE)\n  }\n  \n  # VIF visualization\n  vif_top20 <- vif_tbl %>% top_n(20, VIF)\n  \n  p_vif <- ggplot(vif_top20, aes(x = reorder(variable, VIF), y = VIF, fill = VIF)) +\n    geom_col(alpha = 0.9) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 5, name = \"VIF Value\"\n    ) +\n    geom_hline(yintercept = 10, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n    geom_hline(yintercept = 5, linetype = \"dashed\", color = \"gray50\", linewidth = 0.7) +\n    coord_flip() +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.major.y = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Variance Inflation Factor (VIF) - Top 20\",\n      subtitle = \"Color gradient: Blue (low VIF) → Red (high VIF)\",\n      x = \"Variable\", y = \"VIF Value\",\n      caption = \"Dark red line: VIF=10 (High multicollinearity) | Gray line: VIF=5 (Moderate)\"\n    )\n  \n  ggsave(\"plot/vif_analysis.png\", p_vif, width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/vif_analysis.png\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# 3. Spatial Visualization\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Spatial Distribution Visualization ===\\n\\n\")\n  \n  # 3.1 Spatial distribution of sale prices\n  p_spatial_price <- ggplot(df, aes(x_coord, y_coord, color = .data[[y_var]])) +\n    geom_point(alpha = 0.6, size = 0.8) +\n    scale_color_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(df[[y_var]], na.rm = TRUE),\n      name = \"Sale Price\\n(log)\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"right\"\n    ) +\n    labs(\n      title = \"Spatial Distribution of Sale Prices\",\n      subtitle = \"Philadelphia housing market\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_price_distribution.png\", p_spatial_price,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_price_distribution.png\\n\")\n  \n  # 3.2 Hexbin density map\n  p_hex <- ggplot(df, aes(x_coord, y_coord)) +\n    geom_hex(aes(fill = after_stat(count)), bins = 40) +\n    scale_fill_gradient(\n      low = \"#F7F7F7\", high = \"#67001F\",\n      name = \"Count\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Housing Density Heatmap\",\n      subtitle = \"Hexagonal binning of property locations\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_density_hexbin.png\", p_hex,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_density_hexbin.png\\n\")\n}\n```\n\n```{r}\n# =========================================================\n# 4. Key Variable Scatterplots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Scatterplots: Key Variables vs. Sale Price ===\\n\\n\")\n\nkey_numeric <- c(\"total_livable_area\", \"median_incomeE\", \"age\", \"number_of_bathrooms\")\nkey_numeric <- intersect(key_numeric, names(df))\n\nif (length(key_numeric) >= 4) {\n  scatter_plots <- list()\n  \n  for (var in key_numeric[1:4]) {\n    # Correlation coefficient\n    corr_val <- cor(df[[var]], df[[y_var]], use = \"pairwise.complete.obs\")\n    \n    p <- ggplot(df, aes(.data[[var]], .data[[y_var]])) +\n      geom_point(alpha = 0.3, size = 0.8, color = \"#053061\") +\n      geom_smooth(method = \"lm\", se = TRUE, color = \"#67001F\", linewidth = 1.2) +\n      annotate(\"text\", x = Inf, y = -Inf, \n               label = sprintf(\"r = %.3f\", corr_val),\n               hjust = 1.1, vjust = -0.5, size = 4, color = \"#67001F\", fontface = \"bold\") +\n      theme_minimal(base_size = 10) +\n      theme(\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 11),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      ) +\n      labs(\n        title = gsub(\"_\", \" \", var),\n        x = gsub(\"_\", \" \", var),\n        y = \"Sale Price (log)\"\n      )\n    \n    scatter_plots[[var]] <- p\n  }\n  \n  p_scatter <- wrap_plots(scatter_plots, ncol = 2) +\n    plot_annotation(\n      title = \"Key Variables vs. Sale Price\",\n      subtitle = \"Linear fit (OLS) with correlation coefficients\",\n      theme = theme(\n        plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 11, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/key_variables_scatter.png\", p_scatter,\n         width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/key_variables_scatter.png\\n\")\n}\n\n```\n\n\n```{r}\n# =========================================================\n# Save cleaned data\n# =========================================================\n\nwrite_csv(df, \"file/opa_sales_step2_clean.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"  Step 2 Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\"  Output files:\\n\\n\")\ncat(\"Tables:\\n\")\ncat(\"  • file/vif_values.csv - VIF analysis results\\n\")\ncat(\"  • file/opa_sales_step2_clean.csv - Cleaned dataset\\n\\n\")\n\ncat(\"Figures:\\n\")\ncat(\"  • plot/corr_matrix_enhanced.png - Correlation matrix\\n\")\ncat(\"  • plot/vif_analysis.png - VIF analysis\\n\")\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  • plot/spatial_price_distribution.png - Spatial distribution of sale prices\\n\")\n  cat(\"  • plot/spatial_density_hexbin.png - Housing density heatmap\\n\")\n}\nif (length(key_numeric) >= 4) {\n  cat(\"  • plot/key_variables_scatter.png - Key variables vs. sale price (scatterplots)\\n\")\n}\ncat(\"\\n\")\n```\n\n\n\n\n\nStep3\n\n\n\n\n\n```{r}\n# =========================================================\n# Step 3: VIF-based Filtering + LASSO Feature Selection (Streamlined)\n# =========================================================\n\nlibrary(glmnet)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\nset.seed(2025)\n\n# === Read data and VIF results ===\ndf <- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nvif_path <- \"file/vif_values.csv\"\nif (!file.exists(vif_path)) stop(\"  file/vif_values.csv not found. Please run step2.R first.\")\nvif_table <- read_csv(vif_path, show_col_types = FALSE)\n\n# === Determine target variable ===\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\"  Target variable sale_price_log or sale_price not found\")\n\n# === Variable filtering (VIF) ===\nvif_threshold <- 5\nvars_keep <- vif_table %>%\n  filter(VIF <= vif_threshold) %>%\n  pull(variable)\nvars_keep <- intersect(vars_keep, names(df))\n\ncat(\"  Number of variables passing VIF ≤\", vif_threshold, \":\", length(vars_keep), \"\\n\")\n```\n\n```{r}\n\n# === Prepare data matrix ===\ndf_lasso <- df %>%\n  dplyr::select(all_of(c(y_var, vars_keep))) %>%\n  na.omit()\n\nx <- as.matrix(df_lasso %>% dplyr::select(-all_of(y_var)))\ny <- df_lasso[[y_var]]\n\n# === Run LASSO regression ===\ncvfit <- cv.glmnet(\n  x, y,\n  alpha = 1,            # LASSO\n  nfolds = 10,\n  standardize = TRUE\n)\n\nlambda_min <- cvfit$lambda.min\nlambda_1se <- cvfit$lambda.1se\ncat(\"λ_min =\", signif(lambda_min, 5), \"\\n\")\ncat(\"λ_1se =\", signif(lambda_1se, 5), \"\\n\")\n\n# === Extract non-zero coefficients ===\ncoef_min <- coef(cvfit, s = \"lambda.min\")\nselected <- rownames(coef_min)[coef_min[, 1] != 0]\nselected <- selected[selected != \"(Intercept)\"]\n\nselected_tbl <- data.frame(\n  variable = selected,\n  coefficient = as.numeric(coef_min[selected, 1])\n)\n\n# === Remove variables with near-zero coefficients ===\nselected_tbl <- selected_tbl %>%\n  filter(abs(coefficient) >= 1e-5) %>%\n  arrange(desc(abs(coefficient)))\n\ncat(\"  Number of variables retained after LASSO:\", nrow(selected_tbl), \"\\n\")\n\n```\n\n```{r}\n\n# === Output files ===\nif (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\nif (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n\nwrite_csv(selected_tbl, \"file/lasso_selected_variables.csv\")\n```\n\n```{r}\n# === Visualization 1: LASSO Coefficient Importance (Top 20) ===\ntop_n_vars <- min(20, nrow(selected_tbl))\nselected_top <- selected_tbl %>% \n  top_n(top_n_vars, abs(coefficient)) %>%\n  arrange(coefficient)\n\np_coef <- ggplot(selected_top, aes(x = reorder(variable, coefficient), y = coefficient, fill = coefficient)) +\n  geom_col(alpha = 0.9) +\n  scale_fill_gradient2(\n    low = \"#053061\",      \n    mid = \"#F7F7F7\",      \n    high = \"#67001F\",     \n    midpoint = 0,\n    name = \"Coefficient\"\n  ) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  coord_flip() +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Selected Variables - Coefficient Importance\",\n    subtitle = paste0(\"Top \", top_n_vars, \" variables by absolute coefficient value\"),\n    x = \"Variable\",\n    y = \"Coefficient\",\n    caption = paste0(\"λ_min = \", signif(lambda_min, 4), \" | Total selected: \", nrow(selected_tbl), \" variables\")\n  )\n\nggsave(\"plot/lasso_coefficients.png\", p_coef, width = 10, height = 8, dpi = 300, bg = \"white\")\n\n```\n\n```{r}\n# === Visualization 2: Cross-Validation Curve (λ Selection) ===\ncv_df <- data.frame(\n  lambda = cvfit$lambda,\n  cvm = cvfit$cvm,\n  cvsd = cvfit$cvsd,\n  cvlo = cvfit$cvm - cvfit$cvsd,\n  cvup = cvfit$cvm + cvfit$cvsd\n)\n\np_cv <- ggplot(cv_df, aes(x = log(lambda), y = cvm)) +\n  geom_ribbon(aes(ymin = cvlo, ymax = cvup), fill = \"#4393C3\", alpha = 0.3) +\n  geom_line(color = \"#053061\", linewidth = 1) +\n  geom_point(color = \"#053061\", size = 2, alpha = 0.6) +\n  geom_vline(xintercept = log(lambda_min), linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  geom_vline(xintercept = log(lambda_1se), linetype = \"dashed\", color = \"#D73027\", linewidth = 0.7) +\n  annotate(\"text\", x = log(lambda_min), y = max(cv_df$cvm) * 0.95, \n           label = paste0(\"λ_min = \", signif(lambda_min, 3)), \n           hjust = -0.1, size = 3.5, color = \"#67001F\") +\n  annotate(\"text\", x = log(lambda_1se), y = max(cv_df$cvm) * 0.90, \n           label = paste0(\"λ_1se = \", signif(lambda_1se, 3)), \n           hjust = -0.1, size = 3.5, color = \"#D73027\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Cross-Validation Curve\",\n    subtitle = \"Mean Squared Error vs. log(λ)\",\n    x = \"log(λ)\",\n    y = \"Mean Squared Error\",\n    caption = \"Shaded area represents ±1 standard error\"\n  )\n\nggsave(\"plot/lasso_cv_curve.png\", p_cv, width = 10, height = 7, dpi = 300, bg = \"white\")\n```\n\n```{r}\n# === Selection Summary ===\ncat(\"\\n\", rep(\"=\", 60), \"\\n\", sep = \"\")\ncat(\" Summary of Feature Selection Results\\n\")\ncat(rep(\"=\", 60), \"\\n\")\ncat(\"Initial number of predictors:\", ncol(x), \"\\n\")\ncat(\"After VIF ≤\", vif_threshold, \":\", length(vars_keep), \"\\n\")\ncat(\"Retained after LASSO:\", nrow(selected_tbl), \"\\n\")\ncat(\"Selection rate:\", round((1 - nrow(selected_tbl) / ncol(x)) * 100, 1), \"%\\n\")\ncat(rep(\"=\", 60), \"\\n\\n\")\n\ncat(\" Step 3 Completed\\n\")\ncat(\" LASSO results: file/lasso_selected_variables.csv\\n\")\ncat(\" Coefficient importance: plot/lasso_coeff_importance_top20.png\\n\")\ncat(\" CV curve: plot/lasso_cv_curve.png\\n\")\n\n```\n\n\n\nStep4\n\n\n\n```{r}\n# =========================================================\n# Step 4 Enhanced: Four Progressive Models + Coefficient Tables + Feature Importance\n# =========================================================\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(car)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"  Step 4: OLS Progressive Modeling and Full Analysis\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n```\n\n```{r}\n# === Read Data ===\ndf <- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\n\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat(sprintf(\"Target variable: %s\\n\", y_var))\ncat(sprintf(\"Sample size: %d\\n\\n\", nrow(df)))\n\n```\n\n```{r}\n# === Data preprocessing ===\ncat(\"=== Data preprocessing ===\\n\")\n\ncat_vars <- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncat_vars <- intersect(cat_vars, names(df))\n\nfor (cat_var in cat_vars) {\n  if (cat_var %in% c(\"zip_code\", \"census_tract\")) {\n    freq_table <- table(df[[cat_var]])\n    sparse_levels <- names(freq_table[freq_table < 100])\n    df[[cat_var]] <- as.character(df[[cat_var]])\n    df[[cat_var]][df[[cat_var]] %in% sparse_levels] <- \"Other\"\n    df[[cat_var]] <- factor(df[[cat_var]])\n    cat(sprintf(\"  %s: %d → %d categories\\n\", cat_var, length(freq_table), length(levels(df[[cat_var]]))))\n  } else {\n    df[[cat_var]] <- factor(df[[cat_var]])\n  }\n}\n\ncoord_vars <- c(\"x_coord\", \"y_coord\")\nif (all(coord_vars %in% names(df))) {\n  center_x <- median(df$x_coord, na.rm = TRUE)\n  center_y <- median(df$y_coord, na.rm = TRUE)\n  df$dist_to_center <- sqrt((df$x_coord - center_x)^2 + (df$y_coord - center_y)^2)\n  cat(\"  Added: dist_to_center\\n\")\n} else {\n  coord_vars <- c()\n}\n\n# Identify variable groups\nstructural_vars <- c()\nfor (pattern in c(\"livable_area\", \"bedroom\", \"bathroom\", \"stories\", \"garage\", \"age\")) {\n  matched <- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  structural_vars <- c(structural_vars, matched)\n}\nstructural_vars <- unique(structural_vars)\nstructural_vars <- structural_vars[sapply(df[structural_vars], is.numeric)]\n\ncensus_vars <- c()\nfor (pattern in c(\"income\", \"poverty\", \"education\", \"bachelor\", \"population\", \"household\")) {\n  matched <- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  census_vars <- c(census_vars, matched)\n}\ncensus_vars <- unique(census_vars)\ncensus_vars <- census_vars[sapply(df[census_vars], is.numeric)]\n\nspatial_vars <- c(coord_vars, \"dist_to_center\")\nspatial_vars <- intersect(spatial_vars, names(df))\n\nfixed_effects <- c(\"zip_code\", \"census_tract\")\nfixed_effects <- intersect(fixed_effects, names(df))\n\ncat(sprintf(\"\\n  Structural features: %d\\n\", length(structural_vars)))\ncat(sprintf(\"  Census features: %d\\n\", length(census_vars)))\ncat(sprintf(\"  Spatial features: %d\\n\", length(spatial_vars)))\ncat(sprintf(\"  Fixed effects: %d\\n\", length(fixed_effects)))\n\n```\n\n```{r}\n# =========================================================\n# Build 4 Progressive Models\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Building 4 Progressive Models ===\\n\\n\")\n\nmake_formula <- function(y, main_vars, interact_vars = NULL, fe_vars = NULL) {\n  rhs <- main_vars\n  if (!is.null(interact_vars) && length(interact_vars) == 2) {\n    rhs <- c(rhs, paste(interact_vars, collapse = \":\"))\n  }\n  if (!is.null(fe_vars)) {\n    rhs <- c(rhs, fe_vars)\n  }\n  as.formula(paste(y, \"~\", paste(rhs, collapse = \" + \")))\n}\n\nf1 <- make_formula(y_var, structural_vars)\nf2 <- make_formula(y_var, c(structural_vars, census_vars))\nf3 <- make_formula(y_var, c(structural_vars, census_vars, spatial_vars))\n\ninteract_pairs <- NULL\nif (\"total_livable_area\" %in% structural_vars && \"median_incomeE\" %in% census_vars) {\n  interact_pairs <- c(\"total_livable_area\", \"median_incomeE\")\n}\n\nf4 <- make_formula(y_var, c(structural_vars, census_vars, spatial_vars),\n                   interact_vars = interact_pairs, fe_vars = fixed_effects)\n\nmodels <- list()\ncat(\"  Fitting Model 1...\"); models[[\"M1: Structural\"]] <- lm(f1, data = df, na.action = na.exclude); cat(\" ✓\\n\")\ncat(\"  Fitting Model 2...\"); models[[\"M2: +Census\"]] <- lm(f2, data = df, na.action = na.exclude); cat(\" ✓\\n\")\ncat(\"  Fitting Model 3...\"); models[[\"M3: +Spatial\"]] <- lm(f3, data = df, na.action = na.exclude); cat(\" ✓\\n\")\ncat(\"  Fitting Model 4...\"); models[[\"M4: +Interact+FE\"]] <- lm(f4, data = df, na.action = na.exclude); cat(\" ✓\\n\")\n\n\n```\n\n```{r}\n# =========================================================\n# Model Performance Evaluation\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Model Performance Evaluation ===\\n\\n\")\n\ntrain_perf <- lapply(models, function(m) {\n  pred <- fitted(m)\n  actual <- df[[y_var]][!is.na(fitted(m))]\n  \n  # Log-scale metrics\n  rmse_log <- sqrt(mean((pred - actual)^2, na.rm = TRUE))\n  mae_log <- mean(abs(pred - actual), na.rm = TRUE)\n  \n  # Original-scale metrics — use expm1 to invert log1p\n  pred_original <- expm1(pred)\n  actual_original <- expm1(actual)\n  \n  # Method 3: remove outliers based on residuals (|residual| > 3 standard deviations)\n  residuals_original <- pred_original - actual_original\n  res_sd <- sd(residuals_original, na.rm = TRUE)\n  valid_idx <- abs(residuals_original) <= 3 * res_sd\n  \n  # Compute RMSE and MAE after removing outliers\n  rmse_original <- sqrt(mean((pred_original[valid_idx] - actual_original[valid_idx])^2, na.rm = TRUE))\n  mae_original <- mean(abs(pred_original[valid_idx] - actual_original[valid_idx]), na.rm = TRUE)\n  \n  data.frame(\n    RMSE_log = rmse_log,\n    RMSE_original = rmse_original,\n    MAE_log = mae_log,\n    MAE_original = mae_original,\n    R2 = summary(m)$r.squared,\n    Adj_R2 = summary(m)$adj.r.squared,\n    N_vars = length(coef(m)) - 1,\n    N = length(pred),\n    N_valid = sum(valid_idx, na.rm = TRUE),      # retained sample count\n    Pct_valid = mean(valid_idx, na.rm = TRUE) * 100  # percentage of retained samples\n  )\n}) %>% bind_rows(.id = \"Model\")\n\n# Print removal stats\ncat(\"\\nOutlier removal statistics (|residual| > 3σ):\\n\")\nprint(train_perf %>% select(Model, N, N_valid, Pct_valid, RMSE_original, MAE_original))\n\ncat(\"Performing 10-fold cross-validation...\\n\")\ncv_ctrl <- trainControl(method = \"cv\", number = 10, verboseIter = FALSE)\nformulas <- list(f1, f2, f3, f4)\nnames(formulas) <- names(models)\n\ncv_results <- lapply(formulas, function(fm) {\n  tryCatch(train(fm, data = df, method = \"lm\", trControl = cv_ctrl, na.action = na.omit),\n           error = function(e) NULL)\n})\ncv_results <- Filter(Negate(is.null), cv_results)\n\ncv_perf <- lapply(cv_results, function(m) {\n  data.frame(CV_RMSE_log = m$results$RMSE[1], CV_R2 = m$results$Rsquared[1])\n}) %>% bind_rows(.id = \"Model\")\n\nperf_table <- train_perf %>%\n  left_join(cv_perf, by = \"Model\") %>%\n  mutate(Overfit_RMSE_log = CV_RMSE_log - RMSE_log, Overfit_R2 = R2 - CV_R2)\n\ncat(\"\\n\")\nprint(perf_table, digits = 4, row.names = FALSE)\n\n```\n\n```{r}\n# =========================================================\n# Heteroskedasticity Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Heteroskedasticity Test (Model 4) ===\\n\\n\")\n\nbp_test <- bptest(models[[\"M4: +Interact+FE\"]])\ncat(sprintf(\"Breusch–Pagan Test:\\n  Statistic = %.2f\\n  p-value = %.4f\\n\", \n            bp_test$statistic, bp_test$p.value))\n\nif (bp_test$p.value < 0.05) {\n  cat(\"\\n️ Significant heteroskedasticity detected (p < 0.05)\\n   Recommendation: use robust standard errors\\n\")\n} else {\n  cat(\"\\n No significant heteroskedasticity detected\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# Model 4 Full Coefficient Table (Robust Standard Errors)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Model 4: Full Coefficient Table (Robust SE) ===\\n\\n\")\n\nmodel4 <- models[[\"M4: +Interact+FE\"]]\nrobust_coef <- coeftest(model4, vcov = vcovHC(model4, type = \"HC1\"))\n\ncoef_table <- tidy(robust_coef) %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(\n    sig = case_when(\n      p.value < 0.001 ~ \"***\",\n      p.value < 0.01 ~ \"**\",\n      p.value < 0.05 ~ \"*\",\n      p.value < 0.10 ~ \".\",\n      TRUE ~ \"\"\n    )\n  ) %>%\n  arrange(p.value) %>%\n  dplyr::select(term, estimate, std.error, statistic, p.value, sig)\n\nif (!dir.exists(\"file\")) dir.create(\"file\")\nif (!dir.exists(\"table\")) dir.create(\"table\")\n\nwrite_csv(coef_table, \"table/model4_full_coefficients.csv\")\ncat(\"  ✓ table/model4_full_coefficients.csv (All coefficients)\\n\")\n\ncoef_sig <- coef_table %>% filter(p.value < 0.05)\nwrite_csv(coef_sig, \"table/model4_significant_coefficients.csv\")\ncat(\"  ✓ table/model4_significant_coefficients.csv (Significant coefficients)\\n\")\n\ncat(sprintf(\"\\nNumber of significant variables (p < 0.05): %d\\n\", nrow(coef_sig)))\ncat(\"\\nTop 20 significant variables:\\n\")\nprint(as.data.frame(head(coef_sig, 20)), row.names = FALSE)\n\n\n```\n\n```{r}\n# =========================================================\n# Stargazer Regression Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generating Stargazer Regression Table ===\\n\\n\")\n\nif (requireNamespace(\"stargazer\", quietly = TRUE)) {\n  sink(\"table/regression_table.txt\")\n  stargazer::stargazer(\n    models[[1]], models[[2]], models[[3]], models[[4]],\n    type = \"text\",\n    title = \"Progressive OLS Regression Results\",\n    dep.var.labels = \"Log Sale Price\",\n    column.labels = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"),\n    omit.stat = c(\"ser\", \"f\"),\n    digits = 3,\n    star.cutoffs = c(0.05, 0.01, 0.001),\n    notes = \"Robust standard errors in parentheses\"\n  )\n  sink()\n  cat(\"   table/regression_table.txt\\n\")\n}\n\n```\n\n```{r}\n# =========================================================\n# Cross-Model Feature Importance\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Cross-Model Feature Importance Comparison ===\\n\\n\")\n\nimportance_list <- list()\nfor (i in 1:4) {\n  model_name <- names(models)[i]\n  model <- models[[i]]\n  \n  coefs <- tidy(model) %>%\n    filter(term != \"(Intercept)\") %>%\n    mutate(abs_estimate = abs(estimate)) %>%\n    arrange(desc(abs_estimate)) %>%\n    head(10) %>%\n    mutate(Model = model_name) %>%\n    dplyr::select(Model, term, estimate, abs_estimate)\n  \n  importance_list[[i]] <- coefs\n}\n\nimportance_all <- bind_rows(importance_list)\nwrite_csv(importance_all, \"table/feature_importance_all_models.csv\")\ncat(\"  ✓ table/feature_importance_all_models.csv\\n\")\n\n# Identify variables that are important across multiple models\ntop_vars <- importance_all %>%\n  group_by(term) %>%\n  summarise(appearances = n(), .groups = \"drop\") %>%\n  filter(appearances >= 3) %>%\n  pull(term)\n\nif (length(top_vars) > 0) {\n  importance_key <- importance_all %>% filter(term %in% top_vars)\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\")\n  \n  p_importance <- ggplot(importance_key, aes(Model, abs_estimate, fill = Model)) +\n    geom_col(alpha = 0.8) +\n    facet_wrap(~term, scales = \"free_y\", ncol = 4) +\n    scale_fill_manual(\n      values = c(\"#053061\", \"#2166AC\", \"#4393C3\", \"#D6604D\"),\n      guide = \"none\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1, size = 7),\n      strip.text = element_text(face = \"bold\", size = 9),\n      panel.grid.major.x = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      plot.subtitle = element_text(hjust = 0.5, size = 10),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Feature Importance Across Models\",\n      subtitle = \"Variables appearing in top 10 of at least 3 models\",\n      x = NULL, y = \"Absolute Coefficient\"\n    )\n  \n  ggsave(\"plot/feature_importance_across_models.png\", p_importance,\n         width = 14, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/feature_importance_across_models.png\\n\")\n}\n```\n\n```{r}\n# =========================================================\n# Visualization (Original 3 Figures)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Generate Performance Visualizations ===\\n\\n\")\n\nperf_plot_data <- perf_table %>% mutate(Model_Num = 1:n())\n\np_r2 <- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = R2, color = \"Training R²\"), linewidth = 1.2) +\n  geom_line(aes(y = CV_R2, color = \"CV R²\"), linewidth = 1.2) +\n  geom_point(aes(y = R2, color = \"Training R²\"), size = 3) +\n  geom_point(aes(y = CV_R2, color = \"CV R²\"), size = 3) +\n  scale_color_manual(values = c(\"Training R²\" = \"#67001F\", \"CV R²\" = \"#053061\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"R² Improvement: Progressive Model Building\", x = NULL, y = \"R² Value\")\n\np_rmse <- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), linewidth = 1.2) +\n  geom_point(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), size = 3) +\n  scale_color_manual(values = c(\"RMSE ($1000s)\" = \"#67001F\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"RMSE Reduction (Original Scale)\", x = NULL, y = \"RMSE ($1000s)\")\n\np_combined <- (p_r2 | p_rmse) +\n  plot_annotation(\n    title = \"4-Model Progressive OLS: Performance Evolution\",\n    subtitle = \"From structural features to full specification with interactions and fixed effects\",\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 11, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/ols_4model_performance.png\", p_combined, width = 14, height = 6, dpi = 300, bg = \"white\")\ncat(\"  ✓ plot/ols_4model_performance.png\\n\")\n```\n\n```{r}\n# Prediction Scatter Plot\npred_m4 <- fitted(models[[\"M4: +Interact+FE\"]])\nactual_m4 <- df[[y_var]][!is.na(pred_m4)]\npred_data <- data.frame(\n  actual = actual_m4,\n  predicted = pred_m4[!is.na(pred_m4)],\n  residual = pred_m4[!is.na(pred_m4)] - actual_m4\n)\n\np_pred <- ggplot(pred_data, aes(actual, predicted)) +\n  geom_point(aes(color = residual), alpha = 0.4, size = 0.8) +\n  scale_color_gradient2(low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\", midpoint = 0, name = \"Residual\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = min(pred_data$actual) + 0.5, y = max(pred_data$predicted) - 0.5,\n           label = sprintf(\"R² = %.4f\\nRMSE = %.4f\", perf_table$R2[4], perf_table$RMSE[4]),\n           hjust = 0, size = 4.5, fontface = \"bold\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"Model 4: Prediction vs. Actual\",\n       subtitle = \"Full specification with interactions and fixed effects\",\n       x = \"Actual Sale Price (log)\", y = \"Predicted Sale Price (log)\")\n\nggsave(\"plot/ols_4model_prediction.png\", p_pred, width = 10, height = 8, dpi = 300, bg = \"white\")\ncat(\"   plot/ols_4model_prediction.png\\n\")\n```\n\n```{r}\n# Residual Diagnostics\nresid_data <- data.frame(\n  fitted = pred_m4[!is.na(pred_m4)],\n  residual = pred_data$residual,\n  std_resid = rstandard(models[[\"M4: +Interact+FE\"]])[!is.na(pred_m4)]\n)\n\np_r1 <- ggplot(resid_data, aes(fitted, residual)) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residuals vs. Fitted\", x = \"Fitted\", y = \"Residuals\")\n\np_r2 <- ggplot(resid_data, aes(sample = std_resid)) +\n  stat_qq(alpha = 0.4, size = 0.6, color = \"#053061\") +\n  stat_qq_line(color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical\", y = \"Std. Residuals\")\n\np_r3 <- ggplot(resid_data, aes(std_resid)) +\n  geom_histogram(bins = 50, fill = \"#053061\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residual Distribution\", x = \"Std. Residuals\", y = \"Count\")\n\np_r4 <- ggplot(resid_data, aes(fitted, sqrt(abs(std_resid)))) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Scale-Location\", x = \"Fitted\", y = \"√|Std. Residuals|\")\n\np_diag <- (p_r1 | p_r2) / (p_r3 | p_r4) +\n  plot_annotation(\n    title = sprintf(\"Model 4 Diagnostics (BP test p = %.4f)\", bp_test$p.value),\n    theme = theme(plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n                  plot.background = element_rect(fill = \"white\", color = NA))\n  )\n\nggsave(\"plot/ols_4model_diagnostics.png\", p_diag, width = 12, height = 10, dpi = 300, bg = \"white\")\ncat(\"  ✓ plot/ols_4model_diagnostics.png\\n\")\n```\n\n```{r}\n# =========================================================\n# Spatial Prediction Comparison Plot\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Generating Spatial Prediction Comparison Plot ===\\n\\n\")\n  \n  # Prepare data\n  spatial_pred_data <- df %>%\n    filter(!is.na(x_coord) & !is.na(y_coord)) %>%\n    mutate(\n      predicted = fitted(models[[\"M4: +Interact+FE\"]]),\n      residual = predicted - .data[[y_var]]\n    ) %>%\n    filter(!is.na(predicted))\n  \n  # If the dataset is too large, randomly sample to speed up plotting\n  if (nrow(spatial_pred_data) > 10000) {\n    set.seed(2025)\n    spatial_pred_data <- spatial_pred_data %>% \n      slice_sample(n = 10000)\n    cat(\"  Large dataset detected; randomly sampled 10,000 points for plotting\\n\")\n  }\n\n# 1. Spatial Distribution of Actual Sale Prices\np_actual <- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = .data[[y_var]])) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Actual Sale Price\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 2. Spatial Distribution of Predicted Sale Prices\np_predicted <- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = predicted)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Predicted Sale Price (Model 4)\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 3. Spatial Distribution of Residuals\np_residual_spatial <- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = residual)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = 0,\n    name = \"Residual\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Prediction Residuals\",\n    subtitle = \"Blue = Underpredicted | Red = Overpredicted\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# Combine the 3 plots\np_spatial_comparison <- (p_actual | p_predicted | p_residual_spatial) +\n  plot_annotation(\n    title = \"Spatial Distribution: Actual vs. Predicted Sale Prices\",\n    subtitle = sprintf(\"Model 4 predictions (R² = %.4f, RMSE = %.4f)\", \n                      perf_table$R2[4], perf_table$RMSE[4]),\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/spatial_prediction_comparison.png\", p_spatial_comparison,\n       width = 18, height = 6, dpi = 300, bg = \"white\")\ncat(\"   plot/spatial_prediction_comparison.png\\n\")\n\n  # 4. High-Resolution Hexbin Heatmap Version\n  p_actual_hex <- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = .data[[y_var]])) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Actual (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_predicted_hex <- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = predicted)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Predicted (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_residual_hex <- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = residual)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0,\n      name = \"Avg Residual\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Residuals (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_spatial_hexbin <- (p_actual_hex | p_predicted_hex | p_residual_hex) +\n    plot_annotation(\n      title = \"Spatial Distribution (Hexbin Aggregation): Actual vs. Predicted\",\n      subtitle = \"Hexagonal bins show average values in each area\",\n      theme = theme(\n        plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/spatial_prediction_hexbin.png\", p_spatial_hexbin,\n         width = 18, height = 6, dpi = 300, bg = \"white\")\n  cat(\"  ✓ plot/spatial_prediction_hexbin.png\\n\")\n  \n  # 5. Spatial Clustering of Residuals\n  # Calculate spatial statistics of residuals\n  residual_stats <- spatial_pred_data %>%\n    summarise(\n      mean_residual = mean(residual, na.rm = TRUE),\n      sd_residual = sd(residual, na.rm = TRUE),\n      min_residual = min(residual, na.rm = TRUE),\n      max_residual = max(residual, na.rm = TRUE)\n    )\n  \n  cat(\"\\n  Spatial Residual Statistics:\\n\")\n  cat(sprintf(\"    Mean: %.4f\\n\", residual_stats$mean_residual))\n  cat(sprintf(\"    Standard Deviation: %.4f\\n\", residual_stats$sd_residual))\n  cat(sprintf(\"    Range: [%.4f, %.4f]\\n\", \n              residual_stats$min_residual, residual_stats$max_residual))\n  \n  # Identify high-residual areas\n  high_residual_areas <- spatial_pred_data %>%\n    filter(abs(residual) > 2 * residual_stats$sd_residual) %>%\n    mutate(residual_type = ifelse(residual > 0, \"Overpredicted\", \"Underpredicted\"))\n  \n  if (nrow(high_residual_areas) > 0) {\n    cat(sprintf(\"\\n  ️ Detected %d high-residual points (|residual| > 2σ):\\n\", \n                nrow(high_residual_areas)))\n    cat(sprintf(\"    Overpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Overpredicted\")))\n    cat(sprintf(\"    Underpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Underpredicted\")))\n    \n    # Save high-residual point data\n    write_csv(high_residual_areas %>% \n                dplyr::select(x_coord, y_coord, !!sym(y_var), predicted, residual, residual_type),\n              \"file/high_residual_locations.csv\")\n    cat(\"     file/high_residual_locations.csv\\n\")\n  }\n}\n```\n\n```{r}\n# =========================================================\n# Save Results\n# =========================================================\n\nwrite_csv(perf_table, \"file/ols_4model_performance.csv\")\nsaveRDS(models, \"file/ols_4models.rds\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 4 Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\" Output Files:\\n\\n\")\ncat(\"Tables:\\n\")\ncat(\"  • table/model4_full_coefficients.csv - Full coefficient table (Model 4)\\n\")\ncat(\"  • table/model4_significant_coefficients.csv - Significant coefficients (Model 4)\\n\")\ncat(\"  • table/regression_table.txt - Stargazer regression summary\\n\")\ncat(\"  • table/feature_importance_all_models.csv - Cross-model feature importance\\n\")\ncat(\"  • file/ols_4model_performance.csv - Model performance comparison\\n\")\ncat(\"  • file/ols_4models.rds - RDS objects of all 4 models\\n\\n\")\n\ncat(\"Figures:\\n\")\ncat(\"  • plot/ols_4model_performance.png - Performance evolution\\n\")\ncat(\"  • plot/ols_4model_prediction.png - Model 4 prediction vs. actual\\n\")\ncat(\"  • plot/ols_4model_diagnostics.png - Model 4 diagnostic plots\\n\")\ncat(\"  • plot/feature_importance_across_models.png - Cross-model feature importance\\n\")\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  • plot/spatial_prediction_comparison.png - Spatial prediction comparison\\n\")\n  cat(\"  • plot/spatial_prediction_hexbin.png - Spatial prediction heatmap\\n\")\n}\ncat(\"\\n\")\n\ncat(\" Model Summary:\\n\")\n\nfor (i in 1:4) {\n  rmse_fmt <- formatC(perf_table$RMSE_original[i], format = \"f\", digits = 0, big.mark = \",\")\n  cat(sprintf(\"  Model %d: R² = %.4f, RMSE = $%s [%d variables]\\n\",\n              i, perf_table$R2[i], rmse_fmt, perf_table$N_vars[i]))\n}\n\ncat(\"\\n Paper Writing Suggestions:\\n\")\ncat(\"  1. Results (Opening): Descriptive statistics table\\n\")\ncat(\"  2. Results (Core): Stargazer regression summary (comparison of 4 models)\\n\")\ncat(\"  3. Results (Details): Significant coefficients of Model 4\\n\")\ncat(\"  4. Discussion: Cross-model feature importance comparison\\n\")\ncat(\"  5. Mention the use of robust standard errors to address heteroskedasticity\\n\\n\")\n\n```\n\n\n\n\nStep4 plus\n\n\n\n```{r}\n# =========================================================\n# Step 4 Supplement: Cook’s Distance + Moran’s I Spatial Autocorrelation\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(spdep)  # Moran’s I\nlibrary(sf)     # Spatial data\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 4 Supplement: Influential Point Diagnostics and Spatial Autocorrelation Test\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n# === Load Data and Models ===\ndf <- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nmodels <- readRDS(\"file/ols_4models.rds\")\nmodel4 <- models[[\"M4: +Interact+FE\"]]\n\ny_var <- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n```\n\n```{r}\n# =========================================================\n# 1. Cook’s Distance – Influential Observation Detection\n# =========================================================\n\ncat(\"=== Cook’s Distance Analysis ===\\n\\n\")\n\n# Calculate Cook’s distance\ncooks_d <- cooks.distance(model4)\nn <- length(cooks_d)\nthreshold <- 4/n  # Classic threshold\n\n# Identify influential points\ninfluential <- which(cooks_d > threshold)\ncat(sprintf(\"Sample size: %d\\n\", n))\ncat(sprintf(\"Cook’s D threshold: %.6f (4/n)\\n\", threshold))\ncat(sprintf(\"Number of influential points: %d (%.2f%%)\\n\", length(influential), 100*length(influential)/n))\n\n# Save influential observations\nif (length(influential) > 0) {\n  influential_data <- df[influential, ] %>%\n    mutate(\n      cooks_d = cooks_d[influential],\n      fitted = fitted(model4)[influential],\n      residual = residuals(model4)[influential]\n    ) %>%\n    arrange(desc(cooks_d)) %>%\n    head(100)  # Save Top 100\n  \n  write_csv(influential_data, \"file/influential_observations.csv\")\n  cat(\"  ✓ file/influential_observations.csv (Top 100 Influential Points)\\n\")\n  \n  # Display Top 10\n  cat(\"\\nTop 10 Influential Points:\\n\")\n  print(influential_data %>% \n          select(parcel_number, sale_price, cooks_d, fitted, residual) %>% \n          head(10), \n        n = 10)\n}\n\n# Visualize Cook’s Distance\ncooks_df <- data.frame(\n  index = 1:n,\n  cooks_d = cooks_d,\n  influential = cooks_d > threshold\n)\n\np_cooks <- ggplot(cooks_df, aes(index, cooks_d, color = influential)) +\n  geom_point(alpha = 0.4, size = 0.8) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  scale_color_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"Normal\", \"TRUE\" = \"Influential\"),\n    name = NULL\n  ) +\n  annotate(\"text\", x = n*0.8, y = threshold*1.2,\n           label = sprintf(\"Threshold = %.6f\", threshold),\n           color = \"#67001F\", size = 4) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Cook’s Distance – Detection of Influential Observations\",\n    subtitle = sprintf(\"%d influential points (%.2f%% of data)\", \n                      length(influential), 100*length(influential)/n),\n    x = \"Observation Index\",\n    y = \"Cook’s Distance\",\n    caption = \"Threshold = 4/n (classic rule)\"\n  )\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nggsave(\"plot/cooks_distance.png\", p_cooks, width = 12, height = 6, dpi = 300, bg = \"white\")\ncat(\"\\n   plot/cooks_distance.png\\n\")\n\n```\n\n```{r}\n# =========================================================\n# 2. Moran’s I – Spatial Autocorrelation Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\"=== Moran’s I Spatial Autocorrelation Test ===\\n\\n\")\n\n# Check coordinates\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  \n  # Prepare spatial data\n  df_spatial <- df %>%\n    filter(!is.na(x_coord), !is.na(y_coord)) %>%\n    mutate(\n      residual = residuals(model4)[!is.na(x_coord) & !is.na(y_coord)],\n      sale_price_actual = .data[[y_var]]\n    ) %>%\n    filter(!is.na(residual))\n  \n  # Convert to sf object\n  coords <- df_spatial %>% select(x_coord, y_coord)\n  coords_sf <- st_as_sf(coords, coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n  \n  cat(sprintf(\"Valid samples: %d\\n\\n\", nrow(df_spatial)))\n  \n  # === 2.1 Moran’s I for Actual Sale Price ===\n  cat(\"--- Spatial Autocorrelation of Actual Sale Price ---\\n\")\n  \n  # Build spatial weight matrix (K-nearest neighbors, k=8)\n  nb_knn <- knn2nb(knearneigh(coords_sf, k = 8))\n  listw_knn <- nb2listw(nb_knn, style = \"W\")\n  \n  # Moran’s I test\n  moran_price <- moran.test(df_spatial$sale_price_actual, listw_knn)\n  \n  cat(sprintf(\"  Moran’s I = %.4f\\n\", moran_price$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_price$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_price$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_price$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_price$p.value))\n  \n  if (moran_price$p.value < 0.001) {\n    cat(\"  *** Highly significant positive spatial autocorrelation (p < 0.001)\\n\")\n  } else if (moran_price$p.value < 0.05) {\n    cat(\"  ** Significant positive spatial autocorrelation (p < 0.05)\\n\")\n  } else {\n    cat(\"  No significant spatial autocorrelation detected\\n\")\n  }\n  \n  # === 2.2 Moran’s I for Residuals ===\n  cat(\"\\n--- Spatial Autocorrelation of Model 4 Residuals ---\\n\")\n  \n  moran_resid <- moran.test(df_spatial$residual, listw_knn)\n  \n  cat(sprintf(\"  Moran’s I = %.4f\\n\", moran_resid$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_resid$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_resid$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_resid$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_resid$p.value))\n  \n  if (moran_resid$p.value < 0.001) {\n    cat(\"  ️ Strong residual spatial autocorrelation detected (p < 0.001)\\n\")\n    cat(\"  Suggestion: Consider Spatial Lag Model (SAR) or Spatial Error Model (SEM)\\n\")\n  } else if (moran_resid$p.value < 0.05) {\n    cat(\"  ️ Residual spatial autocorrelation detected (p < 0.05)\\n\")\n    cat(\"  Suggestion: Add more spatial variables or use spatial regression models\\n\")\n  } else {\n    cat(\"  ✓ No significant residual spatial autocorrelation – spatial effects well captured\\n\")\n  }\n  \n  # === 2.3 Moran Scatterplots ===\n  \n  # Calculate spatial lags\n  lag_price <- lag.listw(listw_knn, df_spatial$sale_price_actual)\n  lag_resid <- lag.listw(listw_knn, df_spatial$residual)\n  \n  # Standardize\n  price_std <- scale(df_spatial$sale_price_actual)[,1]\n  lag_price_std <- scale(lag_price)[,1]\n  resid_std <- scale(df_spatial$residual)[,1]\n  lag_resid_std <- scale(lag_resid)[,1]\n  \n  # Define quadrants\n  quadrant_price <- case_when(\n    price_std > 0 & lag_price_std > 0 ~ \"HH (High-High)\",\n    price_std < 0 & lag_price_std < 0 ~ \"LL (Low-Low)\",\n    price_std > 0 & lag_price_std < 0 ~ \"HL (High-Low)\",\n    TRUE ~ \"LH (Low-High)\"\n  )\n  \n  quadrant_resid <- case_when(\n    resid_std > 0 & lag_resid_std > 0 ~ \"HH\",\n    resid_std < 0 & lag_resid_std < 0 ~ \"LL\",\n    resid_std > 0 & lag_resid_std < 0 ~ \"HL\",\n    TRUE ~ \"LH\"\n  )\n  \n  moran_df_price <- data.frame(\n    value = price_std,\n    lag_value = lag_price_std,\n    quadrant = quadrant_price\n  )\n  \n  moran_df_resid <- data.frame(\n    value = resid_std,\n    lag_value = lag_resid_std,\n    quadrant = quadrant_resid\n  )\n  \n  # Plot – Sale Price\n  p_moran_price <- ggplot(moran_df_price, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH (High-High)\" = \"#67001F\", \n                 \"LL (Low-Low)\" = \"#053061\",\n                 \"HL (High-Low)\" = \"#F4A582\",\n                 \"LH (Low-High)\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran’s I = %.4f***\", moran_price$estimate[1]),\n             size = 5, fontface = \"bold\", color = \"#67001F\") +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran’s I Scatter Plot – Sale Price\",\n      subtitle = \"Strong positive spatial autocorrelation\",\n      x = \"Standardized Sale Price\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Plot – Residuals\n  p_moran_resid <- ggplot(moran_df_resid, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH\" = \"#67001F\", \"LL\" = \"#053061\",\n                 \"HL\" = \"#F4A582\", \"LH\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran’s I = %.4f%s\", \n                           moran_resid$estimate[1],\n                           ifelse(moran_resid$p.value < 0.001, \"***\",\n                                  ifelse(moran_resid$p.value < 0.05, \"**\", \"\"))),\n             size = 5, fontface = \"bold\", \n             color = ifelse(moran_resid$p.value < 0.05, \"#67001F\", \"gray50\")) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran’s I Scatter Plot – Model 4 Residuals\",\n      subtitle = ifelse(moran_resid$p.value < 0.05, \n                       \"Residual spatial autocorrelation detected\",\n                       \"No significant residual spatial autocorrelation\"),\n      x = \"Standardized Residual\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Combine plots\n  p_moran_combined <- (p_moran_price | p_moran_resid) +\n    plot_annotation(\n      title = \"Spatial Autocorrelation Analysis (Moran’s I)\",\n      subtitle = \"Left: Original prices show strong clustering | Right: Model residuals\",\n      theme = theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/morans_i_scatter.png\", p_moran_combined, \n         width = 16, height = 7, dpi = 300, bg = \"white\")\n  cat(\"\\n   plot/morans_i_scatter.png\\n\")\n  \n  # Save Moran’s I results\n  moran_summary <- data.frame(\n    Variable = c(\"Sale Price\", \"Model 4 Residuals\"),\n    Morans_I = c(moran_price$estimate[1], moran_resid$estimate[1]),\n    Expected = c(moran_price$estimate[2], moran_resid$estimate[2]),\n    Variance = c(moran_price$estimate[3], moran_resid$estimate[3]),\n    Z_score = c(moran_price$statistic, moran_resid$statistic),\n    P_value = c(moran_price$p.value, moran_resid$p.value),\n    Significant = c(moran_price$p.value < 0.05, moran_resid$p.value < 0.05)\n  )\n  \n  write_csv(moran_summary, \"file/morans_i_results.csv\")\n  cat(\"  ✓ file/morans_i_results.csv\\n\")\n  \n} else {\n  cat(\" Missing coordinate data – Moran’s I test cannot be performed\\n\")\n}\n```\n\n```{r}\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\ncat(\" Step 4 Supplement Completed\\n\")\ncat(rep(\"=\", 80), \"\\n\\n\")\n\ncat(\" Output Files:\\n\\n\")\ncat(\"Tables:\\n\")\nif (exists(\"influential_data\")) {\n  cat(\"  • file/influential_observations.csv – List of influential observations\\n\")\n}\nif (exists(\"moran_summary\")) {\n  cat(\"  • file/morans_i_results.csv – Moran’s I test results\\n\")\n}\n\ncat(\"\\nFigures:\\n\")\ncat(\"  • plot/cooks_distance.png – Cook’s Distance Scatter Plot\\n\")\nif (exists(\"p_moran_combined\")) {\n  cat(\"  • plot/morans_i_scatter.png – Moran’s I Scatter Plot\\n\")\n}\n\ncat(\"\\n Key Findings:\\n\")\nif (exists(\"moran_resid\")) {\n  if (moran_resid$p.value < 0.05) {\n    cat(\"  ️ Significant spatial autocorrelation detected in residuals\\n\")\n    cat(\"     → The model did not fully capture spatial effects\\n\")\n    cat(\"     → Suggest using spatial econometric models (SAR/SEM)\\n\")\n  } else {\n    cat(\"   No significant spatial autocorrelation in residuals\\n\")\n    cat(\"     → Fixed effects adequately controlled for spatial dependence\\n\")\n  }\n}\n\ncat(\"\\n\")\n\n```\n\n```{r}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(patchwork)\n\n# tidytext\nif (!require(\"tidytext\", quietly = TRUE)) {\n  reorder_within <- function(x, by, within, fun = mean, sep = \"___\", ...) {\n    new_x <- paste(x, within, sep = sep)\n    stats::reorder(new_x, by, FUN = fun)\n  }\n  \n  scale_x_reordered <- function(..., sep = \"___\") {\n    reg <- paste0(sep, \".+$\")\n    ggplot2::scale_x_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n  \n  scale_y_reordered <- function(..., sep = \"___\") {\n    reg <- paste0(sep, \".+$\")\n    ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n} else {\n  library(tidytext)\n}\n```\n\n```{r}\n# Read data\ndf <- read_csv(\"./table/feature_importance_all_models.csv\", show_col_types = FALSE)\n\n# Data cleaning\ndf <- df %>%\n  mutate(\n    estimate = as.numeric(estimate),\n    abs_estimate = as.numeric(abs_estimate)\n  )\n\n# Variable classification\ndf <- df %>%\n  mutate(\n    var_type = case_when(\n      grepl(\"census_tract|zip_code\", term) ~ \"Location Fixed Effects\",\n      grepl(\"income|POVERTY\", term) ~ \"Socioeconomic\",\n      grepl(\"bathroom|bedroom|livable_area\", term) ~ \"Structural\",\n      grepl(\"age\", term) ~ \"Age\",\n      grepl(\"coord|dist\", term) ~ \"Spatial\",\n      TRUE ~ \"Other\"\n    ),\n    # Clean variable names\n    term_clean = case_when(\n      term == \"number_of_bathrooms\" ~ \"Bathrooms\",\n      term == \"number_of_bedrooms\" ~ \"Bedrooms\",\n      term == \"total_livable_area\" ~ \"Livable Area\",\n      term == \"per_cap_incomeE\" ~ \"Per Capita Income\",\n      term == \"median_incomeE\" ~ \"Median Income\",\n      term == \"PCTPOVERTY\" ~ \"Poverty Rate\",\n      term == \"age2\" ~ \"Age²\",\n      term == \"dist_to_center\" ~ \"Distance to Center\",\n      term == \"x_coord\" ~ \"X Coordinate\",\n      term == \"y_coord\" ~ \"Y Coordinate\",\n      grepl(\"census_tract\", term) ~ gsub(\"census_tract\", \"Census Tract \", term),\n      grepl(\"zip_code\", term) ~ gsub(\"zip_code\", \"ZIP \", term),\n      TRUE ~ term\n    )\n  )\n\n# Keep only top 10 most important variables per model\ntop_features <- df %>%\n  group_by(Model) %>%\n  slice_max(abs_estimate, n = 10) %>%\n  ungroup()\n\n# Color palette (deep blue to deep red)\ncolor_palette <- c(\n  \"Structural\" = \"#67001F\",        # deep red\n  \"Socioeconomic\" = \"#4393C3\",     # medium blue  \n  \"Age\" = \"#2166AC\",               # darker blue\n  \"Spatial\" = \"#053061\",           # darkest blue\n  \"Location Fixed Effects\" = \"#D6604D\"  # reddish tone\n)\n\ncat(\"Generating visualization (using deep blue–deep red palette)...\\n\\n\")\n\n```\n\n```{r}\n# ========== Figure 1: Lollipop Chart ==========\ncat(\"  Generating Figure 1: Lollipop chart...\\n\")\n\np1 <- ggplot(top_features, aes(x = reorder_within(term_clean, abs_estimate, Model), \n                                y = abs_estimate, color = var_type)) +\n  geom_segment(aes(xend = reorder_within(term_clean, abs_estimate, Model), yend = 0), \n               size = 1.2, alpha = 0.7) +\n  geom_point(size = 4, alpha = 0.9) +\n  coord_flip() +\n  facet_wrap(~Model, scales = \"free_y\", ncol = 2) +\n  scale_x_reordered() +\n  scale_color_manual(values = color_palette, name = \"Feature Type\") +\n  labs(\n    title = \"Feature Importance Across Progressive Models\",\n    subtitle = \"Lollipop chart showing top 10 features per model\",\n    x = NULL,\n    y = \"Absolute Coefficient\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    strip.background = element_rect(fill = \"gray95\", color = NA),\n    legend.position = \"bottom\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray90\", size = 0.3),\n    axis.text.y = element_text(size = 9),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_lollipop_v2.png\", p1, \n       width = 14, height = 10, dpi = 300, bg = \"white\")\n\ncat(\"    ✓ Saved to: plot/feature_importance_lollipop_v2.png\\n\")\n\n```\n\n```{r}\n# ========== Figure 2: Heatmap ==========\ncat(\"  Generating Figure 2: Heatmap...\\n\")\n\nheatmap_data <- df %>%\n  group_by(Model) %>%\n  slice_max(abs_estimate, n = 8) %>%\n  ungroup() %>%\n  mutate(Model_short = gsub(\"M[0-9]: \", \"\", Model))\n\np2 <- ggplot(heatmap_data, aes(x = Model_short, y = fct_reorder(term_clean, abs_estimate), \n                                fill = abs_estimate)) +\n  geom_tile(color = \"white\", size = 1) +\n  geom_text(aes(label = sprintf(\"%.3f\", abs_estimate)), \n            color = \"white\", size = 3.5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(heatmap_data$abs_estimate),\n    name = \"Coefficient\\nMagnitude\"\n  ) +\n  labs(\n    title = \"Feature Importance Heatmap Across Models\",\n    subtitle = \"Top 8 features per model (darker = stronger effect)\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\", size = 11),\n    axis.text.y = element_text(size = 10),\n    legend.position = \"right\",\n    panel.grid = element_blank(),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_heatmap_v2.png\", p2, \n       width = 12, height = 8, dpi = 300, bg = \"white\")\n\ncat(\"    ✓ Saved to: plot/feature_importance_heatmap_v2.png\\n\")\n\n```\n\n\n```{r}\n# ========== Figure 3: Nightingale Rose Chart ==========\ncat(\"  Generating Figure 3: Nightingale rose chart...\\n\")\n\nmodels <- unique(top_features$Model)\n\nrose_plots <- lapply(models, function(m) {\n  data_m <- top_features %>%\n    filter(Model == m) %>%\n    arrange(desc(abs_estimate)) %>%\n    head(8) %>%\n    mutate(term_clean = factor(term_clean, levels = term_clean))\n  \n  ggplot(data_m, aes(x = term_clean, y = abs_estimate, fill = abs_estimate)) +\n    geom_col(width = 1, alpha = 0.9, color = \"white\", size = 0.5) +\n    coord_polar(theta = \"x\") +\n    scale_fill_gradient2(\n      low = \"#4393C3\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(data_m$abs_estimate),\n      name = \"Importance\"\n    ) +\n    labs(title = m, x = NULL, y = NULL) +\n    theme_minimal(base_size = 10) +\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5, size = 11),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(size = 12, face = \"bold\"),\n      panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      legend.position = \"none\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n})\n\np3 <- wrap_plots(rose_plots, ncol = 2) +\n  plot_annotation(\n    title = \"Feature Importance: Nightingale Rose Chart\",\n    subtitle = \"Top 8 features visualized as rose petals (larger petal = more important)\",\n    theme = theme(\n      plot.title = element_text(face = \"bold\", size = 18, hjust = 0.5),\n      plot.subtitle = element_text(size = 13, hjust = 0.5, color = \"gray40\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/feature_importance_rose_v2.png\", p3, \n       width = 14, height = 12, dpi = 300, bg = \"white\")\n\ncat(\"     Saved to: plot/feature_importance_rose_v2.png\\n\")\n\n```\n\n```{r}\n# Print Summary\ncat(\"\\n==============================================\\n\")\ncat(\"🎨 All visualizations have been successfully generated! (Deep Blue–Red Palette)\\n\")\ncat(\"==============================================\\n\\n\")\ncat(\"📊 Generated Figures:\\n\")\ncat(\"  1. feature_importance_lollipop_v2.png  - Lollipop Chart\\n\")\ncat(\"  2. feature_importance_heatmap_v2.png   - Heatmap\\n\")\ncat(\"  3. feature_importance_rose_v2.png      - Nightingale Rose Chart\\n\\n\")\ncat(\"🎨 Color Palette (following Moran's I visualization):\\n\")\ncat(\"  • Structural: Deep Red (#67001F)\\n\")\ncat(\"  • Socioeconomic: Deep Blue (#4393C3)\\n\")\ncat(\"  • Age: Darker Blue (#2166AC)\\n\")\ncat(\"  • Spatial: Deepest Blue (#053061)\\n\")\ncat(\"  • Location FE: Reddish Tone (#D6604D)\\n\\n\")\n\n```\n\n```{r}\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"output-file":"Yang_Chen_Fang_Gao_Xiang_Zhao_Appendix.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","theme":"cosmo","title":"Philadelphia Housing Price Prediction – Technical Appendix","author":"Zicheng Xiang","date":"October 24, 2025"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}